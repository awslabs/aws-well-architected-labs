{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"README.html","text":"AWS Well-Architected Labs Introduction The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time. This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Labs: The labs are structured around the five pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Well-Architected Tool License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Home"},{"location":"README.html#aws-well-architected-labs","text":"","title":"AWS Well-Architected Labs"},{"location":"README.html#introduction","text":"The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time. This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.","title":"Introduction"},{"location":"README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"README.html#labs","text":"The labs are structured around the five pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Well-Architected Tool","title":"Labs:"},{"location":"README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/README.html","text":"AWS Well-Architected Cost Optimization Labs http://wellarchitectedlabs.com Introduction Cost optimization is a continual process of refinement and improvement of a system over its entire lifecycle. By using these labs, you gain practical experience on how to implement the Cost Optimization best practices and ensure your workloads are Well-Architected. For more information about cost optimization best practices in the cloud, visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper. Getting Started - Cost Optimization Fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Fundamentals covers the following: Account setup, AWS billing console, AWS Budgets, AWS Cost Explorer, Reserved Instances (RIs), Cost and Usage Report (CUR), cost and usage analysis, and cost and usage visualization. Expenditure Awareness The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Awareness broken into three areas: Usage Governance Monitoring Cost and Usage Decommissioning Resources It covers the following topics: Organizational policies, account structure, groups and roles, cost controls, tracking project lifecycle, cost and usage report, cost attribution categories, organization metrics, tagging, billing and cost management tools, reporting and notification, proactive monitoring, allocating cost, decommissioning process. Cost Effective Resources Using the appropriate resources for your workload is key to cost savings, this is not only the size and type of resource but may include managed or application level services. Using the best pricing models for the resources/services will also ensure the lowest costs. Cost effective resources consists of: Selection of Services Resource Type and Size Pricing Models Data Transfer Charges It covers the following topics: Organization cost requirements, analyze workload components, cost modeling, resource type and size, pricing models (Reserved Instances, Spot, OnDemand), region cost, data transfer modeling, data transfer services. Manage Demand and Supply Resources Demand on your systems may or may not be predictable, it can also be static or dynamic. By analyzing and understanding your system demand you can supply resources as required to achieve minimal costs: Manage Demand and Supply Resources It covers the following topics: demand analysis, reactive resource provisioning, dynamic resource provisioning (Auto Scaling, buffers). Optimizing Over Time As AWS releases new services and features, it is a best practice to review your existing architectural decisions to ensure they continue to be the most cost-effective: Evaluate new Services It covers the following topics: evaluating new services.","title":"Overview"},{"location":"Cost/README.html#aws-well-architected-cost-optimization-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/README.html#introduction","text":"Cost optimization is a continual process of refinement and improvement of a system over its entire lifecycle. By using these labs, you gain practical experience on how to implement the Cost Optimization best practices and ensure your workloads are Well-Architected. For more information about cost optimization best practices in the cloud, visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper.","title":"Introduction"},{"location":"Cost/README.html#getting-started-cost-optimization-fundamentals","text":"The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Fundamentals covers the following: Account setup, AWS billing console, AWS Budgets, AWS Cost Explorer, Reserved Instances (RIs), Cost and Usage Report (CUR), cost and usage analysis, and cost and usage visualization.","title":"Getting Started - Cost Optimization Fundamentals"},{"location":"Cost/README.html#expenditure-awareness","text":"The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Awareness broken into three areas: Usage Governance Monitoring Cost and Usage Decommissioning Resources It covers the following topics: Organizational policies, account structure, groups and roles, cost controls, tracking project lifecycle, cost and usage report, cost attribution categories, organization metrics, tagging, billing and cost management tools, reporting and notification, proactive monitoring, allocating cost, decommissioning process.","title":"Expenditure Awareness"},{"location":"Cost/README.html#cost-effective-resources","text":"Using the appropriate resources for your workload is key to cost savings, this is not only the size and type of resource but may include managed or application level services. Using the best pricing models for the resources/services will also ensure the lowest costs. Cost effective resources consists of: Selection of Services Resource Type and Size Pricing Models Data Transfer Charges It covers the following topics: Organization cost requirements, analyze workload components, cost modeling, resource type and size, pricing models (Reserved Instances, Spot, OnDemand), region cost, data transfer modeling, data transfer services.","title":"Cost Effective Resources"},{"location":"Cost/README.html#manage-demand-and-supply-resources","text":"Demand on your systems may or may not be predictable, it can also be static or dynamic. By analyzing and understanding your system demand you can supply resources as required to achieve minimal costs: Manage Demand and Supply Resources It covers the following topics: demand analysis, reactive resource provisioning, dynamic resource provisioning (Auto Scaling, buffers).","title":"Manage Demand and Supply Resources"},{"location":"Cost/README.html#optimizing-over-time","text":"As AWS releases new services and features, it is a best practice to review your existing architectural decisions to ensure they continue to be the most cost-effective: Evaluate new Services It covers the following topics: evaluating new services.","title":"Optimizing Over Time"},{"location":"Cost/CostEffectiveResources.html","text":"Cost Effective Resources http://wellarchitectedlabs.com About cost effective resources Using the appropriate services, instances and resources for your workload is key to cost savings. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. AWS offers a variety of flexible and cost-effective pricing options to acquire instances from EC2 and other services in a way that best fits your needs. Step 1 - Pricing Models - Reserved Instances By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Cost Effective Resources"},{"location":"Cost/CostEffectiveResources.html#cost-effective-resources","text":"http://wellarchitectedlabs.com","title":"Cost Effective Resources"},{"location":"Cost/CostEffectiveResources.html#about-cost-effective-resources","text":"Using the appropriate services, instances and resources for your workload is key to cost savings. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. AWS offers a variety of flexible and cost-effective pricing options to acquire instances from EC2 and other services in a way that best fits your needs.","title":"About cost effective resources"},{"location":"Cost/CostEffectiveResources.html#step-1-pricing-models-reserved-instances","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Step 1 - Pricing Models - Reserved Instances"},{"location":"Cost/ExpenditureAwareness.html","text":"Expenditure Awareness http://wellarchitectedlabs.com About expenditure awareness The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget. Step 1 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 2 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 3 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 4 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 5 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 6 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source. Step 7 (Optional) - Automated CUR Updates and Ingestion This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena, every time a new CUR file is delivered. 300 Level Lab : This lab uses s3 events and Lambda to trigger a Glue crawler and update Athena when a new CUR is delivered. Step 8 (Optional) - Multi-Account CUR Access This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. 300 Level Lab : This Lab demonstrates 3 different ways to access your CUR from another account. Step 9 (Optional) - Splitting and sharing the CUR This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. Ideal for partners to deliver a sub-account only CUR to each of its customers, or large enterprises. 300 Level Lab : This Lab uses S3 events, Lambda and Athena to extract part of a CUR file and deliver it to an S3 bucket for another account.","title":"Expenditure Awareness"},{"location":"Cost/ExpenditureAwareness.html#expenditure-awareness","text":"http://wellarchitectedlabs.com","title":"Expenditure Awareness"},{"location":"Cost/ExpenditureAwareness.html#about-expenditure-awareness","text":"The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.","title":"About expenditure awareness"},{"location":"Cost/ExpenditureAwareness.html#step-1-cost-and-usage-governance-notifications","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Step 1 - Cost and Usage Governance - Notifications"},{"location":"Cost/ExpenditureAwareness.html#step-2-monitor-usage-and-cost-analysis","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"Step 2 - Monitor Usage and Cost - Analysis"},{"location":"Cost/ExpenditureAwareness.html#step-3-monitor-usage-and-cost-visualization","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Step 3 - Monitor Usage and Cost - Visualization"},{"location":"Cost/ExpenditureAwareness.html#step-4-govern-usage-and-cost-controls","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.","title":"Step 4 - Govern Usage and Cost - Controls"},{"location":"Cost/ExpenditureAwareness.html#step-5-monitor-usage-and-cost-advanced-analysis","text":"Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.","title":"Step 5 - Monitor Usage and Cost - Advanced Analysis"},{"location":"Cost/ExpenditureAwareness.html#step-6-monitor-usage-and-cost-advanced-visualization","text":"Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Step 6 - Monitor Usage and Cost - Advanced Visualization"},{"location":"Cost/ExpenditureAwareness.html#step-7-optional-automated-cur-updates-and-ingestion","text":"This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena, every time a new CUR file is delivered. 300 Level Lab : This lab uses s3 events and Lambda to trigger a Glue crawler and update Athena when a new CUR is delivered.","title":"Step 7 (Optional) - Automated CUR Updates and Ingestion"},{"location":"Cost/ExpenditureAwareness.html#step-8-optional-multi-account-cur-access","text":"This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. 300 Level Lab : This Lab demonstrates 3 different ways to access your CUR from another account.","title":"Step 8 (Optional) - Multi-Account CUR Access"},{"location":"Cost/ExpenditureAwareness.html#step-9-optional-splitting-and-sharing-the-cur","text":"This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. Ideal for partners to deliver a sub-account only CUR to each of its customers, or large enterprises. 300 Level Lab : This Lab uses S3 events, Lambda and Athena to extract part of a CUR file and deliver it to an S3 bucket for another account.","title":"Step 9 (Optional) - Splitting and sharing the CUR"},{"location":"Cost/Fundamentals.html","text":"Cost Optimization Fundamentals http://wellarchitectedlabs.com About cost optimization fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Step 1 - Account Setup This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Step 2 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 3 - Pricing Models - Reserved Instances By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. Step 4 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 5 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 6 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 7 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 8 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Fundamentals"},{"location":"Cost/Fundamentals.html#cost-optimization-fundamentals","text":"http://wellarchitectedlabs.com","title":"Cost Optimization Fundamentals"},{"location":"Cost/Fundamentals.html#about-cost-optimization-fundamentals","text":"The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles.","title":"About cost optimization fundamentals"},{"location":"Cost/Fundamentals.html#step-1-account-setup","text":"This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Step 1 - Account Setup"},{"location":"Cost/Fundamentals.html#step-2-cost-and-usage-governance-notifications","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Step 2 - Cost and Usage Governance - Notifications"},{"location":"Cost/Fundamentals.html#step-3-pricing-models-reserved-instances","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Step 3 - Pricing Models - Reserved Instances"},{"location":"Cost/Fundamentals.html#step-4-monitor-usage-and-cost-analysis","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"Step 4 - Monitor Usage and Cost - Analysis"},{"location":"Cost/Fundamentals.html#step-5-monitor-usage-and-cost-visualization","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Step 5 - Monitor Usage and Cost - Visualization"},{"location":"Cost/Fundamentals.html#step-6-govern-usage-and-cost-controls","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.","title":"Step 6 - Govern Usage and Cost - Controls"},{"location":"Cost/Fundamentals.html#step-7-monitor-usage-and-cost-advanced-analysis","text":"Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.","title":"Step 7 - Monitor Usage and Cost - Advanced Analysis"},{"location":"Cost/Fundamentals.html#step-8-monitor-usage-and-cost-advanced-visualization","text":"Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Step 8 - Monitor Usage and Cost - Advanced Visualization"},{"location":"Cost/arc204.html","text":"ARC204 Cost Optimizing a Workload http://wellarchitectedlabs.com Step1 - Setup This first step will help you to you setup your account, load the first set of application logs and Cost and Usage files, create a database to query them, and setup the visualization platform. After this step you will have everything configured to be able to perform analysis by querying log files and perform visualizations. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Analysis Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Licensing By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. TBA Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Monitoring Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Well-Architected Tool Implementing usage controls will ensure excess usage and accompanying costs does not occur. 100 Level Lab : This lab will walk you through the Well-Architected Tool.","title":"ARC204 Cost Optimizing a Workload"},{"location":"Cost/arc204.html#arc204-cost-optimizing-a-workload","text":"http://wellarchitectedlabs.com","title":"ARC204 Cost Optimizing a Workload"},{"location":"Cost/arc204.html#step1-setup","text":"This first step will help you to you setup your account, load the first set of application logs and Cost and Usage files, create a database to query them, and setup the visualization platform. After this step you will have everything configured to be able to perform analysis by querying log files and perform visualizations. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Step1 - Setup"},{"location":"Cost/arc204.html#analysis","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Analysis"},{"location":"Cost/arc204.html#licensing","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Licensing"},{"location":"Cost/arc204.html#tba","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"TBA"},{"location":"Cost/arc204.html#monitoring","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Monitoring"},{"location":"Cost/arc204.html#well-architected-tool","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 100 Level Lab : This lab will walk you through the Well-Architected Tool.","title":"Well-Architected Tool"},{"location":"Cost/arc219.html","text":"ARC219 AWS Cost Management Tools for Cost and Usage Optimization http://wellarchitectedlabs.com Governance This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Analysis Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Licensing By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. TBA Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Monitoring Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Well-Architected Tool Implementing usage controls will ensure excess usage and accompanying costs does not occur. 100 Level Lab : This lab will walk you through the Well-Architected Tool.","title":"ARC219 AWS Cost Management Tools for Cost and Usage Optimization"},{"location":"Cost/arc219.html#arc219-aws-cost-management-tools-for-cost-and-usage-optimization","text":"http://wellarchitectedlabs.com","title":"ARC219 AWS Cost Management Tools for Cost and Usage Optimization"},{"location":"Cost/arc219.html#governance","text":"This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Governance"},{"location":"Cost/arc219.html#analysis","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Analysis"},{"location":"Cost/arc219.html#licensing","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Licensing"},{"location":"Cost/arc219.html#tba","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"TBA"},{"location":"Cost/arc219.html#monitoring","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Monitoring"},{"location":"Cost/arc219.html#well-architected-tool","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 100 Level Lab : This lab will walk you through the Well-Architected Tool.","title":"Well-Architected Tool"},{"location":"Cost/ent206.html","text":"ENT206 Optimize AWS costs and utilization with AWS managment tools http://wellarchitectedlabs.com Governance This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Analysis Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Licensing By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. TBA Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Monitoring Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Well-Architected Tool Implementing usage controls will ensure excess usage and accompanying costs does not occur. 100 Level Lab : This lab will walk you through the Well-Architected Tool.","title":"ENT206 Optimize AWS costs and utilization with AWS managment tools"},{"location":"Cost/ent206.html#ent206-optimize-aws-costs-and-utilization-with-aws-managment-tools","text":"http://wellarchitectedlabs.com","title":"ENT206 Optimize AWS costs and utilization with AWS managment tools"},{"location":"Cost/ent206.html#governance","text":"This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Governance"},{"location":"Cost/ent206.html#analysis","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Analysis"},{"location":"Cost/ent206.html#licensing","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Licensing"},{"location":"Cost/ent206.html#tba","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"TBA"},{"location":"Cost/ent206.html#monitoring","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Monitoring"},{"location":"Cost/ent206.html#well-architected-tool","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 100 Level Lab : This lab will walk you through the Well-Architected Tool.","title":"Well-Architected Tool"},{"location":"Cost/thankyou.html","text":"Thankyou for your feedback Return to the Cost Labs","title":"Thankyou for your feedback"},{"location":"Cost/thankyou.html#thankyou-for-your-feedback","text":"","title":"Thankyou for your feedback"},{"location":"Cost/thankyou.html#return-to-the-cost-labs","text":"","title":"Return to the Cost Labs"},{"location":"Cost/Cost_Fundamentals/README.html","text":"Cost Optimization Fundamentals http://wellarchitectedlabs.com About cost optimization fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Step 1 - Account Setup This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people withing your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Step 2 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 3 - Pricing Models - Reserved Instances By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. Step 4 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 5 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 6 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 7 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 8 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Cost Optimization Fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#cost-optimization-fundamentals","text":"http://wellarchitectedlabs.com","title":"Cost Optimization Fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#about-cost-optimization-fundamentals","text":"The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles.","title":"About cost optimization fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#step-1-account-setup","text":"This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people withing your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Step 1 - Account Setup"},{"location":"Cost/Cost_Fundamentals/README.html#step-2-cost-and-usage-governance-notifications","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Step 2 - Cost and Usage Governance - Notifications"},{"location":"Cost/Cost_Fundamentals/README.html#step-3-pricing-models-reserved-instances","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Step 3 - Pricing Models - Reserved Instances"},{"location":"Cost/Cost_Fundamentals/README.html#step-4-monitor-usage-and-cost-analysis","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"Step 4 - Monitor Usage and Cost - Analysis"},{"location":"Cost/Cost_Fundamentals/README.html#step-5-monitor-usage-and-cost-visualization","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Step 5 - Monitor Usage and Cost - Visualization"},{"location":"Cost/Cost_Fundamentals/README.html#step-6-govern-usage-and-cost-controls","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.","title":"Step 6 - Govern Usage and Cost - Controls"},{"location":"Cost/Cost_Fundamentals/README.html#step-7-monitor-usage-and-cost-advanced-analysis","text":"Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.","title":"Step 7 - Monitor Usage and Cost - Advanced Analysis"},{"location":"Cost/Cost_Fundamentals/README.html#step-8-monitor-usage-and-cost-advanced-visualization","text":"Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Step 8 - Monitor Usage and Cost - Advanced Visualization"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html","text":"Level 100: AWS Account Setup https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework. Goals Implement an account structure Configure billing services Prerequisites Multiple AWS accounts (at least two) Root user access to the master account Permissions required Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user ./Code/IAM_policy IAM policy required to create the cost optimization team NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags [ ] Create a cost optimization team, to manage cost optimization across your organization License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#level-100-aws-account-setup","text":"https://wellarchitectedlabs.com","title":"Level 100: AWS Account Setup"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#introduction","text":"This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#goals","text":"Implement an account structure Configure billing services","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#prerequisites","text":"Multiple AWS accounts (at least two) Root user access to the master account","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#permissions-required","text":"Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user ./Code/IAM_policy IAM policy required to create the cost optimization team NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#best-practice-checklist","text":"[ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags [ ] Create a cost optimization team, to manage cost optimization across your organization","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html","text":"Level 100: AWS Account Setup: Lab Guide Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Create a cost optimization team Tear down Rate this Lab 1. Configure IAM access to your billing NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activeate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing. 2. Create an account structure NOTE : Do NOT do this step if you already have an organization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to the master account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data. 2.1 Create an AWS Organization You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to. 2.2 Join member accounts You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization. 3. Configure billing account settings It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by muptile team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update : 4. Configure Cost and Usage Reports Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient. 4.1 Configure a Cost and Usage Report If you configure multiple Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multiple CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Esure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered. 4.2 Enable monthly billing report The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. NOTE : Billing files will only be delivered from the current month onwards. It will not generate previous months billing files. Go to the billng console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences : 5. Enable AWS Cost Explorer AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated: 6. Enable AWS-Generated Cost Allocation Tags Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated: 7. Create a cost optimization team We are going to create a cost optimization team within your master/payer account - which is where the billing information is. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab 7.1 Create an IAM policy for the team This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. 1 - Log in and go to the IAM Service page: 2 - Select Policies from the left menu: 3 - Select Create Policy : 4 - Select the JSON tab: 5 - Copy paste the following policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ aws-portal:ViewUsage , aws-portal:ModifyBilling , aws-portal:ViewBilling , aws-portal:ViewAccount , budgets:* ], Resource : * } ] } 6 - Click Review policy : 7 - Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy. 7.2 Create an IAM Group This group will bring together IAM users and apply the required policies. 1 - While in the IAM console, select Groups from the left menu: 2 - Click on Create New Group : 3 - Enter a Group Name and click Next Step : 4 - Click Policy Type and select Customer Managed : 5 - Select the CostOptimization_Summit policy (created previously): 6 - Click Create Group : You have now successfully created the cost optimization group, and attached the required policies. 7.3 Create an IAM User For this lab we will create a user and join them to the group above. 1 - In the IAM console, select Users from the left menu: 2 - Click Add user : 3 - Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : 4 - Select the CostOptimization group (created previously), and click Next: Tags : 5 - Click Next Review : 6 - Click Create user : 7 - Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: 8 - Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization. 8. Tear down This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used. 9. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#level-100-aws-account-setup-lab-guide","text":"","title":"Level 100: AWS Account Setup: Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#table-of-contents","text":"Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Create a cost optimization team Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#1-configure-iam-access-to-your-billing","text":"NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activeate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing.","title":"1. Configure IAM access to your billing"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#2-create-an-account-structure","text":"NOTE : Do NOT do this step if you already have an organization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to the master account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data.","title":"2. Create an account structure"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#21-create-an-aws-organization","text":"You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to.","title":"2.1 Create an AWS Organization"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#22-join-member-accounts","text":"You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization.","title":"2.2 Join member accounts"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#3-configure-billing-account-settings","text":"It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by muptile team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update :","title":"3. Configure billing account settings"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#4-configure-cost-and-usage-reports","text":"Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient.","title":"4. Configure Cost and Usage Reports"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#41-configure-a-cost-and-usage-report","text":"If you configure multiple Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multiple CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Esure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered.","title":"4.1 Configure a Cost and Usage Report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#42-enable-monthly-billing-report","text":"The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. NOTE : Billing files will only be delivered from the current month onwards. It will not generate previous months billing files. Go to the billng console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences :","title":"4.2 Enable monthly billing report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#5-enable-aws-cost-explorer","text":"AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated:","title":"5. Enable AWS Cost Explorer"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#6-enable-aws-generated-cost-allocation-tags","text":"Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated:","title":"6. Enable AWS-Generated Cost Allocation Tags"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#7-create-a-cost-optimization-team","text":"We are going to create a cost optimization team within your master/payer account - which is where the billing information is. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab","title":"7. Create a cost optimization team "},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#71-create-an-iam-policy-for-the-team","text":"This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. 1 - Log in and go to the IAM Service page: 2 - Select Policies from the left menu: 3 - Select Create Policy : 4 - Select the JSON tab: 5 - Copy paste the following policy into the the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ aws-portal:ViewUsage , aws-portal:ModifyBilling , aws-portal:ViewBilling , aws-portal:ViewAccount , budgets:* ], Resource : * } ] } 6 - Click Review policy : 7 - Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy.","title":"7.1 Create an IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#72-create-an-iam-group","text":"This group will bring together IAM users and apply the required policies. 1 - While in the IAM console, select Groups from the left menu: 2 - Click on Create New Group : 3 - Enter a Group Name and click Next Step : 4 - Click Policy Type and select Customer Managed : 5 - Select the CostOptimization_Summit policy (created previously): 6 - Click Create Group : You have now successfully created the cost optimization group, and attached the required policies.","title":"7.2 Create an IAM Group"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#73-create-an-iam-user","text":"For this lab we will create a user and join them to the group above. 1 - In the IAM console, select Users from the left menu: 2 - Click Add user : 3 - Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : 4 - Select the CostOptimization group (created previously), and click Next: Tags : 5 - Click Next Review : 6 - Click Create user : 7 - Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: 8 - Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully create a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization.","title":"7.3 Create an IAM User"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#8-tear-down","text":"This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used.","title":"8. Tear down"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#9-rate-this-lab","text":"","title":"9. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/IAM_policy.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ iam:ListPolicies , iam:GetPolicyVersion , iam:CreateGroup , iam:GetPolicy , iam:DeletePolicy , iam:DetachGroupPolicy , iam:ListGroupPolicies , iam:AttachUserPolicy , iam:CreateUser , iam:GetGroup , iam:CreatePolicy , iam:CreateLoginProfile , iam:AddUserToGroup , iam:ListPolicyVersions , iam:AttachGroupPolicy , iam:ListUsers , iam:ListAttachedGroupPolicies , iam:ListGroups , iam:GetGroupPolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion , iam:GetLoginProfile ], Resource : * } ] }","title":"IAM policy"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/master_policy.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ organizations:InviteAccountToOrganization , organizations:ListRoots , aws-portal:ModifyAccount , s3:ListBucketVersions , organizations:DescribeAccount , s3:CreateBucket , s3:ListBucket , s3:GetBucketPolicy , organizations:ListChildren , aws-portal:ModifyBilling , organizations:ListCreateAccountStatus , organizations:DescribeOrganization , organizations:EnableAllFeatures , aws-portal:ViewBilling , organizations:DescribeHandshake , s3:PutBucketVersioning , organizations:DescribeCreateAccountStatus , organizations:CreateOrganization , s3:GetBucketPolicyStatus , s3:GetBucketPublicAccessBlock , s3:PutBucketPublicAccessBlock , aws-portal:ViewAccount , s3:GetBucketVersioning , organizations:ListAWSServiceAccessForOrganization , s3:DeleteBucketPolicy , organizations:ListHandshakesForOrganization , organizations:ListAccounts , iam:CreateServiceLinkedRole , s3:ListAllMyBuckets , s3:PutBucketPolicy , s3:GetBucketLocation ], Resource : * } ] }","title":"Master policy"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/member_policy.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ iam:CreateServiceLinkedRole , organizations:AcceptHandshake , organizations:DescribeHandshake ], Resource : [ arn:aws:iam::*:role/* , arn:aws:organizations::*:handshake/o-*/*/h-* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : organizations:DescribeAccount , Resource : arn:aws:organizations::*:account/o-*/* }, { Sid : VisualEditor2 , Effect : Allow , Action : [ organizations:ListHandshakesForAccount , organizations:DescribeOrganization , organizations:DescribeCreateAccountStatus ], Resource : * } ] }","title":"Member policy"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html","text":"Level 100: Cost and Usage Governance https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Create a Cost Optimization team to monitor usage and cost Implement AWS Budgets to notify on usage and spend Prerequisites An AWS Account AWS Account Setup has been completed Permissions required Access to the Cost Optimization team created in AWS Account Setup Costs https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed Time to complete The lab should take approximately 15 minutes to complete Best Practice Checklist [ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on RI Coverage [ ] Create a weekly AWS budget report License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#level-100-cost-and-usage-governance","text":"https://wellarchitectedlabs.com","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#goals","text":"Create a Cost Optimization team to monitor usage and cost Implement AWS Budgets to notify on usage and spend","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#prerequisites","text":"An AWS Account AWS Account Setup has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#permissions-required","text":"Access to the Cost Optimization team created in AWS Account Setup","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#costs","text":"https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed","title":"Costs"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#best-practice-checklist","text":"[ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on RI Coverage [ ] Create a weekly AWS budget report","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html","text":"Level 100: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - SP Coverage Create an AWS Budget Report Tear down Rate this Lab 1. Create and implement an AWS Budget for monthly forecasted cost Budgets allow you to manage cost and usage by providing notifications when cost or usage are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. Budgets and notifications are updated when your billing data is updated, which is at least once per day. NOTE : You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account. Create a monthly cost budget for your account We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Log into the console as an IAM user with the required permissions, go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget : Create a cost budget, enter the following details: Name : CostBudget1 Period : Monthly Budget effective dates : Recurring Budget Start Month : (select current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave as defaults: Scroll down and click Configure alerts : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You should see the current forecast will exceed the budget (it should be red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount. 2. Create and implement an AWS Budget for EC2 actual cost We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget : Create a cost budget, enter the following details: Name : EC2_actual Period : Monthly Budget effective dates : Recurring Budget Start Month : (current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for services other than EC2. 3. Create and implement an AWS Budget for EC2 Savings Plan coverage We will create a monthly savings plan coverage budget which will notify if the coverage of Savings Plan for EC2 is below the specified amount. Click Create budget : Select Savings Plans budget , and click Set your budget : Create a cost budget, enter the following details: Name : SP_Coverage Period : Monthly Savings Plans budget type : Savings Plans Coverage Coverage threshold : 90% Leave all other fields as defaults NOTE : NEVER create a utilization budget, unless you are doing it for a single and specific discount rate by using filters. For example you want to track the utilization of m5.large Linux discount. A utilization budget across different discounts will most likely lead to confusion and unnecessary work. Scroll down and click Configure alerts : Enter an address for Email contacts and click Confirm budget : Review the configuration, and click Create in the lower right: You have created an Savings Plans Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes: 4. Create and implement an AWS Budget Report AWS Budgets Reports allow you to create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budgets. From the Budgets dashboard, Click on Budgets Reports : Click Create budget report : Create a report with the following details: Report name : WeeklyBudgets Select all budgets Click Configure delivery settings : Configure the delivery settings: Report frequency : Weekly Day of week : Monday Email recipients : Click Confirm budget report : Review the configuration, click Create : Your budget report should now be complete: You should receive an email similar to the one below: 5. Tear down Delete a budget report We will delete the bugdet report we created in section 4. From the Budgets Reports dashboard, click on the three dots next to the Weekly Budgets budget report: You can see there are no budget reports: Delete a budget We will delete all three budgets that were configured in sections 1,2 and 3. From the budgets homepage, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name SP_Coverage : Click on the 3 dot menu in the top right, select Delete : All budgets should be deleted that were created in this workshop: 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#level-100-cost-and-usage-governance","text":"","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#table-of-contents","text":"Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - SP Coverage Create an AWS Budget Report Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#1-create-and-implement-an-aws-budget-for-monthly-forecasted-cost","text":"Budgets allow you to manage cost and usage by providing notifications when cost or usage are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. Budgets and notifications are updated when your billing data is updated, which is at least once per day. NOTE : You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account.","title":"1. Create and implement an AWS Budget for monthly forecasted cost"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#create-a-monthly-cost-budget-for-your-account","text":"We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Log into the console as an IAM user with the required permissions, go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget : Create a cost budget, enter the following details: Name : CostBudget1 Period : Monthly Budget effective dates : Recurring Budget Start Month : (select current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave as defaults: Scroll down and click Configure alerts : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You should see the current forecast will exceed the budget (it should be red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount.","title":"Create a monthly cost budget for your account"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#2-create-and-implement-an-aws-budget-for-ec2-actual-cost","text":"We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget : Create a cost budget, enter the following details: Name : EC2_actual Period : Monthly Budget effective dates : Recurring Budget Start Month : (current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for services other than EC2.","title":"2. Create and implement an AWS Budget for EC2 actual cost"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#3-create-and-implement-an-aws-budget-for-ec2-savings-plan-coverage","text":"We will create a monthly savings plan coverage budget which will notify if the coverage of Savings Plan for EC2 is below the specified amount. Click Create budget : Select Savings Plans budget , and click Set your budget : Create a cost budget, enter the following details: Name : SP_Coverage Period : Monthly Savings Plans budget type : Savings Plans Coverage Coverage threshold : 90% Leave all other fields as defaults NOTE : NEVER create a utilization budget, unless you are doing it for a single and specific discount rate by using filters. For example you want to track the utilization of m5.large Linux discount. A utilization budget across different discounts will most likely lead to confusion and unnecessary work. Scroll down and click Configure alerts : Enter an address for Email contacts and click Confirm budget : Review the configuration, and click Create in the lower right: You have created an Savings Plans Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes:","title":"3. Create and implement an AWS Budget for EC2 Savings Plan coverage"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#4-create-and-implement-an-aws-budget-report","text":"AWS Budgets Reports allow you to create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budgets. From the Budgets dashboard, Click on Budgets Reports : Click Create budget report : Create a report with the following details: Report name : WeeklyBudgets Select all budgets Click Configure delivery settings : Configure the delivery settings: Report frequency : Weekly Day of week : Monday Email recipients : Click Confirm budget report : Review the configuration, click Create : Your budget report should now be complete: You should receive an email similar to the one below:","title":"4. Create and implement an AWS Budget Report "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#5-tear-down","text":"","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-budget-report","text":"We will delete the bugdet report we created in section 4. From the Budgets Reports dashboard, click on the three dots next to the Weekly Budgets budget report: You can see there are no budget reports:","title":"Delete a budget report"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-budget","text":"We will delete all three budgets that were configured in sections 1,2 and 3. From the budgets homepage, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name SP_Coverage : Click on the 3 dot menu in the top right, select Delete : All budgets should be deleted that were created in this workshop:","title":"Delete a budget"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html","text":"Level 100: Cost and Usage Analysis https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#level-100-cost-and-usage-analysis","text":"https://wellarchitectedlabs.com","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#best-practice-checklist","text":"[ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html","text":"Level 100: Cost and Usage Analysis Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Rate this Lab 1. View your AWS Invoices At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below: 2. View your cost and usage in detail You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage: 3. Download your monthly cost and usage file It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_ansalysis . 4. Tear down There is no configuration performed within this lab, so no teardown is required. 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#level-100-cost-and-usage-analysis","text":"","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#table-of-contents","text":"View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#1-view-your-aws-invoices","text":"At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below:","title":"1. View your AWS Invoices "},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#2-view-your-cost-and-usage-in-detail","text":"You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage:","title":"2. View your cost and usage in detail"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#3-download-your-monthly-cost-and-usage-file","text":"It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_ansalysis .","title":"3. Download your monthly cost and usage file"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#4-tear-down","text":"There is no configuration performed within this lab, so no teardown is required.","title":"4. Tear down"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html","text":"Level 100: Billing Visualization https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#level-100-billing-visualization","text":"https://wellarchitectedlabs.com","title":"Level 100: Billing Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#best-practice-checklist","text":"[ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html","text":"Level 100: Cost Visualization Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your cost and usage by service View your cost and usage by account View your Reserved Instance coverage Create a custom EC2 report Tear down Rate this Lab 1. View your cost and usage by service AWS Cost Explorer is a free built in tool to that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Green line): We will remove the RI recurring fees. Click on the Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exlude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is Glue: We will remove the Glue service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to Glue , select Exclude only , and click Apply filters : Glue has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multiple filters. You can continue to modify the report by timeframe and apply other filters. 2. View your cost and usage by account We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multiple accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type : You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type : Here is the usage type breakdown: You have now viewed the costs by account and applied multiple filters. You can continue to modify the report by timeframe and apply other filters. 3. View your Reserved Instance coverage To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RI's) is required. A typical goal is to aim for approximately 80% of running instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage. 4. Create custom EC2 reports We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Explore , and click Cost Usage : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports: 5. Tear down We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available: 6. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#level-100-cost-visualization","text":"","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#table-of-contents","text":"View your cost and usage by service View your cost and usage by account View your Reserved Instance coverage Create a custom EC2 report Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#1-view-your-cost-and-usage-by-service","text":"AWS Cost Explorer is a free built in tool to that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Green line): We will remove the RI recurring fees. Click on the Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exlude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is Glue: We will remove the Glue service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to Glue , select Exclude only , and click Apply filters : Glue has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.","title":"1. View your cost and usage by service "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#2-view-your-cost-and-usage-by-account","text":"We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multiple accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type : You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type : Here is the usage type breakdown: You have now viewed the costs by account and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.","title":"2. View your cost and usage by account "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#3-view-your-reserved-instance-coverage","text":"To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RI's) is required. A typical goal is to aim for approximately 80% of running instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage.","title":"3. View your Reserved Instance coverage "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#4-create-custom-ec2-reports","text":"We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Explore , and click Cost Usage : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports:","title":"4. Create custom EC2 reports "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#5-tear-down","text":"We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available:","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#6-rate-this-lab","text":"","title":"6. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html","text":"Level 200: Cost and Usage Governance https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Create a Cost Optimization team to monitor usage, cost, and enforce policies Implement IAM Policies to control usage Prerequisites An AWS Account Completed all previous labs in the Cost Fundamentals series Permissions required ./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#level-200-cost-and-usage-governance","text":"https://wellarchitectedlabs.com","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#goals","text":"Create a Cost Optimization team to monitor usage, cost, and enforce policies Implement IAM Policies to control usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#prerequisites","text":"An AWS Account Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#permissions-required","text":"./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#best-practice-checklist","text":"[ ] Create a cost optimizaion team, to manage cost optimization across your organization [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html","text":"Level 200: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Modify the cost optimization team Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Rate this Lab 1. Modify the cost optimization team We are going to modify the cost optimization team created previously, as this lab has additional access requirements to implementing IAM policies. 1.1 Modify the IAM policy for the team 1 - Log into the console as an IAM user with the required permissions and go to the IAM Service page: 2 - Select Policies from the left menu: 3 - Click Filter policies , and select Customer managed : 4 - Click the existing Cost Optimization Policy: 5 - Click Edit policy : 6 - Click JSON : 7 - Modify the policy below, replace -billing bucket- (2 replacements) with the name of the bucket your CUR files are delivered to. Then copy paste the policy into the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : [ arn:aws:s3:::-billing bucket- , arn:aws:s3:::-billing bucket-/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : [ iam:GetPolicyVersion , quicksight:CreateAdmin , iam:DeletePolicy , iam:CreateRole , iam:AttachRolePolicy , aws-portal:ViewUsage , iam:GetGroup , aws-portal:ModifyBilling , iam:DetachRolePolicy , iam:ListAttachedRolePolicies , ds:UnauthorizeApplication , aws-portal:ViewBilling , iam:DetachGroupPolicy , iam:ListAttachedGroupPolicies , iam:CreatePolicyVersion , ds:CheckAlias , quicksight:Subscribe , ds:DeleteDirectory , iam:ListPolicies , iam:GetRole , ds:CreateIdentityPoolDirectory , ds:DescribeTrusts , iam:GetPolicy , iam:ListGroupPolicies , aws-portal:ViewAccount , iam:ListEntitiesForPolicy , iam:AttachUserPolicy , iam:ListRoles , iam:DeleteRole , budgets:* , iam:CreatePolicy , quicksight:CreateUser , s3:ListAllMyBuckets , iam:ListPolicyVersions , iam:AttachGroupPolicy , quicksight:Unsubscribe , iam:ListAccountAliases , ds:DescribeDirectories , iam:ListGroups , iam:GetGroupPolicy , ds:CreateAlias , ds:AuthorizeApplication , iam:DeletePolicyVersion ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully modified the cost optimization teams policy, you can complete the rest of the lab using the cost optimization group. 2. Create an IAM Policy to restrict service usage by region To manage costs you need to manage and control your usage. AWS offers multilpe regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 2.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/Region_Restrict Click Review policy : NOTE: the policy may be different from the image above Enter a Name and Description , and click Create policy : You have successfully created the Policy. 2.2 Apply it to a group Select Groups from the left menu: Click on the CostOptimization group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the Cost Optimizaiton group. 2.3 Verify the policy is in effect Go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : You will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only): Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you can not launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. 3. Create an IAM Policy to restirct EC2 usage by family AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 3.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2Family_Restrict Click Review policy : Enter a Name , a Description , and click on Create Policy : 3.2 Attach the policy to the group Click on Groups from the left menu: Click on the CostOptimization group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy : 3.3 Verify the policy is in effect Click on Services and click EC2 : Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above: 4. Extend an IAM Policy to restrict EC2 usage by instance size We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed. 4.1 Extend the EC2Family_Restrict IAM Policy Go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, be careful not to change the syntax and only remove the * characters. Click on Review policy : Click on Save changes : 4.2 Verify the policy is in effect Click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuraion and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasnt a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size. 5. Create an IAM policy to restrict EBS Volume creation by volume type Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimise cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 5.1 Create the IAM Policy Go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2EBS_Restrict Click on Review Policy : Enter a Name and a Description , and click Create policy : 5.2 Attach the policy to the Cost Optimization group Click on Groups from the left menu: Click on the CostOptimization group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy : 5.3 Verify the policy is in effect Click on Services then click EC2 : Click Launch Instance : Click Select next to Aamzon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesnt contain an io1 volume type. Click on the instance ID and terminate the instance as above: 6. Tear down NOTE: To replace the cost optimization group IAM policy, follow the same process and implement the original policy contained in the Account Setup lab. Delete a policy We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies. 7. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#level-200-cost-and-usage-governance","text":"","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#table-of-contents","text":"Modify the cost optimization team Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#1-modify-the-cost-optimization-team","text":"We are going to modify the cost optimization team created previously, as this lab has additional access requirements to implementing IAM policies.","title":"1. Modify the cost optimization team "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#11-modify-the-iam-policy-for-the-team","text":"1 - Log into the console as an IAM user with the required permissions and go to the IAM Service page: 2 - Select Policies from the left menu: 3 - Click Filter policies , and select Customer managed : 4 - Click the existing Cost Optimization Policy: 5 - Click Edit policy : 6 - Click JSON : 7 - Modify the policy below, replace -billing bucket- (2 replacements) with the name of the bucket your CUR files are delivered to. Then copy paste the policy into the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : [ arn:aws:s3:::-billing bucket- , arn:aws:s3:::-billing bucket-/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : [ iam:GetPolicyVersion , quicksight:CreateAdmin , iam:DeletePolicy , iam:CreateRole , iam:AttachRolePolicy , aws-portal:ViewUsage , iam:GetGroup , aws-portal:ModifyBilling , iam:DetachRolePolicy , iam:ListAttachedRolePolicies , ds:UnauthorizeApplication , aws-portal:ViewBilling , iam:DetachGroupPolicy , iam:ListAttachedGroupPolicies , iam:CreatePolicyVersion , ds:CheckAlias , quicksight:Subscribe , ds:DeleteDirectory , iam:ListPolicies , iam:GetRole , ds:CreateIdentityPoolDirectory , ds:DescribeTrusts , iam:GetPolicy , iam:ListGroupPolicies , aws-portal:ViewAccount , iam:ListEntitiesForPolicy , iam:AttachUserPolicy , iam:ListRoles , iam:DeleteRole , budgets:* , iam:CreatePolicy , quicksight:CreateUser , s3:ListAllMyBuckets , iam:ListPolicyVersions , iam:AttachGroupPolicy , quicksight:Unsubscribe , iam:ListAccountAliases , ds:DescribeDirectories , iam:ListGroups , iam:GetGroupPolicy , ds:CreateAlias , ds:AuthorizeApplication , iam:DeletePolicyVersion ], Resource : * } ] } Click Review policy : Enter a Name and Description for the policy and click Create policy : You have successfully modified the cost optimization teams policy, you can complete the rest of the lab using the cost optimization group.","title":"1.1 Modify the IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#2-create-an-iam-policy-to-restrict-service-usage-by-region","text":"To manage costs you need to manage and control your usage. AWS offers multilpe regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"2. Create an IAM Policy to restrict service usage by region "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#21-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/Region_Restrict Click Review policy : NOTE: the policy may be different from the image above Enter a Name and Description , and click Create policy : You have successfully created the Policy.","title":"2.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#22-apply-it-to-a-group","text":"Select Groups from the left menu: Click on the CostOptimization group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the Cost Optimizaiton group.","title":"2.2 Apply it to a group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#23-verify-the-policy-is-in-effect","text":"Go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : You will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only): Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you can not launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region.","title":"2.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#3-create-an-iam-policy-to-restirct-ec2-usage-by-family","text":"AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"3. Create an IAM Policy to restirct EC2 usage by family "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#31-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2Family_Restrict Click Review policy : Enter a Name , a Description , and click on Create Policy :","title":"3.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#32-attach-the-policy-to-the-group","text":"Click on Groups from the left menu: Click on the CostOptimization group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy :","title":"3.2 Attach the policy to the group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#33-verify-the-policy-is-in-effect","text":"Click on Services and click EC2 : Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above:","title":"3.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#4-extend-an-iam-policy-to-restrict-ec2-usage-by-instance-size","text":"We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed.","title":"4. Extend an IAM Policy to restrict EC2 usage by instance size "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#41-extend-the-ec2family_restrict-iam-policy","text":"Go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, be careful not to change the syntax and only remove the * characters. Click on Review policy : Click on Save changes :","title":"4.1 Extend the EC2Family_Restrict IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#42-verify-the-policy-is-in-effect","text":"Click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuraion and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasnt a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size.","title":"4.2 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#5-create-an-iam-policy-to-restrict-ebs-volume-creation-by-volume-type","text":"Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimise cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"5. Create an IAM policy to restrict EBS Volume creation by volume type "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#51-create-the-iam-policy","text":"Go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2EBS_Restrict Click on Review Policy : Enter a Name and a Description , and click Create policy :","title":"5.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#52-attach-the-policy-to-the-cost-optimization-group","text":"Click on Groups from the left menu: Click on the CostOptimization group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy :","title":"5.2 Attach the policy to the Cost Optimization group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#53-verify-the-policy-is-in-effect","text":"Click on Services then click EC2 : Click Launch Instance : Click Select next to Aamzon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesnt contain an io1 volume type. Click on the instance ID and terminate the instance as above:","title":"5.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#6-tear-down","text":"NOTE: To replace the cost optimization group IAM policy, follow the same process and implement the original policy contained in the Account Setup lab.","title":"6. Tear down "},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-policy","text":"We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies.","title":"Delete a policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#7-rate-this-lab","text":"","title":"7. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/EC2EBS_Restrict.html","text":"{ Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Deny , Action : ec2:* , Resource : * , Condition : { StringEquals : { ec2:VolumeType : io1 } } } ] }","title":"EC2EBS Restrict"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/EC2Family_Restrict.html","text":"{ Version : 2012-10-17 , Statement : [ { Effect : Allow , Action : ec2:* , Resource : * , Condition : { ForAllValues:StringLike : { ec2:InstanceType : [ t3.* , a1.* , m5.* ] } } } ] }","title":"EC2Family Restrict"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/IAM_policy.html","text":"NOTE: Policy is only required to complete the exercise to create a cost optimization team. It can be delelted once the first exercise is complete. { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ iam:ListPolicies , iam:GetPolicyVersion , iam:CreateGroup , iam:GetPolicy , iam:DeletePolicy , iam:DetachGroupPolicy , iam:ListGroupPolicies , iam:AttachUserPolicy , iam:CreateUser , iam:GetGroup , iam:CreatePolicy , iam:CreateLoginProfile , iam:AddUserToGroup , iam:ListPolicyVersions , iam:AttachGroupPolicy , iam:ListUsers , iam:ListAttachedGroupPolicies , iam:ListGroups , iam:GetGroupPolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion , iam:GetLoginProfile ], Resource : * } ] }","title":"IAM policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/Region_Restrict.html","text":"{ Version : 2012-10-17 , Statement : [ { Effect : Allow , Action : [ ec2:* , rds:* , s3:* ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : us-east-1 }} } ] }","title":"Region Restrict"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html","text":"Level 200: Pricing Models https://wellarchitectedlabs.com Introduction In a highly dynamic cloud environment it can be challenging to forecast your usage. This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework. Goals Perform a Reserved Instance analysis Filter and sort the recommendations Prerequisites An AWS Account Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#level-200-pricing-models","text":"https://wellarchitectedlabs.com","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#introduction","text":"In a highly dynamic cloud environment it can be challenging to forecast your usage. This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#goals","text":"Perform a Reserved Instance analysis Filter and sort the recommendations","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#prerequisites","text":"An AWS Account Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#best-practice-checklist","text":"[ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html","text":"Level 200: Pricing Models Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Rate this Lab 1. View an RI report We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : On the right select the filters: RI term 1 year, Payment Option (your preference), Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable. 2. Download and prepare the RI CSV files 1 - Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : 2 - If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec , in the same spreadsheet. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv 3 - Create a new column called RI ID to the left of the Recommendation column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type , Location , OS and Tenancy . On row 5 of the sample files, paste the formula below. If using your own files modify the row numbers. =CONCATENATE(C5,L5,M5,N5) 4 - Add a column in the 30Day worksheet to the right of the Recommendation column. This will be for the 7Day recommendations. Add a VLOOKUP formula to get the values from the 7Day worksheet, modify this formula for the number of rows you require (U column). Paste this on row 5 of the sample files, or modify if using your own: =VLOOKUP(T5,'7Day Rec'!T$4:U$48,2,FALSE) 5 - We will now create a Fully Paid Day column. This shows us how long it will take to pay off the full term of the RI, and will help to measure risk. The closer to 12months the fully paid day is, the higher the risk. This is because it takes longer to pay off the investment/commitment. The break even is the wrong measure, as it only shows how quickly you pay off the upfront component, and not the full amount. Paste the following formula into the last column z : =(R5+S5*12)/(R5/12+S5+W5) The formula is: (yearly RI cost) / (monthly on-demand cost) (Upfront cost + recurring monthly cost x 12) / (upfront cost/12 + recurring monthly cost + estimated monthly savings) 6 - Delete the following columns as they are not necessary: Recommendation Date , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Payment Option , Break Even Months . We now have the required data required to be able to analyze, and filter out the high risk and low return RIs. 3. Sort and filter the RI CSV files RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs. 3.1 Filter out low risk, and high return RIs 1 - To get the lowest risk, we sort by Fully Paid Day smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI's below are fully paid off in around 7months - so if they are used for 7 months - they have paid themselves off completely. 2 - We will separate the very low, low, and medium risk recommendations. Add in some empty lines between Fully Paid Day of 8, 10, and copy the header line across: 3 - We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings , largest to smallest : 4 - Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this: 3.2 Filter out usage patterns It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. 1 - If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: 2 - If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max : 3 - Minimum utilization required for an RI varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum utilization of 80%. While this is reflected through the Fully Paid Day (if utilization is low, Fully Paid would be very late), we'll double check filter out only the very high utilization. Highlight anything above 90% in green: 4 - Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: 5 - Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here: - Combined_EC2_RI_Rec.xls 3.3 Making recommendations We look at each of the risk categories as follows: 1 - Very low and low risk For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that are highlighted in the Average hourly usage and Projected RI Utilization , select a percentage of either the 30Day or 7Day column (which ever is lower). 2 - Medium risk From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above: Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully filtered and processed all the recommendations. You can now make low risk and high return recommendations that are suitable based on your ongoing usage patterns with high confidence. You can then take those recommendations, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible). 4. Teardown There are no resources or configuration items that are created during this workshop. 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#level-200-pricing-models","text":"","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#table-of-contents","text":"View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#1-view-an-ri-report","text":"We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : On the right select the filters: RI term 1 year, Payment Option (your preference), Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable.","title":"1. View an RI report"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#2-download-and-prepare-the-ri-csv-files","text":"1 - Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : 2 - If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec , in the same spreadsheet. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv 3 - Create a new column called RI ID to the left of the Recommendation column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type , Location , OS and Tenancy . On row 5 of the sample files, paste the formula below. If using your own files modify the row numbers. =CONCATENATE(C5,L5,M5,N5) 4 - Add a column in the 30Day worksheet to the right of the Recommendation column. This will be for the 7Day recommendations. Add a VLOOKUP formula to get the values from the 7Day worksheet, modify this formula for the number of rows you require (U column). Paste this on row 5 of the sample files, or modify if using your own: =VLOOKUP(T5,'7Day Rec'!T$4:U$48,2,FALSE) 5 - We will now create a Fully Paid Day column. This shows us how long it will take to pay off the full term of the RI, and will help to measure risk. The closer to 12months the fully paid day is, the higher the risk. This is because it takes longer to pay off the investment/commitment. The break even is the wrong measure, as it only shows how quickly you pay off the upfront component, and not the full amount. Paste the following formula into the last column z : =(R5+S5*12)/(R5/12+S5+W5) The formula is: (yearly RI cost) / (monthly on-demand cost) (Upfront cost + recurring monthly cost x 12) / (upfront cost/12 + recurring monthly cost + estimated monthly savings) 6 - Delete the following columns as they are not necessary: Recommendation Date , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Payment Option , Break Even Months . We now have the required data required to be able to analyze, and filter out the high risk and low return RIs.","title":"2. Download and prepare the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#3-sort-and-filter-the-ri-csv-files","text":"RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs.","title":"3. Sort and filter the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#31-filter-out-low-risk-and-high-return-ris","text":"1 - To get the lowest risk, we sort by Fully Paid Day smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI's below are fully paid off in around 7months - so if they are used for 7 months - they have paid themselves off completely. 2 - We will separate the very low, low, and medium risk recommendations. Add in some empty lines between Fully Paid Day of 8, 10, and copy the header line across: 3 - We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings , largest to smallest : 4 - Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this:","title":"3.1 Filter out low risk, and high return RIs"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#32-filter-out-usage-patterns","text":"It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. 1 - If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: 2 - If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max : 3 - Minimum utilization required for an RI varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum utilization of 80%. While this is reflected through the Fully Paid Day (if utilization is low, Fully Paid would be very late), we'll double check filter out only the very high utilization. Highlight anything above 90% in green: 4 - Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: 5 - Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here: - Combined_EC2_RI_Rec.xls","title":"3.2 Filter out usage patterns"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#33-making-recommendations","text":"We look at each of the risk categories as follows: 1 - Very low and low risk For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that are highlighted in the Average hourly usage and Projected RI Utilization , select a percentage of either the 30Day or 7Day column (which ever is lower). 2 - Medium risk From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above: Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully filtered and processed all the recommendations. You can now make low risk and high return recommendations that are suitable based on your ongoing usage patterns with high confidence. You can then take those recommendations, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible).","title":"3.3 Making recommendations"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#4-teardown","text":"There are no resources or configuration items that are created during this workshop.","title":"4. Teardown"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html","text":"Level 200: Cost and Usage Analysis https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Have usage that is tagged (preferred, not mandatory) Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#level-200-cost-and-usage-analysis","text":"https://wellarchitectedlabs.com","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#introduction","text":"This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#goals","text":"Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Have usage that is tagged (preferred, not mandatory) Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#best-practice-checklist","text":"[ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html","text":"Level 200: Cost and Usage Analysis Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down Rate this Lab 1. Verify your CUR files are being delivered We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. You will need to edit the SQL file with the correct name. Workshop.sql October 2018 Usage November 2018 Usage December 2018 Usage 2. Use AWS Glue to enable access to CUR files via Amazon Athena We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included. We will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. 1 - Go to the Glue console: 2 - Click on Get started if you have not used Glue before 3 - Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler : 4 - Enter a Crawler name and click Next : 5 - Select Select data stores , and click Next : 6 - Ensure you select Specified path in my account , and click the Folder icon : 7 - Select the bucket with the CUR files, and click Select : 8 - Enter the following exclude patterns (1 per line), and click Next : **.json, **.yml, **.sql, **.csv, **.gz, **.zip 9 - Click Next : 10 - Select Create an IAM role , enter a role name , and click Next : 11 - Click the Down arrow , and select a Daily Frequency: 12 - Enter in a Start Hour and Start Minute , then click Next : 13 - Click Add database : 14 - Enter a Database name , and click Create : 15 - Click Next : 16 - Review the crawler and click Finish : 17 - Select the checkbox next to the crawler, click Run crawler : 18 - You will see the Crawler was successful and created a table: 19 - Go to the Athena Console: 20 - Select the drop down arrow, and click on the new database: 21 - A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : 22 - You will see it execute the command MSCK REPAIR TABLE , and in the results it will add partitions to the metastore for each month that has a billing file: NOTE: If it did not add partitions, then there is an error and there will be no data. Check - The database name is correct the same as the SQL file - The folder names year and month are in S3 and the case matches - There are parquet files in each of the month folders 23 - We will now preview the data. Click on the 3 dot menu and select Preview table : 24 - It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: 25 - (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM workshopcur . workshop_c_u_r where line_item_usage_account_id = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL. 3. Cost and Usage analysis We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 3.1 What data is available in the CUR file? We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct line_item_line_item_description from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from workshopcur . workshop_c_u_r where line_item_line_item_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM workshopcur . workshop_c_u_r limit 10; 3.2 Top Costs To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select line_item_usage_account_id , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_account_id order by cost desc limit 10; Top10 Costs by Product: select line_item_product_code , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code order by cost desc limit 10; Top Costs by Line Item Description select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 OnDemand Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; 3.3 Tagging and Chargeback Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billng files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by resource_tags_user_cost_center , bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 3.4 Reserved Instance, On Demand and Spot Usage To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , sum( line_item_usage_amount ) as Usage, line_item_unblended_rate , sum( line_item_unblended_cost ) as Cost, line_item_line_item_description , pricing_public_on_demand_rate , sum( pricing_public_on_demand_cost ) as PublicCost from workshopcur . workshop_c_u_r where line_item_line_item_Type like '%DiscountedUsage%' group by bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , line_item_unblended_rate , line_item_line_item_description , pricing_public_on_demand_rate T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select line_item_usage_type , sum( line_item_usage_amount ) as usage, round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_usage_type like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select line_item_usage_type , round(sum( line_item_usage_amount ),2) as usage, round(sum( line_item_unblended_cost ),2) as cost, round(avg( line_item_unblended_cost / line_item_usage_amount ),4) as hourly_rate from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from workshopcur . workshop_c_u_r where length(reservation_reservation_a_r_n) 0 and reservation_unused_quantity 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc 4. Tear down Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab. 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#level-200-cost-and-usage-analysis","text":"","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#table-of-contents","text":"Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#1-verify-your-cur-files-are-being-delivered","text":"We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. You will need to edit the SQL file with the correct name. Workshop.sql October 2018 Usage November 2018 Usage December 2018 Usage","title":"1. Verify your CUR files are being delivered "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#2-use-aws-glue-to-enable-access-to-cur-files-via-amazon-athena","text":"We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included. We will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. 1 - Go to the Glue console: 2 - Click on Get started if you have not used Glue before 3 - Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler : 4 - Enter a Crawler name and click Next : 5 - Select Select data stores , and click Next : 6 - Ensure you select Specified path in my account , and click the Folder icon : 7 - Select the bucket with the CUR files, and click Select : 8 - Enter the following exclude patterns (1 per line), and click Next : **.json, **.yml, **.sql, **.csv, **.gz, **.zip 9 - Click Next : 10 - Select Create an IAM role , enter a role name , and click Next : 11 - Click the Down arrow , and select a Daily Frequency: 12 - Enter in a Start Hour and Start Minute , then click Next : 13 - Click Add database : 14 - Enter a Database name , and click Create : 15 - Click Next : 16 - Review the crawler and click Finish : 17 - Select the checkbox next to the crawler, click Run crawler : 18 - You will see the Crawler was successful and created a table: 19 - Go to the Athena Console: 20 - Select the drop down arrow, and click on the new database: 21 - A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : 22 - You will see it execute the command MSCK REPAIR TABLE , and in the results it will add partitions to the metastore for each month that has a billing file: NOTE: If it did not add partitions, then there is an error and there will be no data. Check - The database name is correct the same as the SQL file - The folder names year and month are in S3 and the case matches - There are parquet files in each of the month folders 23 - We will now preview the data. Click on the 3 dot menu and select Preview table : 24 - It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: 25 - (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM workshopcur . workshop_c_u_r where line_item_usage_account_id = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL.","title":"2. Use AWS Glue to enable access to CUR files via Amazon Athena "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#3-cost-and-usage-analysis","text":"We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018","title":"3. Cost and Usage analysis "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#31-what-data-is-available-in-the-cur-file","text":"We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct line_item_line_item_description from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from workshopcur . workshop_c_u_r where line_item_line_item_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM workshopcur . workshop_c_u_r limit 10;","title":"3.1 What data is available in the CUR file?"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#32-top-costs","text":"To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select line_item_usage_account_id , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_account_id order by cost desc limit 10; Top10 Costs by Product: select line_item_product_code , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code order by cost desc limit 10; Top Costs by Line Item Description select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10; Top EC2 OnDemand Costs select line_item_product_code , line_item_line_item_description , round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_product_code , line_item_line_item_description order by cost desc limit 10;","title":"3.2 Top Costs"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#33-tagging-and-chargeback","text":"Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billng files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by resource_tags_user_cost_center , bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description , round(sum(line_item_unblended_cost),2) as cost FROM workshopcur . workshop_c_u_r where length( resource_tags_user_cost_center ) = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by bill_payer_account_id , product_product_name , line_item_usage_type , line_item_line_item_description order by cost desc limit 20","title":"3.3 Tagging and Chargeback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#34-reserved-instance-on-demand-and-spot-usage","text":"To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , sum( line_item_usage_amount ) as Usage, line_item_unblended_rate , sum( line_item_unblended_cost ) as Cost, line_item_line_item_description , pricing_public_on_demand_rate , sum( pricing_public_on_demand_cost ) as PublicCost from workshopcur . workshop_c_u_r where line_item_line_item_Type like '%DiscountedUsage%' group by bill_payer_account_id , bill_billing_period_start_date , line_item_usage_account_id , reservation_reservation_a_r_n , line_item_product_code , line_item_usage_type , line_item_unblended_rate , line_item_line_item_description , pricing_public_on_demand_rate T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select line_item_usage_type , sum( line_item_usage_amount ) as usage, round(sum( line_item_unblended_cost ),2) as cost from workshopcur . workshop_c_u_r where line_item_usage_type like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select line_item_usage_type , round(sum( line_item_usage_amount ),2) as usage, round(sum( line_item_unblended_cost ),2) as cost, round(avg( line_item_unblended_cost / line_item_usage_amount ),4) as hourly_rate from workshopcur . workshop_c_u_r where line_item_product_code like '%AmazonEC2%' and line_item_usage_type like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by line_item_usage_type order by line_item_usage_type Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from workshopcur . workshop_c_u_r where length(reservation_reservation_a_r_n) 0 and reservation_unused_quantity 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc","title":"3.4 Reserved Instance, On Demand and Spot Usage"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#4-tear-down","text":"Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab.","title":"4. Tear down "},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html","text":"Level 200: Cost Visualization https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage Prerequisites A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#level-200-cost-visualization","text":"https://wellarchitectedlabs.com","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#introduction","text":"This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#goals","text":"Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#prerequisites","text":"A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#best-practice-checklist","text":"[ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html","text":"Level 200: Cost Visualization Authors Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Rate this Lab 1. Setup Amazon QuickSight The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console: 1.1 Setup QuickSight for the first time If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left: 2. Create a data set We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data. 3. Create visualizations We will now start to visualize our costs and usage, and create a dashboard. 3.1 Cost by account and product The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter : 3.2 Elasticity The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made. 3.3 Cost by line item description The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible. 3.4 Dashboard Complete Your dashboard is now complete, you should have a similar dashboard to below: 4. Share your Analysis and Dashboard Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set. 4.1 Share an analysis To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.: 4.2 Publish a dashboard To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email: 5. Tear down It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution. 5.1 Cancel your QuickSight subscription Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe : 6. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#level-200-cost-visualization","text":"","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#authors","text":"Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#table-of-contents","text":"Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#1-setup-amazon-quicksight","text":"The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console:","title":"1. Setup Amazon QuickSight "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#11-setup-quicksight-for-the-first-time","text":"If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left:","title":"1.1 Setup QuickSight for the first time"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#2-create-a-data-set","text":"We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data.","title":"2. Create a data set "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#3-create-visualizations","text":"We will now start to visualize our costs and usage, and create a dashboard.","title":"3. Create visualizations "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#31-cost-by-account-and-product","text":"The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter :","title":"3.1 Cost by account and product"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#32-elasticity","text":"The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made.","title":"3.2 Elasticity"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#33-cost-by-line-item-description","text":"The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible.","title":"3.3 Cost by line item description"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#34-dashboard-complete","text":"Your dashboard is now complete, you should have a similar dashboard to below:","title":"3.4 Dashboard Complete"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#4-share-your-analysis-and-dashboard","text":"Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set.","title":"4. Share your Analysis and Dashboard "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#41-share-an-analysis","text":"To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.:","title":"4.1 Share an analysis"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#42-publish-a-dashboard","text":"To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email:","title":"4.2 Publish a dashboard"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#5-tear-down","text":"It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution.","title":"5. Tear down "},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#51-cancel-your-quicksight-subscription","text":"Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe :","title":"5.1 Cancel your QuickSight subscription"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#6-rate-this-lab","text":"","title":"6. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html","text":"Level 300: Automated CUR Updates and Ingestion https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework. Goals Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multiple Cost and Usage Reports in the same bucket Prerequisites An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features Start the Lab! Best Practice Checklist [ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multiple CURs in AWS Glue/Athena License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#level-300-automated-cur-updates-and-ingestion","text":"https://wellarchitectedlabs.com","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#introduction","text":"This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#goals","text":"Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multiple Cost and Usage Reports in the same bucket","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#prerequisites","text":"An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#best-practice-checklist","text":"[ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multiple CURs in AWS Glue/Athena","title":"Best Practice Checklist"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html","text":"Level 300: Automated CUR Updates and Ingestion Authors Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create the CloudFormation stack Multiple CURs Tear down Rate this Lab 1. Create the CloudFormation Stack This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports. 2. Multiple CURs This step is used when there are multiple CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: bucket name / prefix / report_name / Configuration: bucket name /DailyCUR/daily/ bucket name /HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3::: bucket name /DailyCUR/daily/daily* Resource: arn:aws:s3::: bucket name * Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3:// bucket name /DailyCUR/daily/daily' Path: 's3:// bucket name ' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3:// bucket name /DailyCUR/daily/cost_and_usage_data_status/' Location: 's3:// bucket name /cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashboard and verify that there is a single database, containing multiple tables: 3. Tear down Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack : 4. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#level-300-automated-cur-updates-and-ingestion","text":"","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#table-of-contents","text":"Create the CloudFormation stack Multiple CURs Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#1-create-the-cloudformation-stack","text":"This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports.","title":"1. Create the CloudFormation Stack"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#2-multiple-curs","text":"This step is used when there are multiple CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: bucket name / prefix / report_name / Configuration: bucket name /DailyCUR/daily/ bucket name /HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3::: bucket name /DailyCUR/daily/daily* Resource: arn:aws:s3::: bucket name * Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3:// bucket name /DailyCUR/daily/daily' Path: 's3:// bucket name ' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3:// bucket name /DailyCUR/daily/cost_and_usage_data_status/' Location: 's3:// bucket name /cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashboard and verify that there is a single database, containing multiple tables:","title":"2. Multiple CURs"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#3-tear-down","text":"Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack :","title":"3. Tear down"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#4-rate-this-lab","text":"","title":"4. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html","text":"Level 300: Multi Account CUR Access https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. This will ensure that all users and business units throughout your organization can access their cost and usage information, critical to ensuring they can track and further optimize their cost. Goals Allow users in another account (either a member/linked or another master/payer) account, access to the (full) payer account cost and usage data Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required IAM - create role, users, groups, and policies Modify S3 Bucket Policies Modify an existing Lambda function Upload a file to your S3 billing bucket Start the Lab! Best Practice Checklist [ ] Create a role to allow cross account least privilege access to resouces [ ] Serverless automated access configuration License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#level-300-multi-account-cur-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Multi Account CUR Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#introduction","text":"This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. This will ensure that all users and business units throughout your organization can access their cost and usage information, critical to ensuring they can track and further optimize their cost.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#goals","text":"Allow users in another account (either a member/linked or another master/payer) account, access to the (full) payer account cost and usage data","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#prerequisites","text":"Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#permissions-required","text":"IAM - create role, users, groups, and policies Modify S3 Bucket Policies Modify an existing Lambda function Upload a file to your S3 billing bucket","title":"Permissions required"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#best-practice-checklist","text":"[ ] Create a role to allow cross account least privilege access to resouces [ ] Serverless automated access configuration","title":"Best Practice Checklist"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html","text":"Level 300: Multi Account CUR Access https://wellarchitectedlabs.com Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Access Master/Payer via a Role Access Master/Payer via a User Use Athena to access a CUR in Master/Payer Tear down Rate this Lab 1. Access Master/Payer via a Role To provide access to the Cost and Usage data in the master/payer account to another account, you can create a role for the users in that account to assume. The users then have access to the data, in the same way a user in the master/payer account would. This is an ideal solution when you want to provide access to the data by allowing them to use only specific services in your account (Athena/Glue), and requires minimum coding and complexity. 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create a new policy Athena_List_Read with the required permissions. A sample policy that can be used as a starting point is here: ./Code/IAM_Athena It provides: Athena: List, Read, and Start Query (write) access to all Athena resources; Glue: Read access to all resources; S3: Read access to the bucket containing the Cost and Usage reports; S3: List, Read, Write access to the bucket containing Athena query results. NOTE: You must modify this policy in line with security best practices (least privilege) before implementation. Next we will create a role Sub_Acct_Athena and attach the newly created policy. 3 - From the IAM dashboard, create a role for Another AWS Account , enter in the Sub-Accounts Account ID : 4 - Attach the policy Athena_List_Read 5 - Add any required tags 6 - Enter a Role name and Role description , review and create the role: 7 - The completed role should be similar to this: 8 - Logout from your master/payer account, then login to the other account (which will access your master/payer CUR file). 9 - From the IAM console check the users have permissions to assume a role, the IAM policy they require is: NOTE: Replace (Account ID) and (Role name) inside the brackets { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : sts:AssumeRole , Resource : arn:aws:iam::(Account ID):role/(Role name) } ] } NOTE: Replace (Account ID) and (Role name) inside the brackets 10 - The users in the member/linked account can then assume the role. They will then be able to access Athena in the payer/linked account. You have successfully allowed users in another account to get access to your CUR. They assume the role in the master/payer account, and can use Athena to query the CUR just as a user in the master/payer account would. 2. Access Master/Payer via a User You can also provide access to the Cost and Usage information in the payer account by giving people a login to the payer account and assigning the required permissions. This is against security best practices, as roles and centralized federation should be utilized. If there is a specific reason you need to implement users for this, follow the following procedure: 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create the policy Athena_List_Read as defined above 3 - Create a user and group, and assign them the IAM policy 4 - Users can now login to the primary account and have access to Athena 3. Use Athena to access a CUR in Master/Payer You can use Athena in your member/linked account or an account outside your organization to access the CUR in your master/payer account. This requires cross account S3 access, and a permissions change on the delivered CUR files. This is an ideal solution when you want to make the data available to another account via S3, that account can then use any services within their own account to access the data. This solution provides additional flexibility to the other account, and can be extended with additional features later. NOTE: We assume you have completed the lab 300_Automated_CUR_Updates_and_Ingestion , which creates a Lambda function and puts an S3 Event on your billing bucket, so we will extend this existing solution. First we will add the s3 bucket permissions allowing the member/linked account access. 1 - Login to the Master/Payer account AWS console as an IAM user with the required permissions, navigate to the S3 Dashboard : 2 - Select your bucket containing the CUR reports 3 - Click on Permissions 4 - Click on Bucket Policy 5 - Edit the following lines and add them to the bucket policy. Change [Sub-Account ID] and [S3 Bucket Name] to your member/linked account and S3 bucket: { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } 6 - Save the new bucket policy. 7 - A sample complete policy is here: ./Code/S3_Bucket_Policy Finally, we will update the Cloudformation stack with the code, this will ensure that a new permissions ACL is written on all delivered CUR files - so that newly delivered CURs will be accessible to the other account. 8 - Open the following modified CloudFormation template in a new window: ./Code/crawler-cfn.md 9 - Open the CloudFormation dashboard, select the CUR-Update stack and view the current template: 10 - Copy the current template into a text editor, save this template for rollback. 11 - Make the changes required as per below, and save the new yml file. - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' const util = require('util'); // Read options from the event. console.log( Reading options from event:\\n , util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 12 - In the CloudFormation console under Stack actions , Create change set for current stack , Replace current template , and upload the edited file. 13 - Execute the change set: 14 - You can verify the changes to the stack by viewing the IAM console - CUR-Update-AWSCURCrawlerLambdaExecutor role, and the Lambda console - function CUR-Update-AWSCURInitializer , ensure they contain the changes you made. 15 - We will now test the function. Upload a file to the CUR billing bucket, preferably next to the CUR files for the current month. 16 - Click on the file and you should see that the bucket owner is the owner, and it has multiple Grantees for read: 17 - Click on the permissions and you can confirm the bucket owner has full access, and the required Canonical IDs also have read access, then delete this test file : At this point the other account will have the required access to any NEW CUR files delivered. You will need to modify any previous months billing files and ensure they have the correct permissions. NOTE: If you do not modify previous CUR file permissions, Athena queries from the member/linked account will not work. To change previous CUR files you can use S3 batch operations: (NOTE: the inventory will take time to generate to be able to complete this step) Go to the S3 Console Create a destination bucket for the inventory file Go to the CUR folder, Select Management , select Inventory , then create an inventory to produce a CSV file of all files. Wait until the inventory populates Go to the S3 console, select Batch operations , create job Select the inventory file Choose to replace the ACL (Get this from the Lambda function) Check you have the policy and trust to run the job Start the job Confirm the old CURs are updated The other account will now need to create the tables in Athena, and also update these tables when new versions and new months are delivered. We will do this with a recurring Glue crawler. 18 - Login to the other account as an IAM user with the required permissions, and go into the Glue console . 19 - Add a Crawler with the following details: - Include path : the S3 bucket in the account with the delivered CURs - Exclude patterns (1 per line): **.json, **.yml, **.sql, **.csv, **.gz, **.zip 20 - Create a daily schedule to update the tables each morning before you come into work NOTE: CUR files are updated at least every day, a crawler with a daily schedule is a simple code-free solution for this. 21 - Run the crawler, and check that it has created the database, the tables, and that the tables contain data. 26 - Open up Athena execute a query to verify access: You have now given the sub account access to the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered. 4. Tear down Execute either of these steps depending on the implementation you chose above. 4.1 Access Master/Payer via a Role 1 - Go to the IAM Dashboard , delete the role Sub_Acct_Athena 2 - Delete the policy Athena_List_Read 4.2 Access Master/Payer via a User 1 - Go to the IAM Dashboard , delete the group, delete the user 2 - Delete the policy Athena_List_Read 4.3 Use Athena to access a CUR in Master/Payer 1 - Go to the CloudFormation Dashboard 2 - Update the stack and implement the original yml file. If you didn't save this, it will be in the S3 bucket that contains the CUR files. 3 - If you change the permissions ACL on the old CUR files, follow the same process - but remove the member/linked account from the ACL. 4 - Login to the member/linked account, go to the Glue Dashboard 5 - Delete the database that was created 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#level-300-multi-account-cur-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Multi Account CUR Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#table-of-contents","text":"Access Master/Payer via a Role Access Master/Payer via a User Use Athena to access a CUR in Master/Payer Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#1-access-masterpayer-via-a-role","text":"To provide access to the Cost and Usage data in the master/payer account to another account, you can create a role for the users in that account to assume. The users then have access to the data, in the same way a user in the master/payer account would. This is an ideal solution when you want to provide access to the data by allowing them to use only specific services in your account (Athena/Glue), and requires minimum coding and complexity. 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create a new policy Athena_List_Read with the required permissions. A sample policy that can be used as a starting point is here: ./Code/IAM_Athena It provides: Athena: List, Read, and Start Query (write) access to all Athena resources; Glue: Read access to all resources; S3: Read access to the bucket containing the Cost and Usage reports; S3: List, Read, Write access to the bucket containing Athena query results. NOTE: You must modify this policy in line with security best practices (least privilege) before implementation. Next we will create a role Sub_Acct_Athena and attach the newly created policy. 3 - From the IAM dashboard, create a role for Another AWS Account , enter in the Sub-Accounts Account ID : 4 - Attach the policy Athena_List_Read 5 - Add any required tags 6 - Enter a Role name and Role description , review and create the role: 7 - The completed role should be similar to this: 8 - Logout from your master/payer account, then login to the other account (which will access your master/payer CUR file). 9 - From the IAM console check the users have permissions to assume a role, the IAM policy they require is: NOTE: Replace (Account ID) and (Role name) inside the brackets { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : sts:AssumeRole , Resource : arn:aws:iam::(Account ID):role/(Role name) } ] } NOTE: Replace (Account ID) and (Role name) inside the brackets 10 - The users in the member/linked account can then assume the role. They will then be able to access Athena in the payer/linked account. You have successfully allowed users in another account to get access to your CUR. They assume the role in the master/payer account, and can use Athena to query the CUR just as a user in the master/payer account would.","title":"1. Access Master/Payer via a Role  "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#2-access-masterpayer-via-a-user","text":"You can also provide access to the Cost and Usage information in the payer account by giving people a login to the payer account and assigning the required permissions. This is against security best practices, as roles and centralized federation should be utilized. If there is a specific reason you need to implement users for this, follow the following procedure: 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create the policy Athena_List_Read as defined above 3 - Create a user and group, and assign them the IAM policy 4 - Users can now login to the primary account and have access to Athena","title":"2. Access Master/Payer via a User "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#3-use-athena-to-access-a-cur-in-masterpayer","text":"You can use Athena in your member/linked account or an account outside your organization to access the CUR in your master/payer account. This requires cross account S3 access, and a permissions change on the delivered CUR files. This is an ideal solution when you want to make the data available to another account via S3, that account can then use any services within their own account to access the data. This solution provides additional flexibility to the other account, and can be extended with additional features later. NOTE: We assume you have completed the lab 300_Automated_CUR_Updates_and_Ingestion , which creates a Lambda function and puts an S3 Event on your billing bucket, so we will extend this existing solution. First we will add the s3 bucket permissions allowing the member/linked account access. 1 - Login to the Master/Payer account AWS console as an IAM user with the required permissions, navigate to the S3 Dashboard : 2 - Select your bucket containing the CUR reports 3 - Click on Permissions 4 - Click on Bucket Policy 5 - Edit the following lines and add them to the bucket policy. Change [Sub-Account ID] and [S3 Bucket Name] to your member/linked account and S3 bucket: { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } 6 - Save the new bucket policy. 7 - A sample complete policy is here: ./Code/S3_Bucket_Policy Finally, we will update the Cloudformation stack with the code, this will ensure that a new permissions ACL is written on all delivered CUR files - so that newly delivered CURs will be accessible to the other account. 8 - Open the following modified CloudFormation template in a new window: ./Code/crawler-cfn.md 9 - Open the CloudFormation dashboard, select the CUR-Update stack and view the current template: 10 - Copy the current template into a text editor, save this template for rollback. 11 - Make the changes required as per below, and save the new yml file. - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' const util = require('util'); // Read options from the event. console.log( Reading options from event:\\n , util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 12 - In the CloudFormation console under Stack actions , Create change set for current stack , Replace current template , and upload the edited file. 13 - Execute the change set: 14 - You can verify the changes to the stack by viewing the IAM console - CUR-Update-AWSCURCrawlerLambdaExecutor role, and the Lambda console - function CUR-Update-AWSCURInitializer , ensure they contain the changes you made. 15 - We will now test the function. Upload a file to the CUR billing bucket, preferably next to the CUR files for the current month. 16 - Click on the file and you should see that the bucket owner is the owner, and it has multiple Grantees for read: 17 - Click on the permissions and you can confirm the bucket owner has full access, and the required Canonical IDs also have read access, then delete this test file : At this point the other account will have the required access to any NEW CUR files delivered. You will need to modify any previous months billing files and ensure they have the correct permissions. NOTE: If you do not modify previous CUR file permissions, Athena queries from the member/linked account will not work. To change previous CUR files you can use S3 batch operations: (NOTE: the inventory will take time to generate to be able to complete this step) Go to the S3 Console Create a destination bucket for the inventory file Go to the CUR folder, Select Management , select Inventory , then create an inventory to produce a CSV file of all files. Wait until the inventory populates Go to the S3 console, select Batch operations , create job Select the inventory file Choose to replace the ACL (Get this from the Lambda function) Check you have the policy and trust to run the job Start the job Confirm the old CURs are updated The other account will now need to create the tables in Athena, and also update these tables when new versions and new months are delivered. We will do this with a recurring Glue crawler. 18 - Login to the other account as an IAM user with the required permissions, and go into the Glue console . 19 - Add a Crawler with the following details: - Include path : the S3 bucket in the account with the delivered CURs - Exclude patterns (1 per line): **.json, **.yml, **.sql, **.csv, **.gz, **.zip 20 - Create a daily schedule to update the tables each morning before you come into work NOTE: CUR files are updated at least every day, a crawler with a daily schedule is a simple code-free solution for this. 21 - Run the crawler, and check that it has created the database, the tables, and that the tables contain data. 26 - Open up Athena execute a query to verify access: You have now given the sub account access to the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.","title":"3. Use Athena to access a CUR in Master/Payer "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#4-tear-down","text":"Execute either of these steps depending on the implementation you chose above.","title":"4. Tear down "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#41-access-masterpayer-via-a-role","text":"1 - Go to the IAM Dashboard , delete the role Sub_Acct_Athena 2 - Delete the policy Athena_List_Read","title":"4.1 Access Master/Payer via a Role"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#42-access-masterpayer-via-a-user","text":"1 - Go to the IAM Dashboard , delete the group, delete the user 2 - Delete the policy Athena_List_Read","title":"4.2 Access Master/Payer via a User"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#43-use-athena-to-access-a-cur-in-masterpayer","text":"1 - Go to the CloudFormation Dashboard 2 - Update the stack and implement the original yml file. If you didn't save this, it will be in the S3 bucket that contains the CUR files. 3 - If you change the permissions ACL on the old CUR files, follow the same process - but remove the member/linked account from the ACL. 4 - Login to the member/linked account, go to the Glue Dashboard 5 - Delete the database that was created","title":"4.3 Use Athena to access a CUR in Master/Payer"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/IAM_Athena.html","text":"IAM policy for access to Athena NOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use. { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ athena:StartQueryExecution , glue:GetCrawler , glue:GetDataCatalogEncryptionSettings , glue:GetTableVersions , glue:GetPartitions , athena:GetQueryResults , athena:ListWorkGroups , athena:GetNamedQuery , glue:GetDevEndpoint , glue:GetSecurityConfiguration , glue:GetResourcePolicy , glue:GetTrigger , glue:GetUserDefinedFunction , athena:GetExecutionEngine , glue:GetJobRun , athena:GetExecutionEngines , s3:HeadBucket , glue:GetUserDefinedFunctions , glue:GetClassifier , s3:PutAccountPublicAccessBlock , athena:GetQueryResultsStream , glue:GetJobs , glue:GetTables , glue:GetTriggers , athena:GetNamespace , athena:GetQueryExecutions , athena:GetCatalogs , athena:ListNamedQueries , athena:GetNamespaces , glue:GetPartition , glue:GetDevEndpoints , athena:GetTables , athena:GetTable , athena:BatchGetNamedQuery , athena:BatchGetQueryExecution , glue:GetJob , glue:GetConnections , glue:GetCrawlers , glue:GetClassifiers , athena:ListQueryExecutions , glue:GetCatalogImportStatus , athena:GetWorkGroup , glue:GetConnection , glue:BatchGetPartition , glue:GetSecurityConfigurations , glue:GetDatabases , athena:ListTagsForResource , glue:GetTable , glue:GetDatabase , s3:GetAccountPublicAccessBlock , glue:GetDataflowGraph , s3:ListAllMyBuckets , athena:GetQueryExecution , glue:GetPlan , glue:GetCrawlerMetrics , glue:GetJobRuns ], Resource : * }, { Sid : VisualEditor1 , Effect : Allow , Action : [ s3:PutObject , s3:GetObject , s3:ListBucketMultipartUploads , s3:AbortMultipartUpload , s3:CreateBucket , s3:ListBucket , s3:GetBucketLocation , s3:ListMultipartUploadParts ], Resource : [ arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1 , arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1/* ] }, { Sid : VisualEditor2 , Effect : Allow , Action : [ s3:ListBucketByTags , s3:GetLifecycleConfiguration , s3:GetBucketTagging , s3:GetInventoryConfiguration , s3:GetObjectVersionTagging , s3:ListBucketVersions , s3:GetBucketLogging , s3:ListBucket , s3:GetAccelerateConfiguration , s3:GetBucketPolicy , s3:GetObjectVersionTorrent , s3:GetObjectAcl , s3:GetEncryptionConfiguration , s3:GetBucketRequestPayment , s3:GetObjectVersionAcl , s3:GetObjectTagging , s3:GetMetricsConfiguration , s3:GetBucketPublicAccessBlock , s3:GetBucketPolicyStatus , s3:ListBucketMultipartUploads , s3:GetBucketWebsite , s3:GetBucketVersioning , s3:GetBucketAcl , s3:GetBucketNotification , s3:GetReplicationConfiguration , s3:ListMultipartUploadParts , s3:GetObject , s3:GetObjectTorrent , s3:GetBucketCORS , s3:GetAnalyticsConfiguration , s3:GetObjectVersionForReplication , s3:GetBucketLocation , s3:GetObjectVersion ], Resource : [ arn:aws:s3::: S3 CUR Bucket /* , arn:aws:s3::: S3 CUR Bucket ] } ] }","title":"IAM Athena"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/S3_Bucket_Policy.html","text":"Bucket policy for member/linked account access to CUR files NOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name. { Version : 2008-10-17 , Id : Policy1335892530063 , Statement : [ { Sid : Stmt1335892150622 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : [ s3:GetBucketAcl , s3:GetBucketPolicy ], Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1335892526596 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : s3:PutObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* }, { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } ] }","title":"S3 Bucket Policy"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/crawler-cfn.html","text":"Below is a copy of the crawler config file. Modifications are between ' * ' characters. Variables that need to be changed below: (CUR Billing Bucket) (name): the account name of the Payer containing the CUR, this is the email excluding @companyname.com (name2): the account name of the linked account accessing the CUR (Canonical ID): the Canonical User ID for the Payer containing the CUR, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html (Canonical ID2): the Canonical User ID for the linked account accessing the CUR AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: 'athenacurcfn_workshop_c_u_r' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3:::(CUR Billing Bucket)* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' ***** - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' ***** AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3://(CUR Billing Bucket)' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); ***** const util = require('util'); ***** exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { ***** // Read options from the event. console.log( Reading options from event:\\n , util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3:::(CUR Billing Bucket)' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3:::(CUR Billing Bucket)' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: '(CUR Billing Bucket)' ReportKey: 'cur/WorkshopCUR/WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3://(CUR Billing Bucket)/cur/WorkshopCUR/cost_and_usage_data_status/'","title":"Crawler cfn"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html","text":"Level 300: Splitting the CUR and Sharing Access https://wellarchitectedlabs.com Introduction This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information. Common use cases are: Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement: Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script Goals Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#level-300-splitting-the-cur-and-sharing-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Splitting the CUR and Sharing Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#introduction","text":"This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information. Common use cases are: Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement: Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#goals","text":"Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#prerequisites","text":"Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#permissions-required","text":"Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler","title":"Permissions required"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html","text":"Level 300: Splitting the CUR and Sharing Access https://wellarchitectedlabs.com Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear down Rate this Lab 1. Setup Output S3 Bucket We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We'll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered. So what we'll do is as follows: Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the master/payer account as an IAM user with the required permissions. 2 - Go to the S3 console 3 - Create the output S3 bucket 4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account. 5 - Go to Permissions , and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point: { Version : 2012-10-17 , Statement : [ { Sid : AllowListingOfFolders , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:ListBucket , Resource : arn:aws:s3:::(bucket) }, { Sid : AllowAllS3ActionsInSubFolder , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:* , Resource : arn:aws:s3:::(bucket)/(folder)/* } ] } 6 - Go to the IAM Dashboard 7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point: NOTE: replace (bucket name): { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:PutObjectVersionAcl , s3:PutObjectAcl ], Resource : arn:aws:s3:::(bucket name)/* } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL 9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard 11 - Create the lambda function S3LinkedPutACL with the following details: Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard 13 - Select the Output Bucket , go to Properties , and add an S3 event to trigger on All object create events , and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account. 2. Perform one off Fill of Member/Linked Data Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data. 1 - In the master/payer account go into the Athena service dashboard 2 - Create your query using the template below: The following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month , and output it to the S3 output folder. CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM (database) . (table) where line_item_usage_account_id like '(account ID)' Some key points for your queries: Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb: Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard 5 - Go to the output bucket and folder 6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees : 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data) DROP TABLE (database).temp_table 3. Create Athena Saved Queries to Write new Data Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month. You must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code): The queries MUST start with: create_linked_ and delete_linked_ otherwise you'll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1 , as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_(folder name) , the following sample code is the accompanying query for the previous query above: CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM (database) . (table) where line_item_usage_account_id like '(some value)' and CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') 2 - Create the accompanying delete statement named delete_linked_(folder name) to delete the temporary table: drop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required. 4. Create Lambda function to run the Saved Queries This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself. 1 - Go to the IAM service dashboard 2 - Create a policy named LambdaSubAcctSplit 3 - Edit the following policy inline with security best practices, and add it to the policy: ./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services 5 - Attach the LambdaSubAcctSplit policy 6 - Name the role LambdaSubAcctSplit 7 - Go into the Lambda service dashboard 8 - Create a function named SubAcctSplit , Author from scratch using the Python 3.7 Runtime: 9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top, you will need to change the arrays and Athena variable. 11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end 12 - Change the Execution role to LambdaSubAcctSplit 13 - Save the function 14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test. You have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered. 5. Trigger the Lambda When a CUR is Delivered It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function. 1 - Go to the CloudFormation service Dashboard 2 - Select the current stack which updates the Glue database 3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required) 4 - Open the template up in a text editor of your choice 5 - A sample crawler file is below: ./Code/crawler-cfn.md 6 - Update the AWSCURCrawlerLambdaExecutor IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section: Add the following Action: 'lambda:InvokeFunction' Edit the following line , and add the following resource - 'arn:aws:lambda: region : accountID :function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section: var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file 9 - In the CloudFormation console update the stack 10 - Replace the current template with the new one, and upload your modified template 11 - After the stack has successfully updated, you can test the function 12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original master/payer CUR file 13 - Download the CUR file, and delete the object from the bucket 14 - Re-upload the current CUR file back into its bucket 15 - Navigate to the output bucket and folder for the current month 16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions Setup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions. 6. Sub Account Crawler Setup The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database. 1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console . 2 - Add a Crawler with the following details: Include path : the S3 bucket in the account with the delivered CURs Exclude patterns : **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database 6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables. 8 - Go into Athena and execute a preview query to verify access and the data. You have now given the sub account access to their specific CUR files as extracted from the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered. 7. Tear Down We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab. 7.1 Sub Account 1 - Log into the sub account as an IAM user with the required privileges 2 - Go to the Glue service dashboard 3 - Delete the created database and tables 4 - Delete the recurring Glue crawler 7.2 Master/Payer Account 1 - Log into the master/payer account as an IAM user with the required privileges 2 - Go to the Cloudformation service dashboard, and select the CUR update stack 3 - Update the stack and use the original Template yml file 4 - Go to the Lambda service dashboard 5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions 6 - Go to the IAM service dashboard 7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles 8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies 9 - Go to the Athena service dashboard 10 - Delete the create_linked_ and delete_linked_ Athena saved queries 11 - Delete any temp tables 12 - Go into the S3 service dashboard 13 - Delete the S3 output folder 8. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#level-300-splitting-the-cur-and-sharing-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Splitting the CUR and Sharing Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#table-of-contents","text":"Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#1-setup-output-s3-bucket","text":"We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We'll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered. So what we'll do is as follows: Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the master/payer account as an IAM user with the required permissions. 2 - Go to the S3 console 3 - Create the output S3 bucket 4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account. 5 - Go to Permissions , and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point: { Version : 2012-10-17 , Statement : [ { Sid : AllowListingOfFolders , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:ListBucket , Resource : arn:aws:s3:::(bucket) }, { Sid : AllowAllS3ActionsInSubFolder , Effect : Allow , Principal : { AWS : arn:aws:iam::(account ID):root }, Action : s3:* , Resource : arn:aws:s3:::(bucket)/(folder)/* } ] } 6 - Go to the IAM Dashboard 7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point: NOTE: replace (bucket name): { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ s3:PutObjectVersionAcl , s3:PutObjectAcl ], Resource : arn:aws:s3:::(bucket name)/* } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL 9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard 11 - Create the lambda function S3LinkedPutACL with the following details: Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard 13 - Select the Output Bucket , go to Properties , and add an S3 event to trigger on All object create events , and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account.","title":"1. Setup Output S3 Bucket "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#2-perform-one-off-fill-of-memberlinked-data","text":"Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data. 1 - In the master/payer account go into the Athena service dashboard 2 - Create your query using the template below: The following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month , and output it to the S3 output folder. CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM (database) . (table) where line_item_usage_account_id like '(account ID)' Some key points for your queries: Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb: Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard 5 - Go to the output bucket and folder 6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees : 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data) DROP TABLE (database).temp_table","title":"2. Perform one off Fill of Member/Linked Data "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#3-create-athena-saved-queries-to-write-new-data","text":"Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month. You must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code): The queries MUST start with: create_linked_ and delete_linked_ otherwise you'll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1 , as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_(folder name) , the following sample code is the accompanying query for the previous query above: CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM (database) . (table) where line_item_usage_account_id like '(some value)' and CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') 2 - Create the accompanying delete statement named delete_linked_(folder name) to delete the temporary table: drop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required.","title":"3. Create Athena Saved Queries to Write new Data "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#4-create-lambda-function-to-run-the-saved-queries","text":"This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself. 1 - Go to the IAM service dashboard 2 - Create a policy named LambdaSubAcctSplit 3 - Edit the following policy inline with security best practices, and add it to the policy: ./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services 5 - Attach the LambdaSubAcctSplit policy 6 - Name the role LambdaSubAcctSplit 7 - Go into the Lambda service dashboard 8 - Create a function named SubAcctSplit , Author from scratch using the Python 3.7 Runtime: 9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top, you will need to change the arrays and Athena variable. 11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end 12 - Change the Execution role to LambdaSubAcctSplit 13 - Save the function 14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test. You have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered.","title":"4. Create Lambda function to run the Saved Queries "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#5-trigger-the-lambda-when-a-cur-is-delivered","text":"It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function. 1 - Go to the CloudFormation service Dashboard 2 - Select the current stack which updates the Glue database 3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required) 4 - Open the template up in a text editor of your choice 5 - A sample crawler file is below: ./Code/crawler-cfn.md 6 - Update the AWSCURCrawlerLambdaExecutor IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section: Add the following Action: 'lambda:InvokeFunction' Edit the following line , and add the following resource - 'arn:aws:lambda: region : accountID :function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section: var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file 9 - In the CloudFormation console update the stack 10 - Replace the current template with the new one, and upload your modified template 11 - After the stack has successfully updated, you can test the function 12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original master/payer CUR file 13 - Download the CUR file, and delete the object from the bucket 14 - Re-upload the current CUR file back into its bucket 15 - Navigate to the output bucket and folder for the current month 16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions Setup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions.","title":"5. Trigger the Lambda When a CUR is Delivered "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#6-sub-account-crawler-setup","text":"The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database. 1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console . 2 - Add a Crawler with the following details: Include path : the S3 bucket in the account with the delivered CURs Exclude patterns : **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database 6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables. 8 - Go into Athena and execute a preview query to verify access and the data. You have now given the sub account access to their specific CUR files as extracted from the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.","title":"6. Sub Account Crawler Setup "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#7-tear-down","text":"We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab.","title":"7. Tear Down "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#71-sub-account","text":"1 - Log into the sub account as an IAM user with the required privileges 2 - Go to the Glue service dashboard 3 - Delete the created database and tables 4 - Delete the recurring Glue crawler","title":"7.1 Sub Account"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#72-masterpayer-account","text":"1 - Log into the master/payer account as an IAM user with the required privileges 2 - Go to the Cloudformation service dashboard, and select the CUR update stack 3 - Update the stack and use the original Template yml file 4 - Go to the Lambda service dashboard 5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions 6 - Go to the IAM service dashboard 7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles 8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies 9 - Go to the Athena service dashboard 10 - Delete the create_linked_ and delete_linked_ Athena saved queries 11 - Delete any temp tables 12 - Go into the S3 service dashboard 13 - Delete the S3 output folder","title":"7.2 Master/Payer Account"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/IAM_Athena.html","text":"IAM policy for access to Athena NOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use. { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ athena:StartQueryExecution , glue:GetCrawler , glue:GetDataCatalogEncryptionSettings , glue:GetTableVersions , glue:GetPartitions , athena:GetQueryResults , athena:ListWorkGroups , athena:GetNamedQuery , glue:GetDevEndpoint , glue:GetSecurityConfiguration , glue:GetResourcePolicy , glue:GetTrigger , glue:GetUserDefinedFunction , athena:GetExecutionEngine , glue:GetJobRun , athena:GetExecutionEngines , s3:HeadBucket , glue:GetUserDefinedFunctions , glue:GetClassifier , s3:PutAccountPublicAccessBlock , athena:GetQueryResultsStream , glue:GetJobs , glue:GetTables , glue:GetTriggers , athena:GetNamespace , athena:GetQueryExecutions , athena:GetCatalogs , athena:ListNamedQueries , athena:GetNamespaces , glue:GetPartition , glue:GetDevEndpoints , athena:GetTables , athena:GetTable , athena:BatchGetNamedQuery , athena:BatchGetQueryExecution , glue:GetJob , glue:GetConnections , glue:GetCrawlers , glue:GetClassifiers , athena:ListQueryExecutions , glue:GetCatalogImportStatus , athena:GetWorkGroup , glue:GetConnection , glue:BatchGetPartition , glue:GetSecurityConfigurations , glue:GetDatabases , athena:ListTagsForResource , glue:GetTable , glue:GetDatabase , s3:GetAccountPublicAccessBlock , glue:GetDataflowGraph , s3:ListAllMyBuckets , athena:GetQueryExecution , glue:GetPlan , glue:GetCrawlerMetrics , glue:GetJobRuns ], Resource : * }, { Sid : VisualEditor1 , Effect : Allow , Action : [ s3:PutObject , s3:GetObject , s3:ListBucketMultipartUploads , s3:AbortMultipartUpload , s3:CreateBucket , s3:ListBucket , s3:GetBucketLocation , s3:ListMultipartUploadParts ], Resource : [ arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1 , arn:aws:s3:::aws-athena-query-results- Account ID -us-east-1/* ] }, { Sid : VisualEditor2 , Effect : Allow , Action : [ s3:ListBucketByTags , s3:GetLifecycleConfiguration , s3:GetBucketTagging , s3:GetInventoryConfiguration , s3:GetObjectVersionTagging , s3:ListBucketVersions , s3:GetBucketLogging , s3:ListBucket , s3:GetAccelerateConfiguration , s3:GetBucketPolicy , s3:GetObjectVersionTorrent , s3:GetObjectAcl , s3:GetEncryptionConfiguration , s3:GetBucketRequestPayment , s3:GetObjectVersionAcl , s3:GetObjectTagging , s3:GetMetricsConfiguration , s3:GetBucketPublicAccessBlock , s3:GetBucketPolicyStatus , s3:ListBucketMultipartUploads , s3:GetBucketWebsite , s3:GetBucketVersioning , s3:GetBucketAcl , s3:GetBucketNotification , s3:GetReplicationConfiguration , s3:ListMultipartUploadParts , s3:GetObject , s3:GetObjectTorrent , s3:GetBucketCORS , s3:GetAnalyticsConfiguration , s3:GetObjectVersionForReplication , s3:GetBucketLocation , s3:GetObjectVersion ], Resource : [ arn:aws:s3::: S3 CUR Bucket /* , arn:aws:s3::: S3 CUR Bucket ] } ] }","title":"IAM Athena"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/S3LinkedPutACL.html","text":"Here is the Lambda function to re-write object ACLs. It is triggered by an S3 Event, reads the folder from the object - and then applies the required object ACL: FULL_CONTROL for the owner, READ for the sub account. Edit the following fields in the code below: folder1 : The name of the folder where new files will be placed Owner Account Name : The owner account name - the account email without the @companyname, they will get FULL_CONTROL permissions Owner Canonical ID : The owner canonical ID, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html Sub Account Name : The sub account name - the account email without the @companyname, they will get READ permissions Sub Acct Canonical ID : The sub account canonical ID const AWS = require('aws-sdk'); const util = require('util'); // Permissions for the new objects // Key MUST match the top level folder // Format: owner account name - Canonical ID - sub account name - canonical ID // This will give owner full permission sub account read only permission var permissions = new Array(); var permissions = { ' folder1 ': [' owner acct name ',' Owner Canonical ID ',' sub account name ',' Sub Acct Canonical ID '], ' folder2 ': [' owner acct name ',' Owner Canonical ID ',' sub account name ',' Sub Acct Canonical ID '] }; // Main Loop exports.handler = function(event, context, callback) { // If its an object delete, do nothing if (event.RequestType === 'Delete') { } else // Its an object put { // Get the source bucket from the S3 event var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters, decode it var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, )); // Gets the top level folder, which is the key for the permissions array var folderID = srcKey.split( / )[0]; // Define the object permissions, using the permissions array var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][2], 'ID': permissions[folderID][3] }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); } };","title":"S3LinkedPutACL"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/S3_Bucket_Policy.html","text":"Bucket policy for member/linked account access to CUR files NOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name. { Version : 2008-10-17 , Id : Policy1335892530063 , Statement : [ { Sid : Stmt1335892150622 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : [ s3:GetBucketAcl , s3:GetBucketPolicy ], Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1335892526596 , Effect : Allow , Principal : { AWS : arn:aws:iam::386209384616:root }, Action : s3:PutObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* }, { Sid : Stmt1546900919345 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:ListBucket , Resource : arn:aws:s3:::[S3 Bucket Name] }, { Sid : Stmt1546901049588 , Effect : Allow , Principal : { AWS : arn:aws:iam::[Sub-Account ID]:root }, Action : s3:GetObject , Resource : arn:aws:s3:::[S3 Bucket Name]/* } ] }","title":"S3 Bucket Policy"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/SubAcctSplit_Role.html","text":"Review the policy below, and use it as a starting point to create your policy for the Lambda fuction. The following fields will need to be changed: Output bucket: The S3 bucket that will contain the output from the Athena queries Account ID: the master/payer account ID Source bucket: the location of the original CUR files in the master/payer { Version : 2012-10-17 , Statement : [ { Sid : VisualEditor0 , Effect : Allow , Action : [ athena:StartQueryExecution , s3:DeleteObjectVersion , athena:GetQueryResults , s3:ListBucket , athena:GetNamedQuery , logs:PutLogEvents , athena:ListQueryExecutions , athena:ListNamedQueries , s3:PutObject , s3:GetObject , logs:CreateLogStream , athena:GetQueryExecution , s3:DeleteObject ], Resource : [ arn:aws:s3:::(output bucket)/* , arn:aws:logs:us-east-1:(account ID):log-group:/aws/lambda/SubAcctSplit:* , arn:aws:athena:*:*:workgroup/* ] }, { Sid : VisualEditor1 , Effect : Allow , Action : s3:ListBucket , Resource : arn:aws:s3:::* }, { Sid : VisualEditor2 , Effect : Allow , Action : [ glue:GetDatabase , glue:CreateTable , glue:GetPartitions , glue:DeleteTable , glue:GetTable ], Resource : * }, { Sid : VisualEditor3 , Effect : Allow , Action : [ s3:GetBucketLocation , s3:GetObject , s3:ListBucket , s3:ListBucketMultipartUploads , s3:ListMultipartUploadParts , s3:AbortMultipartUpload , s3:CreateBucket , s3:PutObject ], Resource : [ arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)/* , arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID) ] }, { Sid : VisualEditor4 , Effect : Allow , Action : [ s3:GetObject , s3:ListBucket ], Resource : arn:aws:s3:::(source bucket)/* }, { Sid : VisualEditor5 , Effect : Allow , Action : logs:CreateLogGroup , Resource : arn:aws:logs:us-east-1:(account ID):* } ] }","title":"SubAcctSplit Role"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/Sub_Account_Split.html","text":"Below is the code for the lambda function. You will need to modify the following variable: athena_output : This is where Athena puts output data, this is typically the master/payer Account ID, which is the default folder for Athena output queries output bucket : This is the output bucket for the Athena queries import boto3 import json import datetime import time # Get the current date, so you know which months folder you're working on now = datetime.datetime.now() # Variables to construct the s3 folder name # YES! you can do multiple subfolders if you have multiple queries to run, 1 subfolder per query currentmonth = '/year_1=' + str(now.year) + '/month_1=' + str(now.month) + '/' bucketname = '(output bucket)' # Arrays to hold the Athena delete create queries that we need to run delete_query_strings = [] create_query_strings = [] # Athena output folder athena_output = 's3://aws-athena-query-results-us-east-1- account ID /' # Main loop def lambda_handler(event, context): # Clear the current months S3 folder s3_clear_folders() # Get the athena queries to run get_athena_queries() # Make sure to delete any existing temp tables, so no wobbly's are thrown run_delete_athena_queries() # Create the athena tables, which will actually output data to S3 folders run_create_athena_queries() # Delete the array in case of another Lambda invocation create_query_strings.clear() # You could make another call to delete the tables, however you need to make sure # the creates are finished, which may take some time, consuming time($) in Lambda # run_delete_athena_queries() # Delete the array in case of another Lambda invocation delete_query_strings.clear() return { 'statusCode': 200, 'body': json.dumps('Finished!') } # Clear the S3 folders for the current month def s3_clear_folders(): # Get S3 client/object client = boto3.client('s3') # For each subfolder - in case you have multiple subfolders, i.e. multiple accounts/business units to split data out to for subfolder in subfolders: # List all objects in the current months bucket response = client.list_objects_v2( Bucket=bucketname, Prefix=subfolder + currentmonth ) # Get how many objects there are to delete, if any keys = response['KeyCount'] # Only try to delete if there's objects if (keys 0): # Get the ojbects from the response s3objects = response['Contents'] # For each object, we're going to delete it # cycle through the list of objects for s3object in s3objects: # Get the object key objectkey = s3object['Key'] # Delete the object response = client.delete_object( Bucket=bucketname, Key=objectkey ) # Get the Athena saved queries to run # They need to be labelled 'create_linked' or 'delete_linked' def get_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Get all the saved queries in Athena response = client.list_named_queries() # Get the named query IDs from the response named_query_IDs = response['NamedQueryIds'] # Go through all the query ID, to find the delete create queries we need to run for query_ID in named_query_IDs: # Get all the details of a named query using its ID named_query = client.get_named_query( NamedQueryId=query_ID ) # Get the query string query name of the query querystring = named_query['NamedQuery']['QueryString'] queryname = named_query['NamedQuery']['Name'] # If its a create query, add it to the list of create queries # We also replace the '/subfolder' string in the query with the folder structure for the current month if 'create_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('/subfolder', currentmonth) new_query2 = new_query1.replace('temp_table', 'temp_'+tableID) # Add the create query string to the array create_query_strings.append(new_query2) # If its a delete query, add it to the list of delete queries to execute later if 'delete_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('temp_table', 'temp_'+tableID) # Add the delete query string to the array delete_query_strings.append(new_query1) # Run the delete Athena queries to remove any temp tables def run_delete_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the delete query strings in the list for delete_query_string in delete_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=delete_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } ) # Get the state of the delete execution response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # A busy wait to make sure its finished before moving on # Tables must not exist before creation # If the function runs for a long time ($) you should implement step functions or a cost effective wait # This is a low cost of complexity solution while 'RUNNING' in response: # Busy wait to make sure it finishes time.sleep(1) # Get the current state of the query response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # Run the Athena queries to create the table populate the S3 data def run_create_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the create query strings in the list for create_query_string in create_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=create_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } )","title":"Sub Account Split"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/crawler-cfn.html","text":"Below is a sample crawler config file. It is suggested you modify your existing file, modifications are between '***' characters. Variables that need to be changed in the new code below: (region): The region that contains the Lambda function (accountID): The account that contains the Lambda function AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: '(Database Name)' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3::: bucketname / prefix / folder /WorkshopCUR* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' ***** - 'lambda:InvokeFunction' ***** Resource: ***** - 'arn:aws:logs:*:*:*' - 'arn:aws:lambda: region : accountID :function:SubAcctSplit' ***** - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3:// bucket / prefix / folder /WorkshopCUR' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); ***** var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3::: bucket ' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3::: bucket ' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: ' bucket ' ReportKey: ' prefix / folder /WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3:// bucket / prefix / folder /cost_and_usage_data_status/'","title":"Crawler cfn"},{"location":"Operations/README.html","text":"AWS Well-Architected Operational Excellence Labs https://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper. Labs Operations Fundamentals Level 100: Introduction to Inventory and Patch Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Operations/README.html#aws-well-architected-operational-excellence-labs","text":"https://wellarchitectedlabs.com","title":"AWS Well-Architected Operational Excellence Labs"},{"location":"Operations/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.","title":"Introduction"},{"location":"Operations/README.html#labs","text":"","title":"Labs"},{"location":"Operations/README.html#operations-fundamentals","text":"Level 100: Introduction to Inventory and Patch Management","title":"Operations Fundamentals"},{"location":"Operations/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html","text":"Level 100: Inventory and Patch Management https://wellarchitectedlabs.com Introduction In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management Patch Management Goals: Automated deployment of infrastructure Dynamic management of resources Automated patch management Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#level-100-inventory-and-patch-management","text":"https://wellarchitectedlabs.com","title":"Level 100: Inventory and Patch Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#introduction","text":"In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management Patch Management","title":"Introduction"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#goals","text":"Automated deployment of infrastructure Dynamic management of resources Automated patch management","title":"Goals:"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created.","title":"Prerequisites:"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html","text":"Level 100: Inventory and Patch Management: Lab Guide In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities. In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management * Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested. Creating Maintenance Windows and Scheduling Automated Operations Activities Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. * Removing Lab Resources 1. Setup Requirements You will need the following to be able to perform this lab: Your own device for console access An AWS account that you are able to use for testing, that is not used for production or other purposes * An available region within your account with capacity to add 2 additional VPCs User and Group Management When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account. We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . IAM Users Groups As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \"Administrators\" group to which the AdministratorAccess managed policy is attached. Use administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it . 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user . In Set user details for User name , type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type , select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. If you're creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions . In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group . In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group . Back at Add user to group , in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it's created, see Adding and Removing Users in an IAM Group . 1.2 Log in to the AWS Management Console using your administrator account You can now use this administrator user instead of your root user for this AWS account. Choose the link https\\://\\ yourAccountNumber>.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs. 1.3 Create an EC2 Key Pair Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the IAM navigation pane under Network Security , choose Key Pairs and then choose Create Key Pair . In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create . Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab. 2. Deploy an Environment Using Infrastructure as Code Tagging We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources. AWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality). Apply the following best practices when using tags: Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types Consider tag dimensions that support the following: * Managing resource access control with IAM * Cost tracking * Automation * AWS console organization Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. Err on the side of using too many tags rather than too few tags. * Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words. Management Tools: CloudFormation AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack). There is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments. 2.1 Deploy the Lab Infrastructure To deploy the lab infrastructure: Download the CloudFormation script for this lab from /Code . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack . On the Select Template page, select Specify an Amazon S3 template URL and enter the URL for the downloaded location. AWS CloudFormation Designer AWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources. On the Select Template page, next to Specify an Amazon S3 template URL , choose the link to View/Edit template in Designer . Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page . On the Select Template page, choose Next . A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented. In the Specify Details section, define a Stack name , such as OELabStack1 . In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to https://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test . Choose Next . On the Options page under Tags , define a Key of Owner , with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next . On the Review page, review your choices and then choose Create . On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud. Navigate to the EC2 console to view the deployed systems: Choose Instances . Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created. The impact of Infrastructure as Code With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment. The ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency. 3. Inventory Management using Operations as Code Management Tools: Systems Manager AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab. There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments . You must use a supported operating system * Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS The SSM Agent must be installed * The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents Your EC2 instances must have outbound internet access You must access Systems Manager in a supported region * Systems Manager requires IAM roles * for instances that will process commands * for users executing commands SSM Agent is installed by default on: Amazon Linux base AMIs dated 2017.09 and later Windows Server 2016 instances * Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments. 3.1 Setting up Systems Manager Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles . Then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 ( EC2 Allows EC2 instances to call AWS services on your behalf ) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions . Under Attached permissions policy , verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review . In the Review section: Enter a Role name , such as ManagedInstancesRole . Accept the default in the Role description . Choose Create role . Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances . Select the first instance and then choose Actions , Instance Settings , and Attach/Replace IAM Role . Under Attach/Replace IAM Role , select ManagedInstancesRole from the drop down list and choose Apply . After you receive confirmation of success, choose Close . Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager. 3.2 Create a Second CloudFormation Stack Create a second CloudFormation stack using the procedure in 2.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2 . Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod . Systems Manager: Inventory You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated. 3.3 Using Systems Manager Inventory to Track Your Instances Under Instances Nodes in the AWS Systems Manager navigation bar, choose Inventory . Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by , select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory. Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every , accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory , choose Setup inventory . To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit . Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager . Systems Manager: State Manager In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state. An association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components: * A document that defines the state * Target(s) * A schedule * (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager. 3.4 Review Association Status Under Actions in the navigation bar, select State Manager . At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit . Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name ). Inventory is accomplished through the following: * The activities defined in the AWS-GatherSoftwareInventory command document. * The parameters provided in the Parameters section are passed to the document at execution. * The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets. * The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. * There is the option to specify Output options . Note If you change the command document, the Parameters section will change to be appropriate to the new command document. Navigate to Managed Instances under Instances and Nodes in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Instances Nodes in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section. Systems Manager: Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports. 4. Patch Management Systems Manager: Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates. Note For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags. Warning AWS does not test patches for Windows or Linux before making them available in Patch Manager . If any updates are installed by Patch Manager the patched instance is rebooted . * Always test patches thoroughly before deploying to production environments . Patch Baselines Patch Manager uses patch baselines , which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage. Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent. 4.1 Create a Patch Baseline Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Patch Manager . Click the View predefined patch baselines link under the Configure patching button on the upper right. Choose Create patch baseline . On the Create patch baseline page in the Provide patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline . Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches . Select Amazon Linux from the list. In the Approval rules section: Examine the options in the lists and ensure that Product , Classification , and Severity have values of All . Leave the Auto approval delay at its default of 0 days . Change the value of Compliance reporting - optional to Critical . Choose Add another rule . In the new rule, change the value of Compliance reporting - optional to Medium . Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance reporting , such as Critical or Medium , determines the severity of the compliance violation reported in System Manager Compliance . In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed. Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers ) but the key must be Patch Group . Note An instance can only be in one patch group. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance. If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. If a patch baseline is found for that group, the system applies that patch baseline. * If an instance isn't assigned to a patch group, the system automatically uses the currently configured default patch baseline. 4.2 Assign a Patch Group Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups . In the Modify patch groups window under Patch groups , enter Critical , choose Add , and then choose Close to be returned to the Patch Baseline details screen. AWS-RunPatchBaseline AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are. For Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems. AWS Systems Manager: Document An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including 'AWS-RunPatchBaseline'. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents . You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities. 4.3 Examine AWS-RunPatchBaseline in Documents To examine AWS-RunPatchBaseline in Documents: In the AWS Systems Manager navigation bar under Shared Resources , choose Documents . Click in the search box , select Document name prefix , and then Equal . Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details . Review the content of each tab in the details page of the document. AWS Systems Manager: Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. 4.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon and select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Command parameters section, leave the Operation value as the default Scan . In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload , and under Enter a tag value , enter Test and click Add . The remaining Run Command features enable you to: Specify Rate control , limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix . Note Only the last 2500 characters of a command document's output are displayed in the console. Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it. 4.5 Review Initial Patch Compliance Under Instances Nodes in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page in the Compliance resources summary , you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details. 4.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon, select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload and under Enter a tag value enter Test . In the Command parameters section, change the Operation value to Install . In the Targets section, choose Specify a tag using Workload and Test . Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. Note there are multiple pages of instances. If manually selecting instances, individual selections must be made on each page. In the Rate control section: For Concurrency , ensure that targets is selected and specify the value as 1 . Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times. For Error threshold , ensure that error is selected and specify the value as 1 . Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted. 4.7 Review Patch Compliance After Patching Under Instances Nodes in the the AWS Systems Manager navigation bar, choose Compliance . The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches. The Impact of Operations as Code In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems. Operations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \"swivel chair\" processes that require multiple interfaces and systems to complete a single operations activity. Bonus Content: Creating Maintenance Windows and Scheduling Automated Operations Activities AWS Systems Manager: Maintenance Windows AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following: Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment Note To register Step Function tasks you must use the AWS CLI. 5.1 Setting up Maintenance Windows Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles , and then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2 . This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . Under Attached permissions policy : Search for AmazonSSMMaintenanceWindowRole . Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review . In the Review section: Enter a Role name , such as SSMMaintenanceWindowRole . Enter a Role description , such as Role for Amazon SSMMaintenanceWindow . Choose Create role . Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship . Delete the current policy, and then copy and paste the following policy into the Policy Document field: { Version : 2012-10-17 , Statement :[ { Sid : , Effect : Allow , Principal :{ Service :[ ec2.amazonaws.com , ssm.amazonaws.com , sns.amazonaws.com ] }, Action : sts:AssumeRole } ] } Choose Update Trust Policy . You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window. To create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies , and then choose Create policy . On the Create policy page, in the Select a service area , next to Service choose Choose a service , and then choose IAM . In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \"You choose actions that require the role resource type.\", and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field , delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy . On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy . You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups , and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy . On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy . You will be returned to the Summary page for the group. Creating Maintenance Windows To create a Maintenance Window , you must do the following: Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution. 5.2 Create a Patch Maintenance Window First, you must create the window and define its schedule and duration: Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window . In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers . (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. Note If you choose Allow unregistered targets , then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don't, then you must choose previously registered targets when you register a task with the Maintenance Window. Specify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with , accept the default Cron schedule builder . Under Window starts , choose the third option, specify Every Day at , and select a time, such as 02:00 . In the Duration field, type the number of hours the Maintenance Window should run, such as '3' hours . In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes . Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts , choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window . The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled . 5.3 Assigning Targets to Your Patch Maintenance Window After you create a Maintenance Window, you assign targets where the tasks will run. On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets . On the Register target page under Maintenance window target details : In the Target Name field, enter a name for the targets, such as TestWebServers . (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note : Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window. In the Targets section, under Select Targets by : Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags , enter 'Workload' as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register target to register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags. 5.4 Assigning Tasks to Your Patch Maintenance Window After you assign targets, you assign tasks to perform during the window: From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task . On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers . (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by , select Selecting registered target groups . Select the group you created from the list. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . For Error threshold , leave the default errors selected and specify 1 . In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options , leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix Note Only the last 2500 characters of a command document's output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs. In SNS notifications , leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation , select Install . Choose Register Run command task to complete the task definition and return to the details page. 5.5 Review Maintenance Window Execution After allowing enough time for your maintenance window to complete: Navigte to the AWS Systems Manager console . Choose Maintenance Windows , and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History . Select a Windows execution ID and choose View details . On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID , and choose View output . Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document . Bonus Content: Creating a Simple Notification Service Topic Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic. 6.1 Create and Subscribe to an SNS Topic To create and subscribe to an SNS topic: Navigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic . In the Create new topic window: In the Topic name field, enter AdminAlert . In the Display name field, enter AdminAlert . Choose Create topic . On the Topic details: AdminAlert page, choose Create subscription . In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription . You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN . You can now use this SNS topic to send notifications to your Administrator user. 7 Removing Lab Resources Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier. 7.1 Remove resources created with CloudFormation Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack . Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager . Select the association you created. Choose Delete . If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic , delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics . If you created a Maintenance Window , delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows . Select the maintenance window you created. Choose Delete . In the Delete maintenance window window, choose Delete . If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users . Select your user from the list. Choose Delete user . Select the check box next to \"One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\". Choose Yes, delete . When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.","title":"Lab Guide"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#level-100-inventory-and-patch-management-lab-guide","text":"In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities. In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management * Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested. Creating Maintenance Windows and Scheduling Automated Operations Activities Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. * Removing Lab Resources","title":"Level 100: Inventory and Patch Management: Lab Guide"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#1-setup","text":"","title":"1. Setup"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#requirements","text":"You will need the following to be able to perform this lab: Your own device for console access An AWS account that you are able to use for testing, that is not used for production or other purposes * An available region within your account with capacity to add 2 additional VPCs","title":"Requirements"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#user-and-group-management","text":"When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account. We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"User and Group Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#iam-users-groups","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \"Administrators\" group to which the AdministratorAccess managed policy is attached. Use administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it .","title":"IAM Users &amp; Groups"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user . In Set user details for User name , type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type , select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. If you're creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions . In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group . In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group . Back at Add user to group , in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it's created, see Adding and Removing Users in an IAM Group .","title":"1.1 Create Administrator IAM User and Group"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#12-log-in-to-the-aws-management-console-using-your-administrator-account","text":"You can now use this administrator user instead of your root user for this AWS account. Choose the link https\\://\\ yourAccountNumber>.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs.","title":"1.2 Log in to the AWS Management Console using your administrator account"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#13-create-an-ec2-key-pair","text":"Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the IAM navigation pane under Network Security , choose Key Pairs and then choose Create Key Pair . In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create . Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab.","title":"1.3 Create an EC2 Key Pair"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#2-deploy-an-environment-using-infrastructure-as-code","text":"","title":"2. Deploy an Environment Using Infrastructure as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#tagging","text":"We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources. AWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality). Apply the following best practices when using tags: Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types Consider tag dimensions that support the following: * Managing resource access control with IAM * Cost tracking * Automation * AWS console organization Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. Err on the side of using too many tags rather than too few tags. * Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words.","title":"Tagging"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#management-tools-cloudformation","text":"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack). There is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments.","title":"Management Tools: CloudFormation"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#21-deploy-the-lab-infrastructure","text":"To deploy the lab infrastructure: Download the CloudFormation script for this lab from /Code . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack . On the Select Template page, select Specify an Amazon S3 template URL and enter the URL for the downloaded location. AWS CloudFormation Designer AWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources. On the Select Template page, next to Specify an Amazon S3 template URL , choose the link to View/Edit template in Designer . Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page . On the Select Template page, choose Next . A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented. In the Specify Details section, define a Stack name , such as OELabStack1 . In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to https://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test . Choose Next . On the Options page under Tags , define a Key of Owner , with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next . On the Review page, review your choices and then choose Create . On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud. Navigate to the EC2 console to view the deployed systems: Choose Instances . Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created.","title":"2.1 Deploy the Lab Infrastructure"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#the-impact-of-infrastructure-as-code","text":"With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment. The ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency.","title":"The impact of Infrastructure as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#3-inventory-management-using-operations-as-code","text":"","title":"3. Inventory Management using Operations as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#management-tools-systems-manager","text":"AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab. There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments . You must use a supported operating system * Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS The SSM Agent must be installed * The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents Your EC2 instances must have outbound internet access You must access Systems Manager in a supported region * Systems Manager requires IAM roles * for instances that will process commands * for users executing commands SSM Agent is installed by default on: Amazon Linux base AMIs dated 2017.09 and later Windows Server 2016 instances * Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments.","title":"Management Tools: Systems Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#31-setting-up-systems-manager","text":"Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles . Then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 ( EC2 Allows EC2 instances to call AWS services on your behalf ) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions . Under Attached permissions policy , verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review . In the Review section: Enter a Role name , such as ManagedInstancesRole . Accept the default in the Role description . Choose Create role . Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances . Select the first instance and then choose Actions , Instance Settings , and Attach/Replace IAM Role . Under Attach/Replace IAM Role , select ManagedInstancesRole from the drop down list and choose Apply . After you receive confirmation of success, choose Close . Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager.","title":"3.1 Setting up Systems Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#32-create-a-second-cloudformation-stack","text":"Create a second CloudFormation stack using the procedure in 2.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2 . Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod .","title":"3.2 Create a Second CloudFormation Stack"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-inventory","text":"You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated.","title":"Systems Manager: Inventory"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#33-using-systems-manager-inventory-to-track-your-instances","text":"Under Instances Nodes in the AWS Systems Manager navigation bar, choose Inventory . Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by , select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory. Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every , accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory , choose Setup inventory . To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit . Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager .","title":"3.3 Using Systems Manager Inventory to Track Your Instances"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-state-manager","text":"In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state. An association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components: * A document that defines the state * Target(s) * A schedule * (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager.","title":"Systems Manager: State Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#34-review-association-status","text":"Under Actions in the navigation bar, select State Manager . At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit . Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name ). Inventory is accomplished through the following: * The activities defined in the AWS-GatherSoftwareInventory command document. * The parameters provided in the Parameters section are passed to the document at execution. * The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets. * The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. * There is the option to specify Output options . Note If you change the command document, the Parameters section will change to be appropriate to the new command document. Navigate to Managed Instances under Instances and Nodes in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Instances Nodes in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section.","title":"3.4 Review Association Status"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-compliance","text":"You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.","title":"Systems Manager: Compliance"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#4-patch-management","text":"","title":"4. Patch Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-patch-manager","text":"AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates. Note For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags. Warning AWS does not test patches for Windows or Linux before making them available in Patch Manager . If any updates are installed by Patch Manager the patched instance is rebooted . * Always test patches thoroughly before deploying to production environments .","title":"Systems Manager: Patch Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#patch-baselines","text":"Patch Manager uses patch baselines , which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage. Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent.","title":"Patch Baselines"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#41-create-a-patch-baseline","text":"Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Patch Manager . Click the View predefined patch baselines link under the Configure patching button on the upper right. Choose Create patch baseline . On the Create patch baseline page in the Provide patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline . Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches . Select Amazon Linux from the list. In the Approval rules section: Examine the options in the lists and ensure that Product , Classification , and Severity have values of All . Leave the Auto approval delay at its default of 0 days . Change the value of Compliance reporting - optional to Critical . Choose Add another rule . In the new rule, change the value of Compliance reporting - optional to Medium . Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance reporting , such as Critical or Medium , determines the severity of the compliance violation reported in System Manager Compliance . In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed.","title":"4.1 Create a Patch Baseline"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#patch-groups","text":"A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers ) but the key must be Patch Group . Note An instance can only be in one patch group. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance. If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. If a patch baseline is found for that group, the system applies that patch baseline. * If an instance isn't assigned to a patch group, the system automatically uses the currently configured default patch baseline.","title":"Patch Groups"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#42-assign-a-patch-group","text":"Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups . In the Modify patch groups window under Patch groups , enter Critical , choose Add , and then choose Close to be returned to the Patch Baseline details screen.","title":"4.2 Assign a Patch Group"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-runpatchbaseline","text":"AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are. For Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems.","title":"AWS-RunPatchBaseline"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-document","text":"An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including 'AWS-RunPatchBaseline'. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents . You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.","title":"AWS Systems Manager: Document"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#43-examine-aws-runpatchbaseline-in-documents","text":"To examine AWS-RunPatchBaseline in Documents: In the AWS Systems Manager navigation bar under Shared Resources , choose Documents . Click in the search box , select Document name prefix , and then Equal . Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details . Review the content of each tab in the details page of the document.","title":"4.3 Examine AWS-RunPatchBaseline in Documents"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-run-command","text":"AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.","title":"AWS Systems Manager: Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#44-scan-your-instances-with-aws-runpatchbaseline-via-run-command","text":"Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon and select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Command parameters section, leave the Operation value as the default Scan . In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload , and under Enter a tag value , enter Test and click Add . The remaining Run Command features enable you to: Specify Rate control , limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix . Note Only the last 2500 characters of a command document's output are displayed in the console. Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it.","title":"4.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#45-review-initial-patch-compliance","text":"Under Instances Nodes in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page in the Compliance resources summary , you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details.","title":"4.5 Review Initial Patch Compliance"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#46-patch-your-instances-with-aws-runpatchbaseline-via-run-command","text":"Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon, select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload and under Enter a tag value enter Test . In the Command parameters section, change the Operation value to Install . In the Targets section, choose Specify a tag using Workload and Test . Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. Note there are multiple pages of instances. If manually selecting instances, individual selections must be made on each page. In the Rate control section: For Concurrency , ensure that targets is selected and specify the value as 1 . Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times. For Error threshold , ensure that error is selected and specify the value as 1 . Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted.","title":"4.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#47-review-patch-compliance-after-patching","text":"Under Instances Nodes in the the AWS Systems Manager navigation bar, choose Compliance . The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches.","title":"4.7 Review Patch Compliance After Patching"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#the-impact-of-operations-as-code","text":"In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems. Operations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \"swivel chair\" processes that require multiple interfaces and systems to complete a single operations activity.","title":"The Impact of Operations as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#bonus-content-creating-maintenance-windows-and-scheduling-automated-operations-activities","text":"","title":"Bonus Content: Creating Maintenance Windows and Scheduling Automated Operations Activities"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-maintenance-windows","text":"AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following: Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment Note To register Step Function tasks you must use the AWS CLI.","title":"AWS Systems Manager: Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#51-setting-up-maintenance-windows","text":"Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles , and then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2 . This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . Under Attached permissions policy : Search for AmazonSSMMaintenanceWindowRole . Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review . In the Review section: Enter a Role name , such as SSMMaintenanceWindowRole . Enter a Role description , such as Role for Amazon SSMMaintenanceWindow . Choose Create role . Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship . Delete the current policy, and then copy and paste the following policy into the Policy Document field: { Version : 2012-10-17 , Statement :[ { Sid : , Effect : Allow , Principal :{ Service :[ ec2.amazonaws.com , ssm.amazonaws.com , sns.amazonaws.com ] }, Action : sts:AssumeRole } ] } Choose Update Trust Policy . You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window. To create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies , and then choose Create policy . On the Create policy page, in the Select a service area , next to Service choose Choose a service , and then choose IAM . In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \"You choose actions that require the role resource type.\", and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field , delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy . On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy . You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups , and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy . On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy . You will be returned to the Summary page for the group.","title":"5.1 Setting up Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#creating-maintenance-windows","text":"To create a Maintenance Window , you must do the following: Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution.","title":"Creating Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#52-create-a-patch-maintenance-window","text":"First, you must create the window and define its schedule and duration: Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window . In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers . (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. Note If you choose Allow unregistered targets , then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don't, then you must choose previously registered targets when you register a task with the Maintenance Window. Specify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with , accept the default Cron schedule builder . Under Window starts , choose the third option, specify Every Day at , and select a time, such as 02:00 . In the Duration field, type the number of hours the Maintenance Window should run, such as '3' hours . In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes . Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts , choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window . The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled .","title":"5.2 Create a Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#53-assigning-targets-to-your-patch-maintenance-window","text":"After you create a Maintenance Window, you assign targets where the tasks will run. On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets . On the Register target page under Maintenance window target details : In the Target Name field, enter a name for the targets, such as TestWebServers . (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note : Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window. In the Targets section, under Select Targets by : Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags , enter 'Workload' as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register target to register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags.","title":"5.3 Assigning Targets to Your Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#54-assigning-tasks-to-your-patch-maintenance-window","text":"After you assign targets, you assign tasks to perform during the window: From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task . On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers . (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by , select Selecting registered target groups . Select the group you created from the list. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . For Error threshold , leave the default errors selected and specify 1 . In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options , leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix Note Only the last 2500 characters of a command document's output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs. In SNS notifications , leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation , select Install . Choose Register Run command task to complete the task definition and return to the details page.","title":"5.4 Assigning Tasks to Your Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#55-review-maintenance-window-execution","text":"After allowing enough time for your maintenance window to complete: Navigte to the AWS Systems Manager console . Choose Maintenance Windows , and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History . Select a Windows execution ID and choose View details . On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID , and choose View output . Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document .","title":"5.5 Review Maintenance Window Execution"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#bonus-content-creating-a-simple-notification-service-topic","text":"Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.","title":"Bonus Content: Creating a Simple Notification Service Topic"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#61-create-and-subscribe-to-an-sns-topic","text":"To create and subscribe to an SNS topic: Navigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic . In the Create new topic window: In the Topic name field, enter AdminAlert . In the Display name field, enter AdminAlert . Choose Create topic . On the Topic details: AdminAlert page, choose Create subscription . In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription . You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN . You can now use this SNS topic to send notifications to your Administrator user.","title":"6.1 Create and Subscribe to an SNS Topic"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#7-removing-lab-resources","text":"Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier.","title":"7 Removing Lab Resources"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#71-remove-resources-created-with-cloudformation","text":"Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack . Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager . Select the association you created. Choose Delete . If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic , delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics . If you created a Maintenance Window , delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows . Select the maintenance window you created. Choose Delete . In the Delete maintenance window window, choose Delete . If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users . Select your user from the list. Choose Delete user . Select the check box next to \"One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\". Choose Yes, delete . When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.","title":"7.1 Remove resources created with CloudFormation"},{"location":"Performance/README.html","text":"AWS Well-Architected Performance Efficiency Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about performance efficiency read the AWS Well-Architected Performance Efficiency whitepaper or online https://wa.aws.amazon.com/ . Labs Level 100: Monitoring with CloudWatch Dashboards License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Performance/README.html#aws-well-architected-performance-efficiency-labs","text":"","title":"AWS Well-Architected Performance Efficiency Labs"},{"location":"Performance/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about performance efficiency read the AWS Well-Architected Performance Efficiency whitepaper or online https://wa.aws.amazon.com/ .","title":"Introduction"},{"location":"Performance/README.html#labs","text":"Level 100: Monitoring with CloudWatch Dashboards","title":"Labs"},{"location":"Performance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html","text":"Level 100: Monitoring with CloudWatch Dashboards Introduction This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Monitor resources to ensure they are performing as expected Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#level-100-monitoring-with-cloudwatch-dashboards","text":"","title":"Level 100: Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#introduction","text":"This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#goals","text":"Monitor resources to ensure they are performing as expected","title":"Goals"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html","text":"Level 100: Monitoring with CloudWatch Dashboards Authors Ben Potter, Security Lead, Well-Architected 1. View Amazon CloudWatch Automatic Dashboards Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar. The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. The upper right shows two or four alarms in your account, depending on how many AWS services you use. The alarms shown are those in the ALARM state or those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues. Below these areas is the custom dashboard that you have created and named CloudWatch-Default, if any. This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use, without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. If you use fewer than six AWS services, the cross-service dashboard is shown automatically on this page. References useful resources See Key Metrics From All AWS Services Focus on Metrics and Alarms in a Single AWS Service Focus on Metrics and Alarms in a Resource Group License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#level-100-monitoring-with-cloudwatch-dashboards","text":"","title":"Level 100: Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#1-view-amazon-cloudwatch-automatic-dashboards","text":"Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar. The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. The upper right shows two or four alarms in your account, depending on how many AWS services you use. The alarms shown are those in the ALARM state or those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues. Below these areas is the custom dashboard that you have created and named CloudWatch-Default, if any. This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use, without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. If you use fewer than six AWS services, the cross-service dashboard is shown automatically on this page.","title":"1. View Amazon CloudWatch Automatic Dashboards "},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#references-useful-resources","text":"See Key Metrics From All AWS Services Focus on Metrics and Alarms in a Single AWS Service Focus on Metrics and Alarms in a Resource Group","title":"References &amp; useful resources"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Reliability/README.html","text":"AWS Well-Architected Reliability Labs https://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper . Labs Level 200: Testing for Resiliency of EC2 Level 300: Testing for Resiliency of EC2, RDS, and S3 License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Reliability/README.html#aws-well-architected-reliability-labs","text":"https://wellarchitectedlabs.com","title":"AWS Well-Architected Reliability Labs"},{"location":"Reliability/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper .","title":"Introduction"},{"location":"Reliability/README.html#labs","text":"Level 200: Testing for Resiliency of EC2 Level 300: Testing for Resiliency of EC2, RDS, and S3","title":"Labs"},{"location":"Reliability/README.html#license","text":"","title":"License"},{"location":"Reliability/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html","text":"Level 200: Testing for Resiliency of EC2 https://wellarchitectedlabs.com Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2). The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#level-200-testing-for-resiliency-of-ec2","text":"https://wellarchitectedlabs.com","title":"Level 200: Testing for Resiliency of EC2"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2). The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#prequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#license","text":"","title":"License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html","text":"Level 200: Testing for Resiliency of EC2 Authors Rodney Lester, Reliability Lead, Well-Architected Table of Contents Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear Down 1. Deploy the Infrastructure It is a prerequisite to this lab is that you have deployed the static web application stack. If you have already run the following two labs (and have not torn down the resources) then you have already deployed the necessary infrastructure. Proceed to next step Configure Execution Environment Security: Level 200: Automated Deployment of VPC Security: Level 200: Automated Deployment of EC2 Web Application If you have not already deployed the necessary infrastructure, then follow these steps: 1.1 Deploy the VPC infrastructure If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of VPC lab, and then return here for the next step: 1.2 Deploy the EC2s and Static WebApp infrastructure Express Steps (Deploy the VPC infrastructure) Download the vpc-alb-app-db.yaml CloudFormation template Choose the AWS region you wish to use - if possible we recommend using us-east-2 (Ohio) In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack 1.2 Deploy the EC2s and Static WebApp infrastructure If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of EC2 Web Application Follow directions for the create a static web application option Then return here for the next step: Website URL Express Steps (Deploy the EC2s and Static WebApp infrastructure) Download the staticwebapp.yaml CloudFormation template In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-Static (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack Website URL In the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created Save this URL - you will need it later 2. Configure Execution Environment Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided. 2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 (or region where you deployed your WebApp) Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now and proceed to the next step 2.2 Set up the bash environment If you need help then follow the instructions in either Option A or Option B below Option A AWS CLI This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option B To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option B Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 (or your chosen region) Default output format [None]: json Option B Manually creating credential files If you already did Option A , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 (or your chosen region) output = json 2.2 Set up the bash environment Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the fail_instance.sh script from the resiliency bash scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: bash/fail_instance.sh Set the script to be executable. chmod u+x fail_instance.sh 2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment 3. Test Resiliency Using Failure Injection Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts. Preparation Before testing, please prepare the following: Region must be the one you selected when you deployed your WebApp We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the correct region. For example the following screen shot shows the desired region assuming your WebApp was deployed to Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to WebApp1-VPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever vpc-id is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier * If you do not recall this, then in the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created) Note the instance_id (begins with i- ) - this is the EC2 instance serving this request Refresh the website several times watching these values Note the values change. You have deployed two web servers per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. 3.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service. Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebApp1 . For these EC2 instances note: Each has a unique Instance ID There is two instances per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing vpc-id in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh vpc-id Python python fail_instance.py vpc-id Java java -jar app-resiliency-1.0.jar EC2 vpc-id C# .\\AppResiliency EC2 vpc-id PowerShell .\\fail_instance.ps1 vpc-id The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated . 3.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.2.1 System availability Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id) 3.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with whose name begins with WebAp Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts 3.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebApp1 Click on the Activity History tab and observe: The screen cap below shows that instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand 3.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones 4. Tear down this lab The following instructions will remove the resources that you have created in this lab. If you deployed the CloudFormation stacks as part of the prerequisites for this lab, then delete these stacks to remove all the AWS resources. If you need help with how to delete CloudFormation stacks then follow these instructions to tear down those resources: Delete the WebApp resources Wait for this stack deletion to complete Delete the VPC resources Otherwise, there were no additional new resources added as part of this lab. References useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#level-200-testing-for-resiliency-of-ec2","text":"","title":"Level 200: Testing for Resiliency of EC2"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected","title":"Authors"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#table-of-contents","text":"Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear Down","title":"Table of Contents"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#1-deploy-the-infrastructure","text":"It is a prerequisite to this lab is that you have deployed the static web application stack. If you have already run the following two labs (and have not torn down the resources) then you have already deployed the necessary infrastructure. Proceed to next step Configure Execution Environment Security: Level 200: Automated Deployment of VPC Security: Level 200: Automated Deployment of EC2 Web Application If you have not already deployed the necessary infrastructure, then follow these steps:","title":"1. Deploy the Infrastructure "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#11-deploy-the-vpc-infrastructure","text":"If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of VPC lab, and then return here for the next step: 1.2 Deploy the EC2s and Static WebApp infrastructure","title":"1.1 Deploy the VPC infrastructure"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#express-steps-deploy-the-vpc-infrastructure","text":"Download the vpc-alb-app-db.yaml CloudFormation template Choose the AWS region you wish to use - if possible we recommend using us-east-2 (Ohio) In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack","title":"Express Steps (Deploy the VPC infrastructure)"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#12-deploy-the-ec2s-and-static-webapp-infrastructure","text":"If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of EC2 Web Application Follow directions for the create a static web application option Then return here for the next step: Website URL","title":"1.2 Deploy the EC2s and Static WebApp infrastructure"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#express-steps-deploy-the-ec2s-and-static-webapp-infrastructure","text":"Download the staticwebapp.yaml CloudFormation template In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-Static (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack","title":"Express Steps (Deploy the EC2s and Static WebApp infrastructure)"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#website-url","text":"In the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created Save this URL - you will need it later","title":"Website URL"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#2-configure-execution-environment","text":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided.","title":"2. Configure Execution Environment "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#21-setup-aws-credentials-and-configuration","text":"Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 (or region where you deployed your WebApp) Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now and proceed to the next step 2.2 Set up the bash environment If you need help then follow the instructions in either Option A or Option B below","title":"2.1 Setup AWS credentials and configuration "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#option-a-aws-cli","text":"This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option B To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option B Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 (or your chosen region) Default output format [None]: json","title":"Option A AWS CLI"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#option-b-manually-creating-credential-files","text":"If you already did Option A , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 (or your chosen region) output = json","title":"Option B Manually creating credential files"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#22-set-up-the-bash-environment","text":"Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the fail_instance.sh script from the resiliency bash scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: bash/fail_instance.sh Set the script to be executable. chmod u+x fail_instance.sh","title":"2.2 Set up the bash environment "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#23-set-up-the-programming-language-environment-for-python-java-c-or-powershell","text":"If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment","title":"2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#3-test-resiliency-using-failure-injection","text":"Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.","title":"3. Test Resiliency Using Failure Injection "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#preparation","text":"Before testing, please prepare the following: Region must be the one you selected when you deployed your WebApp We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the correct region. For example the following screen shot shows the desired region assuming your WebApp was deployed to Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to WebApp1-VPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever vpc-id is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier * If you do not recall this, then in the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created) Note the instance_id (begins with i- ) - this is the EC2 instance serving this request Refresh the website several times watching these values Note the values change. You have deployed two web servers per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances.","title":"Preparation"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#31-ec2-failure-injection","text":"This failure injection will simulate a critical problem with one of the three web servers used by your service. Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebApp1 . For these EC2 instances note: Each has a unique Instance ID There is two instances per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing vpc-id in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh vpc-id Python python fail_instance.py vpc-id Java java -jar app-resiliency-1.0.jar EC2 vpc-id C# .\\AppResiliency EC2 vpc-id PowerShell .\\fail_instance.ps1 vpc-id The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated .","title":"3.1 EC2 failure injection"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#32-system-response-to-ec2-instance-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.2 System response to EC2 instance failure"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#321-system-availability","text":"Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id)","title":"3.2.1 System availability"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#322-load-balancing","text":"Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with whose name begins with WebAp Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts","title":"3.2.2 Load balancing"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#323-auto-scaling","text":"Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebApp1 Click on the Activity History tab and observe: The screen cap below shows that instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand","title":"3.2.3 Auto scaling"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#324-ec2-failure-injection-conclusion","text":"Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones","title":"3.2.4 EC2 failure injection - conclusion"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. If you deployed the CloudFormation stacks as part of the prerequisites for this lab, then delete these stacks to remove all the AWS resources. If you need help with how to delete CloudFormation stacks then follow these instructions to tear down those resources: Delete the WebApp resources Wait for this stack deletion to complete Delete the VPC resources Otherwise, there were no additional new resources added as part of this lab.","title":"4. Tear down this lab "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html","text":"Setting up an environment to run the workshop using a programming language If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps 1. Set up AWS credentials If you have not yet setup your AWS credentials, then follow this guide 2. Language specific setup Choose the appropriate section below for your language 2.1 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the fail_instance.py from the resiliency Python scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: python/fail_instance.py 2.2 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B Option A : If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B : Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line 2.3 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 2.4 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the fail_instance.sh script from the resiliency PowerShell scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: powershell/fail_instance.sh If your PowerShell script is refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#setting-up-an-environment-to-run-the-workshop-using-a-programming-language","text":"If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#1-set-up-aws-credentials","text":"If you have not yet setup your AWS credentials, then follow this guide","title":"1. Set up AWS credentials"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#2-language-specific-setup","text":"Choose the appropriate section below for your language","title":"2. Language specific setup"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#21-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the fail_instance.py from the resiliency Python scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: python/fail_instance.py","title":"2.1 Setting Up the Python Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#22-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B Option A : If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B : Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line","title":"2.2 Setting Up the Java Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#23-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.3 Setting Up the C# Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#24-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the fail_instance.sh script from the resiliency PowerShell scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: powershell/fail_instance.sh If your PowerShell script is refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"2.4 Setting up the Powershell Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html","text":"Software Install This reference will help you install software necessary to setup your workshop environment AWS CLI jq AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS. Linux This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here STOP HERE and return to the Lab Guide jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Linux Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq : $ sudo apt-get install jq ...(lots of output)... $ jq --version jq-1.5-1-a5b5cbe * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Software Install"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#software-install","text":"This reference will help you install software necessary to setup your workshop environment AWS CLI jq","title":"Software Install"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#aws-cli","text":"The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.","title":"AWS CLI "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#linux","text":"This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here STOP HERE and return to the Lab Guide","title":"Linux"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#jq","text":"jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.","title":"jq"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#linux_1","text":"Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq : $ sudo apt-get install jq ...(lots of output)... $ jq --version jq-1.5-1-a5b5cbe * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Linux"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html","text":"Level 300: Testing for Resiliency of EC2, RDS, and S3 https://wellarchitectedlabs.com Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document. Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview: Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables. Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#level-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"https://wellarchitectedlabs.com","title":"Level 300: Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#prequisites","text":"An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#overview","text":"Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables.","title":"Overview:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html","text":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3 Introduction This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in. Prerequisites An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Building and uploading the AWS Lambda Functions Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd LambdaDirectory % make % cd .. % aws s3 cp lambda .zip s3:// S3 bucket / directory prefix / lambda .zip Debugging the AWS Lambda Functions The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb lambda_function .py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda. Building and Uploading the Web Application The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3:// S3 bucket / diretory prefix /FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy. The Bootstrapping Script The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs. The SQL in the Bootstrapping Script The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses. Deploying the State Machine The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it. CloudFormation templates The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#builders-guide-for-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#introduction","text":"This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#prerequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON","title":"Prerequisites"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#building-and-uploading-the-aws-lambda-functions","text":"Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd LambdaDirectory % make % cd .. % aws s3 cp lambda .zip s3:// S3 bucket / directory prefix / lambda .zip","title":"Building and uploading the AWS Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#debugging-the-aws-lambda-functions","text":"The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb lambda_function .py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda.","title":"Debugging the AWS Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#building-and-uploading-the-web-application","text":"The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3:// S3 bucket / diretory prefix /FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy.","title":"Building and Uploading the Web Application"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#the-bootstrapping-script","text":"The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs.","title":"The Bootstrapping Script"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#the-sql-in-the-bootstrapping-script","text":"The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses.","title":"The SQL in the Bootstrapping Script"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#deploying-the-state-machine","text":"The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it.","title":"Deploying the State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#cloudformation-templates","text":"The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"CloudFormation templates"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html","text":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3 Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Adrian Hornsby, Tech Evangelist, AWS Seth Eliot, Resiliency Lead, Well-Architected, AWS Table of Contents Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear down this lab 1. Deploy the Infrastructure You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a data store on Amazon Relational Database Service (RDS). 1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor : Follow the instructions here for accessing your AWS account Note : As part of these instructions you are directed to copy and save AWS credentials for your account. Please do so as you will need them later If you are using your own AWS account : Sign in to the AWS Management Console as an IAM user or using a federated role You will need AWS credentials with which you can access your account. For example you can use an AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from an IAM User you control. If you do not have these credentials, follow the instructions here to create them 1.2 Checking for existing service-linked roles If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go directly to step Create the \"deployment machine\" . If you are using your own AWS account : Follow these steps , and then return here and resume with the following instructions. 1.3 Create the \"deployment machine\" Here you will build a state machine using AWS Step Functions and AWS Lambda that orchestrates the deployment of the multi-tier infrastructure. This is not the service infrastructure itself, but meta-infrastructure we use to build the actual infrastructure. Learn more : After the lab see this blog post on how AWS Step Functions and AWS CodePipelines can work together to deploy your infrastructure Choose a deployment option. This lab can be run as single region or multi region (two region) deployment. single region is faster to get up and running multi region enables you to test some additional aspects of cross-regional resilience. Choose one of these options and follows the instructions for it. If you are attending an in-person workshop, your instructor will specify which to use. Please execute only the single region labs at this time Recent updates have made the multi-region approach non-functional. Updates are coming soon to restore this option. Get the CloudFormation template: Download the appropriate file (You can right-click then choose download; or you can right click and copy the link to use with wget ) single region : download CloudFormation template here multi region : download CloudFormation template here Ensure you have selected the Ohio region. This region is also known as us-east-2 , which you will see referenced throughout this lab. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click \u201cCreate Stack:\u201d Leave \"Prepare template\" setting as-is 1 - For \"Template source\" select \"Upload a template file\" 2 - Specify the CloudFormation template you downloaded Click the \u201cNext\u201d button. For \"Stack name\" enter: DeployResiliencyWorkshop On the same screen, for \"Parameters\" enter the appropriate values: If you are attending an in-person workshop and were provided with an AWS account by the instructor : Leave all the parameters at their default values If you are using your own AWS account : Set the first three parameters using these instructions and leave all other parameters at their default values. You optionally may review the default values of this CloudFormation template here Click the \u201cNext\u201d button. On the \"Configure stack options\" page, click \u201cNext\u201d again On the \"Review DeployResiliencyWorkshop\" page, scroll to the bottom and tick the checkbox \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate stack\u201d button. This will take you to the CloudFormation stack status page, showing the stack creation in progress. This will take approximately a minute to deploy. When it shows status CREATE_COMPLETE , then you are finished with this step. 1.4 Deploy infrastructure and run the service Go to the AWS Step Function console at https://console.aws.amazon.com/states On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- random characters .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. On the \"New execution\" dialog, for \"Enter an execution name\" delete the auto-generated name and replace it with: BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. single region uses the following values: { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"secondary_region_name\": \"us-west-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } multi region uses the values here Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Then click the \u201cStart Execution\u201d button. The \"deployment machine\" is now deploying the infrastructure and service you will use for resiliency testing. single region : approximately 20-25 minutes to deploy multi region : approximately 45-50 minutes to deploy. In about 25-30 minutes you can start executing lab exercises. You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. You can also watch the CloudFormation stacks as they are created and transition from CREATE_IN_PROGRESS to CREATE_COMPLETE . Note: If you are in a workshop, the instructor will share background and technical information while your service is deployed. You can resume testing when the web tier has been deployed in the Ohio region. Look for the WaitForWebApp step (for single region ) or WaitForWebApp1 step (for multi region ) to have completed successfully. This will look something like this on the visual workflow. Above screen shot is for single region . for multi region see this diagram instead 1.5 View website for test web service Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . click on the WebServersforResiliencyTesting stack click on the \"Outputs\" tab For the Key WebSiteURL copy the value. This is the URL of your test web service. Click the value and it will bring up the website: (image will vary depending on what you supplied for websiteimage ) 2. Configure Execution Environment Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided. 2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials - You identified these credentials back in step 1 AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now. If you need help then follow these instructions 2.2 Set up the bash environment Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the resiliency bash scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: bash/fail_instance.sh bash/failover_rds.sh bash/fail_az.sh Set the scripts to be executable. chmod u+x fail_instance.sh chmod u+x failover_rds.sh chmod u+x fail_az.sh 2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment 3. Test Resiliency Using Failure Injection Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts. Preparation Before testing, please prepare the following: Region must be Ohio We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to ResiliencyVPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever vpc-id is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier. (If you do not recall this, then see these instructions ) Note the availability_zone and instance_id Refresh the website several times watching these values Note the values change. You have deployed one web server per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. Refer to the diagram at the start of this Lab Guide to review your deployed system architecture. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones 3.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service. Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebServerforResiliency . For these EC2 instances note: Each has a unique Instance ID There is one instance per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing vpc-id in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh vpc-id Python python fail_instance.py vpc-id Java java -jar app-resiliency-1.0.jar EC2 vpc-id C# .\\AppResiliency EC2 vpc-id PowerShell .\\fail_instance.ps1 vpc-id The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated . 3.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.2.1 System availability Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id ) 3.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts 3.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting Click on the Activity History tab and observe: The screen cap below shows that all three instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand 3.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability. 3.3 RDS failure injection This failure injection will simulate a critical failure of the Amazon RDS DB instance. Before you initiate the failure simulation, refresh the service website several times. Every time the image is loaded, the website writes a record to the Amazon RDS database Click on Click here to go to other page and it will show the latest ten entries in the Amazon RDS DB. Click again to return to the image page. Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds From the RDS dashboard Click on \"DB Instances (1/40)\" Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) Select the Configuration tab Look at the configured values. Note the following: Value of the Info field is Available RDS DB is configured to be Multi-AZ . The primary DB instance is in AZ us-east-2a and the standby DB instance is in AZ us-east-2b To failover of the RDS instance, use the VPC ID as the command line argument replacing vpc-id in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./failover_rds.sh vpc-id Python python fail_rds.py vpc-id Java java -jar app-resiliency-1.0.jar RDS vpc-id C# .\\AppResiliency RDS vpc-id PowerShell .\\failover_rds.ps1 vpc-id The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt 3.4 System response to RDS instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.4.1 System availability The website is not available. Some errors you might see reported: No Response / Timeout : Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out : Amazon Elastic Load Balancer has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway : The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site can\u2019t be reached . This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests. Continue on to the next steps, periodically returning to attempt to refresh the website. 3.4.2 Failover to standby On the database console Configuration tab Refresh and note the values of the Info field. It will ultimately return to Available then the failover is complete Note the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. From the AWS RDS console, click on the Logs events tab and scroll down to Recent events . You should see entries like those below. In this case failover took less than a minute. Mon, 14 Oct 2019 19:53:37 GMT - Multi-AZ instance failover started. Mon, 14 Oct 2019 19:53:45 GMT - DB instance restarted Mon, 14 Oct 2019 19:54:21 GMT - Multi-AZ instance failover completed 3.4.2 EC2 server replacement From the AWS RDS console, click on the Monitoring tab and look at DB connections As the failover happens the existing three servers all cannot connect to the DB AWS Auto Scaling detects this (any server not returning an http 200 status is deemed unhealthy), and replaces the three EC2 instances with new ones that establish new connections to the new RDS primary instance The graph shows an unavailability period of about four minutes until at least one DB connection is re-established [optional] Go to the Auto scaling group and AWS Elastic Load Balancer Target group consoles to see how EC2 instance and traffic routing was handled 3.4.3 RDS failure injection - conclusion AWS RDS Database failover took less than a minute Time for AWS Auto Scaling to detect that the instances were unhealthy and to start up new ones took four minutes. This resulted in a four minute non-availability event. Learn more : After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments. 3.6 AZ failure injection This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more. Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in. A good way to run the AZ failure injection is first in an AZ other than this - we'll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we'll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios. To simulate failure of an AZ, select one of the Availability Zones used by your service ( us-east-2a , us-east-2b , or us-east-2c ) as az For scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b use your VPC ID as vpc-id Select one (and only one) of the scripts/programs below. (choose the language that you setup your environment for). Language Command Bash ./fail_az.sh az vpc-id Python python fail_az.py vpc-id az Java java -jar app-resiliency-1.0.jar AZ vpc-id az C# .\\AppResiliency AZ vpc-id az PowerShell .\\fail_az.ps1 az vpc-id The specific output will vary based on the command used. Note whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance 3.7 System response to AZ failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.7.1 System availability Refresh the service website several times Scenario 1 : If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2 : If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur. 3.7.2 Scenario 1 - Load balancer and web server tiers This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens you as for that test: EC2 Instances Load Balancer Target group Auto Scaling Groups One difference from the EC2 failure test that you will observe is that auto scaling will bring up the replacement EC2 instance in an AZ that already has an EC2 instance as it attempts to balance the requested three EC2 instances across the remaining AZs. 3.7.3 Scenario 2 - Load balancer, web server, and data tiers This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console , navigate to your DB screen and observe the following tabs: Configuration Monitoring Logs Events 3.7.4 AZ failure injection - conclusion This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure. 3.7.4 AZ failure recovery This step is optional. To simulate the AZ returning to health do the following: Go to the Auto Scaling Group console Select the WebServersforResiliencyTesting auto scaling group Actions Edit In the Subnet field add the two ResiliencyVPC-PrivateSubnet s that are missing and Save Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions Edit subnet associations Uncheck all boxes and click Edit Actions Delete network ACL Note how the auto scaling redistributes the EC2 serves across the availability zones 3.8 S3 failure injection Failure of S3 means that the image will not be available You may ONLY do this testing if you supplied your own websiteimage reference to an S3 bucket you control 3.8.1 Bucket name You will need to know the bucket name where your image is. For example if the websiteimage value you supplied was \"https://s3.us-east-2.amazonaws.com/my-awesome-bucketname/my_image.jpg\" , then the bucket name is my-awesome-bucketname For this failure simulation it is most straightforward to use the AWS Console as follows. (If you are interested in doing this using the AWS CLI then see here - choose either AWS Console or AWS CLI) AWS Console Navigate to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the object, then select the \"Permissions\" tab Select the \"Public Access\" radio button, and deselect the \"Read object\" checkbox and Save To re-enable access (after testing), do the same steps, tick the \"Read object\" checkbox and Save 3.8.3 System response to S3 failure What is the expected effect? How long does it take to take effect? Note that due to browser caching you may still see the image on refreshing the site. On most systems Shift-F5 does a clean refresh with no cache How would you diagnose if this is a larger problem than permissions? 3.9 More testing you can do You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal the failure modes. 4. Tear down this lab If you are attending an in-person workshop and were provided with an AWS account by the instructor : There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account : You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Remove manually provisioned resources Some resources were created by the failure simulation scripts. You need to remove these first Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions Edit subnet associations Uncheck all boxes and click Edit Actions Delete network ACL Remove AWS CloudFormation provisioned resources As part of lab setup you have deployed several AWS CloudFormation stacks. These directions will show you: How to delete an AWS CloudFormation stack In what specific order the stacks must be deleted How to delete an AWS CloudFormation stack Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal Single region If you deployed the single region option, then delete your stacks in the following order Order CloudFormation stack 1 WebServersforResiliencyTesting 1 MySQLforResiliencyTesting 2 ResiliencyVPC 2 DeployResiliencyWorkshop Multi region If you deployed the multi region option, then see these instructions for the order in which to delete the CloudFormation stacks Delete remaining resources The password(s) for your Amazon RDS instances were stored in AWS Systems Manager secure parameter store. These steps will verify the parameter(s) were deleted, and if not then guide you to deleting them. single region You only need to do the following steps in us-east-2 multi region Do the following steps for both us-east-2 and us-west- 2 Select the region Wait until ResiliencyVPC CloudFormation stack is DELETE_COMPLETE in the region Go to the AWS Console for AWS Systems Manager parameter store Look for the parameter created for your infrastructure. If you used our default values, this will be named 300-ResiliencyofEC2RDSandS3 If it is not present (check all regions you deployed to) then you are finished If it is present then Click on the parameter name Click the Delete button Click Delete again References useful resources EC2 Auto Scaling Groups What Is an Application Load Balancer? High Availability (Multi-AZ) for Amazon RDS Amazon RDS Under the Hood: Multi-AZ Regions and Availability Zones Injecting Chaos to Amazon EC2 using AWS System Manager Build a serverless multi-region, active-active backend solution in an hour License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#level-300-performing-resiliency-testing-for-ec2-rds-and-s3","text":"","title":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS Adrian Hornsby, Tech Evangelist, AWS Seth Eliot, Resiliency Lead, Well-Architected, AWS","title":"Authors"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#table-of-contents","text":"Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear down this lab","title":"Table of Contents"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#1-deploy-the-infrastructure","text":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a data store on Amazon Relational Database Service (RDS).","title":"1. Deploy the Infrastructure "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#11-log-into-the-aws-console","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : Follow the instructions here for accessing your AWS account Note : As part of these instructions you are directed to copy and save AWS credentials for your account. Please do so as you will need them later If you are using your own AWS account : Sign in to the AWS Management Console as an IAM user or using a federated role You will need AWS credentials with which you can access your account. For example you can use an AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from an IAM User you control. If you do not have these credentials, follow the instructions here to create them","title":"1.1 Log into the AWS console "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#12-checking-for-existing-service-linked-roles","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go directly to step Create the \"deployment machine\" . If you are using your own AWS account : Follow these steps , and then return here and resume with the following instructions.","title":"1.2 Checking for existing service-linked roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#13-create-the-deployment-machine","text":"Here you will build a state machine using AWS Step Functions and AWS Lambda that orchestrates the deployment of the multi-tier infrastructure. This is not the service infrastructure itself, but meta-infrastructure we use to build the actual infrastructure. Learn more : After the lab see this blog post on how AWS Step Functions and AWS CodePipelines can work together to deploy your infrastructure Choose a deployment option. This lab can be run as single region or multi region (two region) deployment. single region is faster to get up and running multi region enables you to test some additional aspects of cross-regional resilience. Choose one of these options and follows the instructions for it. If you are attending an in-person workshop, your instructor will specify which to use. Please execute only the single region labs at this time Recent updates have made the multi-region approach non-functional. Updates are coming soon to restore this option. Get the CloudFormation template: Download the appropriate file (You can right-click then choose download; or you can right click and copy the link to use with wget ) single region : download CloudFormation template here multi region : download CloudFormation template here Ensure you have selected the Ohio region. This region is also known as us-east-2 , which you will see referenced throughout this lab. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click \u201cCreate Stack:\u201d Leave \"Prepare template\" setting as-is 1 - For \"Template source\" select \"Upload a template file\" 2 - Specify the CloudFormation template you downloaded Click the \u201cNext\u201d button. For \"Stack name\" enter: DeployResiliencyWorkshop On the same screen, for \"Parameters\" enter the appropriate values: If you are attending an in-person workshop and were provided with an AWS account by the instructor : Leave all the parameters at their default values If you are using your own AWS account : Set the first three parameters using these instructions and leave all other parameters at their default values. You optionally may review the default values of this CloudFormation template here Click the \u201cNext\u201d button. On the \"Configure stack options\" page, click \u201cNext\u201d again On the \"Review DeployResiliencyWorkshop\" page, scroll to the bottom and tick the checkbox \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate stack\u201d button. This will take you to the CloudFormation stack status page, showing the stack creation in progress. This will take approximately a minute to deploy. When it shows status CREATE_COMPLETE , then you are finished with this step.","title":"1.3 Create the \"deployment machine\" "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#14-deploy-infrastructure-and-run-the-service","text":"Go to the AWS Step Function console at https://console.aws.amazon.com/states On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- random characters .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. On the \"New execution\" dialog, for \"Enter an execution name\" delete the auto-generated name and replace it with: BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. single region uses the following values: { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"secondary_region_name\": \"us-west-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } multi region uses the values here Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Then click the \u201cStart Execution\u201d button. The \"deployment machine\" is now deploying the infrastructure and service you will use for resiliency testing. single region : approximately 20-25 minutes to deploy multi region : approximately 45-50 minutes to deploy. In about 25-30 minutes you can start executing lab exercises. You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. You can also watch the CloudFormation stacks as they are created and transition from CREATE_IN_PROGRESS to CREATE_COMPLETE . Note: If you are in a workshop, the instructor will share background and technical information while your service is deployed. You can resume testing when the web tier has been deployed in the Ohio region. Look for the WaitForWebApp step (for single region ) or WaitForWebApp1 step (for multi region ) to have completed successfully. This will look something like this on the visual workflow. Above screen shot is for single region . for multi region see this diagram instead","title":"1.4 Deploy infrastructure and run the service "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#15-view-website-for-test-web-service","text":"Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . click on the WebServersforResiliencyTesting stack click on the \"Outputs\" tab For the Key WebSiteURL copy the value. This is the URL of your test web service. Click the value and it will bring up the website: (image will vary depending on what you supplied for websiteimage )","title":"1.5 View website for test web service "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#2-configure-execution-environment","text":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided.","title":"2. Configure Execution Environment "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#21-setup-aws-credentials-and-configuration","text":"Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials - You identified these credentials back in step 1 AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now. If you need help then follow these instructions","title":"2.1 Setup AWS credentials and configuration"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#22-set-up-the-bash-environment","text":"Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the resiliency bash scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: bash/fail_instance.sh bash/failover_rds.sh bash/fail_az.sh Set the scripts to be executable. chmod u+x fail_instance.sh chmod u+x failover_rds.sh chmod u+x fail_az.sh","title":"2.2 Set up the bash environment "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#23-set-up-the-programming-language-environment-for-python-java-c-or-powershell","text":"If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment","title":"2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#3-test-resiliency-using-failure-injection","text":"Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.","title":"3. Test Resiliency Using Failure Injection "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#preparation","text":"Before testing, please prepare the following: Region must be Ohio We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to ResiliencyVPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever vpc-id is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier. (If you do not recall this, then see these instructions ) Note the availability_zone and instance_id Refresh the website several times watching these values Note the values change. You have deployed one web server per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. Refer to the diagram at the start of this Lab Guide to review your deployed system architecture. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones","title":"Preparation"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#31-ec2-failure-injection","text":"This failure injection will simulate a critical problem with one of the three web servers used by your service. Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebServerforResiliency . For these EC2 instances note: Each has a unique Instance ID There is one instance per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing vpc-id in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh vpc-id Python python fail_instance.py vpc-id Java java -jar app-resiliency-1.0.jar EC2 vpc-id C# .\\AppResiliency EC2 vpc-id PowerShell .\\fail_instance.ps1 vpc-id The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated .","title":"3.1 EC2 failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#32-system-response-to-ec2-instance-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.2 System response to EC2 instance failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#321-system-availability","text":"Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id )","title":"3.2.1 System availability"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#322-load-balancing","text":"Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts","title":"3.2.2 Load balancing"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#323-auto-scaling","text":"Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting Click on the Activity History tab and observe: The screen cap below shows that all three instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand","title":"3.2.3 Auto scaling"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#324-ec2-failure-injection-conclusion","text":"Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.","title":"3.2.4 EC2 failure injection - conclusion"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#33-rds-failure-injection","text":"This failure injection will simulate a critical failure of the Amazon RDS DB instance. Before you initiate the failure simulation, refresh the service website several times. Every time the image is loaded, the website writes a record to the Amazon RDS database Click on Click here to go to other page and it will show the latest ten entries in the Amazon RDS DB. Click again to return to the image page. Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds From the RDS dashboard Click on \"DB Instances (1/40)\" Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) Select the Configuration tab Look at the configured values. Note the following: Value of the Info field is Available RDS DB is configured to be Multi-AZ . The primary DB instance is in AZ us-east-2a and the standby DB instance is in AZ us-east-2b To failover of the RDS instance, use the VPC ID as the command line argument replacing vpc-id in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./failover_rds.sh vpc-id Python python fail_rds.py vpc-id Java java -jar app-resiliency-1.0.jar RDS vpc-id C# .\\AppResiliency RDS vpc-id PowerShell .\\failover_rds.ps1 vpc-id The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt","title":"3.3 RDS failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#34-system-response-to-rds-instance-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.4 System response to RDS instance failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#341-system-availability","text":"The website is not available. Some errors you might see reported: No Response / Timeout : Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out : Amazon Elastic Load Balancer has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway : The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site can\u2019t be reached . This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests. Continue on to the next steps, periodically returning to attempt to refresh the website.","title":"3.4.1 System availability"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#342-failover-to-standby","text":"On the database console Configuration tab Refresh and note the values of the Info field. It will ultimately return to Available then the failover is complete Note the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. From the AWS RDS console, click on the Logs events tab and scroll down to Recent events . You should see entries like those below. In this case failover took less than a minute. Mon, 14 Oct 2019 19:53:37 GMT - Multi-AZ instance failover started. Mon, 14 Oct 2019 19:53:45 GMT - DB instance restarted Mon, 14 Oct 2019 19:54:21 GMT - Multi-AZ instance failover completed","title":"3.4.2 Failover to standby"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#342-ec2-server-replacement","text":"From the AWS RDS console, click on the Monitoring tab and look at DB connections As the failover happens the existing three servers all cannot connect to the DB AWS Auto Scaling detects this (any server not returning an http 200 status is deemed unhealthy), and replaces the three EC2 instances with new ones that establish new connections to the new RDS primary instance The graph shows an unavailability period of about four minutes until at least one DB connection is re-established [optional] Go to the Auto scaling group and AWS Elastic Load Balancer Target group consoles to see how EC2 instance and traffic routing was handled","title":"3.4.2 EC2 server replacement"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#343-rds-failure-injection-conclusion","text":"AWS RDS Database failover took less than a minute Time for AWS Auto Scaling to detect that the instances were unhealthy and to start up new ones took four minutes. This resulted in a four minute non-availability event. Learn more : After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments.","title":"3.4.3 RDS failure injection - conclusion"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#36-az-failure-injection","text":"This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more. Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in. A good way to run the AZ failure injection is first in an AZ other than this - we'll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we'll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios. To simulate failure of an AZ, select one of the Availability Zones used by your service ( us-east-2a , us-east-2b , or us-east-2c ) as az For scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b use your VPC ID as vpc-id Select one (and only one) of the scripts/programs below. (choose the language that you setup your environment for). Language Command Bash ./fail_az.sh az vpc-id Python python fail_az.py vpc-id az Java java -jar app-resiliency-1.0.jar AZ vpc-id az C# .\\AppResiliency AZ vpc-id az PowerShell .\\fail_az.ps1 az vpc-id The specific output will vary based on the command used. Note whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance","title":"3.6 AZ failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#37-system-response-to-az-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.7 System response to AZ failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#371-system-availability","text":"Refresh the service website several times Scenario 1 : If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2 : If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur.","title":"3.7.1 System availability"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#372-scenario-1-load-balancer-and-web-server-tiers","text":"This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens you as for that test: EC2 Instances Load Balancer Target group Auto Scaling Groups One difference from the EC2 failure test that you will observe is that auto scaling will bring up the replacement EC2 instance in an AZ that already has an EC2 instance as it attempts to balance the requested three EC2 instances across the remaining AZs.","title":"3.7.2 Scenario 1 - Load balancer and web server tiers"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#373-scenario-2-load-balancer-web-server-and-data-tiers","text":"This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console , navigate to your DB screen and observe the following tabs: Configuration Monitoring Logs Events","title":"3.7.3 Scenario 2 - Load balancer, web server, and data tiers"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#374-az-failure-injection-conclusion","text":"This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure.","title":"3.7.4 AZ failure injection - conclusion"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#374-az-failure-recovery","text":"This step is optional. To simulate the AZ returning to health do the following: Go to the Auto Scaling Group console Select the WebServersforResiliencyTesting auto scaling group Actions Edit In the Subnet field add the two ResiliencyVPC-PrivateSubnet s that are missing and Save Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions Edit subnet associations Uncheck all boxes and click Edit Actions Delete network ACL Note how the auto scaling redistributes the EC2 serves across the availability zones","title":"3.7.4 AZ failure recovery"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#38-s3-failure-injection","text":"Failure of S3 means that the image will not be available You may ONLY do this testing if you supplied your own websiteimage reference to an S3 bucket you control","title":"3.8 S3 failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#381-bucket-name","text":"You will need to know the bucket name where your image is. For example if the websiteimage value you supplied was \"https://s3.us-east-2.amazonaws.com/my-awesome-bucketname/my_image.jpg\" , then the bucket name is my-awesome-bucketname For this failure simulation it is most straightforward to use the AWS Console as follows. (If you are interested in doing this using the AWS CLI then see here - choose either AWS Console or AWS CLI)","title":"3.8.1 Bucket name"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#aws-console","text":"Navigate to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the object, then select the \"Permissions\" tab Select the \"Public Access\" radio button, and deselect the \"Read object\" checkbox and Save To re-enable access (after testing), do the same steps, tick the \"Read object\" checkbox and Save","title":"AWS Console"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#383-system-response-to-s3-failure","text":"What is the expected effect? How long does it take to take effect? Note that due to browser caching you may still see the image on refreshing the site. On most systems Shift-F5 does a clean refresh with no cache How would you diagnose if this is a larger problem than permissions?","title":"3.8.3 System response to S3 failure "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#39-more-testing-you-can-do","text":"You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal the failure modes.","title":"3.9 More testing you can do"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#4-tear-down-this-lab","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account : You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions","title":"4. Tear down this lab "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#remove-manually-provisioned-resources","text":"Some resources were created by the failure simulation scripts. You need to remove these first Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions Edit subnet associations Uncheck all boxes and click Edit Actions Delete network ACL","title":"Remove manually provisioned resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#remove-aws-cloudformation-provisioned-resources","text":"As part of lab setup you have deployed several AWS CloudFormation stacks. These directions will show you: How to delete an AWS CloudFormation stack In what specific order the stacks must be deleted","title":"Remove AWS CloudFormation provisioned resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#how-to-delete-an-aws-cloudformation-stack","text":"Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events","title":"How to delete an AWS CloudFormation stack"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#delete-workshop-cloudformation-stacks","text":"Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal","title":"Delete workshop CloudFormation stacks"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#single-region","text":"If you deployed the single region option, then delete your stacks in the following order Order CloudFormation stack 1 WebServersforResiliencyTesting 1 MySQLforResiliencyTesting 2 ResiliencyVPC 2 DeployResiliencyWorkshop","title":"Single region"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#multi-region","text":"If you deployed the multi region option, then see these instructions for the order in which to delete the CloudFormation stacks","title":"Multi region"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#delete-remaining-resources","text":"The password(s) for your Amazon RDS instances were stored in AWS Systems Manager secure parameter store. These steps will verify the parameter(s) were deleted, and if not then guide you to deleting them. single region You only need to do the following steps in us-east-2 multi region Do the following steps for both us-east-2 and us-west- 2 Select the region Wait until ResiliencyVPC CloudFormation stack is DELETE_COMPLETE in the region Go to the AWS Console for AWS Systems Manager parameter store Look for the parameter created for your infrastructure. If you used our default values, this will be named 300-ResiliencyofEC2RDSandS3 If it is not present (check all regions you deployed to) then you are finished If it is present then Click on the parameter name Click the Delete button Click Delete again","title":"Delete remaining resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#references-useful-resources","text":"EC2 Auto Scaling Groups What Is an Application Load Balancer? High Availability (Multi-AZ) for Amazon RDS Amazon RDS Under the Hood: Multi-AZ Regions and Availability Zones Injecting Chaos to Amazon EC2 using AWS System Manager Build a serverless multi-region, active-active backend solution in an hour","title":"References &amp; useful resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html","text":"Troubleshooting Guide for 300 - Testing for Resiliency of EC2, RDS, and S3 Introduction The purpose of this guide is to prepare for the expected questions and problems. Common AWS Account Problems If running these labs on your own, you will need to use an AWS account that meets the following qualifications. If you are at a live workshop, you may have been supplied with an AWS account for the lab. If not, and you cannot remedy your account issues, please see a proctor who can help pair you with another student who does have these permissions and you can \u201cpair lab\u201d. You will need to be able to log into the console as a user with permissions to run CloudFormation. If you do not have permission to run CloudFormation, please create a new IAM User with these permissions or use a different AWS account. The next most common problem in deploying the test application is exceeding the default limit of 5 Elastic IPs in an account. The VPC is created with 3 NAT Gateways, which each require an EIP. You will either will have to release some that you are using, or use a different AWS account. The service linked roles may exist already in an account. If they do, you will see a failure to deploy the first CFN stack for lambda_functions_for_deploy.json . You should delete the stack and redeploy it, but please make sure you are appropriately setting the Boolean parameters of the deployment machine stack. If at a live workshop, please see a proctor if you need more help with this. Problems with Service Linked Roles If you don\u2019t see the existing service linked IAM Role and try to create it in the deployment machine, it will not deploy. It will fail back with an error that the Role already exists under another name. Simply set the parameter to false and redeploy. Problems with the Step Functions State Machine and/or Lambda Functions The state machine is idempotent and can be re-run if something times out. If a function fails, you can debug it by creating a test for the Lambda Function. For example, to the test the DeployVPC Lambda function, navigate to the StepFunctions console, and select the DeployVPC function in the Visual Workflow, and click on the Input in the Step details on the right: Once you\u2019ve clicked the Input, you can select the input and copy it into the copy buffer: Then navigate to the Lambda console and click on the DeployVPC Lambda Function: You can then click the down arrow to the left of the \u201cTest\u201d button with the grayed text \u201cSelect a test event..\u201d and click on \u201cConfigure test events:\u201d Name the event TestDeployVPC and insert the copied input from the step function, then click \u201cCreate:\u201d Now you can click the \u201cTest\u201d button to execute the test: After execution, you can click on the \u201cDetails\u201d and see the log of the function to determine what went wrong: You can also go to the CloudWatch logs to see details of the execution. DeployRDS step fails If you get the following error for DeployRDS then there is an obsolete DB password stored in SSM Parameter store. This can happen if you had a problem with deployment, stopped it, and then restarted it. The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. Solution: In the AWS console go to SSM Parameter store Delete the parameters stored there Go to the CloudFormation console and delete (roll back) the ResiliencyVPC stack Resume by re-starting deployment of the infrastructure Note you will need to use a new name, such as BuildResiliency2 RDSStackCompleteChoice - DeployFailedStatus If your deployment machine fails and looks like this And if the following is true: click on the RDSStackCompleteChoice stage of your workflow select Output RDS stack shows status as CREATE_IN_PROGRESS \"rds\": { \"stackname\": \"MySQLforResiliencyTesting\", \"status\": \"CREATE_IN_PROGRESS\" } Then it is likley that your RDS deployment timed out before the workflow could complete. Do the following to continue: Go to the CloudFormation console Verify the status for MySQLforResiliencyTesting is CREATE_COMPLETE Go back to your state machine Click New Execution Give your execution a new name, unique from previous ones (such as \"BuildResiliency3\") The workflow will quickly determine which stacks have already been deployed, and start immediately on the final (web server) stack. Problems Executing the Scripts If you are not using Amazon Linux, you will need to install the AWS CLI. Installing jq is pretty easy, just download the executable and install it where the PATH will see it. Older versions of bash and the windows bash implementation complain about the { and } characters in the sed commands. They can be deleted in older version of bash (but note that they are required in newer versions of bash). Assisting with the Failure Tests Failure modes individual The shell script will delete the first instance it finds running the VPC. There shouldn\u2019t be a problem with the jq parser of the JSON returned from the ec2 describe-instances but it is theoretically possible. If you get a jq error, it is more likely the web layer has not actually been created correctly. If you see null s in output messages, it is possible that you are not specifying the correct VPC ID. Installing boto3 via pip on an Amazon Linux instance will cause you to have errors on the command line: $ aws help Traceback (most recent call last): File \"/usr/local/bin/aws\", line 19, in \\ module\\ import awscli.clidriver File \"/usr/local/lib/python2.7/dist-packages/awscli/clidriver.py\", line 19, in \\ module\\ from botocore.hooks import AliasedEventEmitter ImportError: cannot import name AliasedEventEmitter To fix this, you need to remove the aws-cli , downgrade boto , and install an older version of the aws-cli : $ sudo yum remove aws-cli $ sudo yum downgrade python27-botocore 1.8 $ sudo yum install aws-cli-1.14.9 EC2 Instance Failure Some additional questions to ask yourself: Open fail_instance.sh/fail_instance.py/InstanceFailover.java/InstanceFailover.cs in an editor. How could you make this randomly select an instance? What are the concerns if you have hundreds or thousands of instances? What if you don\u2019t have an Auto Scaling group? How could you recover? How do they test EC2 AutoRecovery? The answer is to open a support ticket. This requires extra effort on our side. How would you \u201cundo\u201d this failure mode? RDS Failover The console does not update the active AZ of the RDS instance until an unknown amount of time passes (~5 min). This can make it appear it didn\u2019t fail over, but it did. You can see the failover in the events log of RDS. If you see nulls in output messages, it is possible that you are not specifying the correct VPC ID. Some additional questions to ask yourself: Why didn\u2019t the Auto Scaling Group terminate them and replace the instances? Or why did it? How could you make the application resilient to the transient failure? What if this was a single AZ RDS? How would you fail the S3 portion of the application? AZ Failure The Java and C# implementations have some improved error checking over the bash implementation, but essentially perform the same logic. This is what the failure simulation does: Loop through all the Auto Scaling Groups by calling Auto Scaling\u2019s DescribeAutoScalingGroups ; for each group, look at the AZs it is configured for. If the desired AZ in in the list, we can reconfigure the group by calling UpdateAutoScalingGroup to update the AZs to the list without this AZ in it. Call EC2\u2019s DescribeSubnets to identify the subnets in the AZ desired within the VPC. It then creates a NACL, adds entries to block ingress and egress of all ports and protocols, then calls EC2\u2019s ReplaceNetworkAclAssociation to associate the subnets with the NACL. This will cause the ELB to route traffic to the other AZs since it has Cross AZ enabled. Loop through all the RDS Instances by calling RDS DescribeInstances . If this instance\u2019s AvailabilityZone is this AZ, then if it is an RDS Multi-AZ, call RDS RebootDBInstance with ForceFailover set to True . Some additional questions to ask yourself: What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes? Region Failure Unfortunately, you need a DNS domain registered to effect a Region failover, so you won\u2019t be able to perform this failure. However, you can think about how you would simulate it. Route53 uses HealthChecks to see if the destination is available. You could use the network ACL modification above that relates to the AZ to specify that change on the health check endpoint. (A SecurityGroup modification would likely work as well). This would cause the HeathCheck to fail and the record set to use the second region. You would want to also have an alert fire on this and send a call to DMS StopReplicationTask to stop the replication if you are using DMS to replicate data to an instance in a second AWS Region. Some additional questions to ask yourself: How would you fail back? You could set up a DMS instance that is configured for the source and target going the other way, then recover by call StartReplicationTask , using a load and cdc configuration.","title":"Trouble Shooting Guide"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#troubleshooting-guide-for-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Troubleshooting Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#introduction","text":"The purpose of this guide is to prepare for the expected questions and problems.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#common-aws-account-problems","text":"If running these labs on your own, you will need to use an AWS account that meets the following qualifications. If you are at a live workshop, you may have been supplied with an AWS account for the lab. If not, and you cannot remedy your account issues, please see a proctor who can help pair you with another student who does have these permissions and you can \u201cpair lab\u201d. You will need to be able to log into the console as a user with permissions to run CloudFormation. If you do not have permission to run CloudFormation, please create a new IAM User with these permissions or use a different AWS account. The next most common problem in deploying the test application is exceeding the default limit of 5 Elastic IPs in an account. The VPC is created with 3 NAT Gateways, which each require an EIP. You will either will have to release some that you are using, or use a different AWS account. The service linked roles may exist already in an account. If they do, you will see a failure to deploy the first CFN stack for lambda_functions_for_deploy.json . You should delete the stack and redeploy it, but please make sure you are appropriately setting the Boolean parameters of the deployment machine stack. If at a live workshop, please see a proctor if you need more help with this.","title":"Common AWS Account Problems"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#problems-with-service-linked-roles","text":"If you don\u2019t see the existing service linked IAM Role and try to create it in the deployment machine, it will not deploy. It will fail back with an error that the Role already exists under another name. Simply set the parameter to false and redeploy.","title":"Problems with Service Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#problems-with-the-step-functions-state-machine-andor-lambda-functions","text":"The state machine is idempotent and can be re-run if something times out. If a function fails, you can debug it by creating a test for the Lambda Function. For example, to the test the DeployVPC Lambda function, navigate to the StepFunctions console, and select the DeployVPC function in the Visual Workflow, and click on the Input in the Step details on the right: Once you\u2019ve clicked the Input, you can select the input and copy it into the copy buffer: Then navigate to the Lambda console and click on the DeployVPC Lambda Function: You can then click the down arrow to the left of the \u201cTest\u201d button with the grayed text \u201cSelect a test event..\u201d and click on \u201cConfigure test events:\u201d Name the event TestDeployVPC and insert the copied input from the step function, then click \u201cCreate:\u201d Now you can click the \u201cTest\u201d button to execute the test: After execution, you can click on the \u201cDetails\u201d and see the log of the function to determine what went wrong: You can also go to the CloudWatch logs to see details of the execution.","title":"Problems with the Step Functions State Machine and/or Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#deployrds-step-fails","text":"If you get the following error for DeployRDS then there is an obsolete DB password stored in SSM Parameter store. This can happen if you had a problem with deployment, stopped it, and then restarted it. The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. Solution: In the AWS console go to SSM Parameter store Delete the parameters stored there Go to the CloudFormation console and delete (roll back) the ResiliencyVPC stack Resume by re-starting deployment of the infrastructure Note you will need to use a new name, such as BuildResiliency2","title":"DeployRDS step fails"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#rdsstackcompletechoice-deployfailedstatus","text":"If your deployment machine fails and looks like this And if the following is true: click on the RDSStackCompleteChoice stage of your workflow select Output RDS stack shows status as CREATE_IN_PROGRESS \"rds\": { \"stackname\": \"MySQLforResiliencyTesting\", \"status\": \"CREATE_IN_PROGRESS\" } Then it is likley that your RDS deployment timed out before the workflow could complete. Do the following to continue: Go to the CloudFormation console Verify the status for MySQLforResiliencyTesting is CREATE_COMPLETE Go back to your state machine Click New Execution Give your execution a new name, unique from previous ones (such as \"BuildResiliency3\") The workflow will quickly determine which stacks have already been deployed, and start immediately on the final (web server) stack.","title":"RDSStackCompleteChoice -&gt; DeployFailedStatus"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#problems-executing-the-scripts","text":"If you are not using Amazon Linux, you will need to install the AWS CLI. Installing jq is pretty easy, just download the executable and install it where the PATH will see it. Older versions of bash and the windows bash implementation complain about the { and } characters in the sed commands. They can be deleted in older version of bash (but note that they are required in newer versions of bash).","title":"Problems Executing the Scripts"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#assisting-with-the-failure-tests","text":"","title":"Assisting with the Failure Tests"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#failure-modes-individual","text":"The shell script will delete the first instance it finds running the VPC. There shouldn\u2019t be a problem with the jq parser of the JSON returned from the ec2 describe-instances but it is theoretically possible. If you get a jq error, it is more likely the web layer has not actually been created correctly. If you see null s in output messages, it is possible that you are not specifying the correct VPC ID. Installing boto3 via pip on an Amazon Linux instance will cause you to have errors on the command line: $ aws help Traceback (most recent call last): File \"/usr/local/bin/aws\", line 19, in \\ module\\ import awscli.clidriver File \"/usr/local/lib/python2.7/dist-packages/awscli/clidriver.py\", line 19, in \\ module\\ from botocore.hooks import AliasedEventEmitter ImportError: cannot import name AliasedEventEmitter To fix this, you need to remove the aws-cli , downgrade boto , and install an older version of the aws-cli : $ sudo yum remove aws-cli $ sudo yum downgrade python27-botocore 1.8 $ sudo yum install aws-cli-1.14.9","title":"Failure modes individual"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#ec2-instance-failure","text":"Some additional questions to ask yourself: Open fail_instance.sh/fail_instance.py/InstanceFailover.java/InstanceFailover.cs in an editor. How could you make this randomly select an instance? What are the concerns if you have hundreds or thousands of instances? What if you don\u2019t have an Auto Scaling group? How could you recover? How do they test EC2 AutoRecovery? The answer is to open a support ticket. This requires extra effort on our side. How would you \u201cundo\u201d this failure mode?","title":"EC2 Instance Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#rds-failover","text":"The console does not update the active AZ of the RDS instance until an unknown amount of time passes (~5 min). This can make it appear it didn\u2019t fail over, but it did. You can see the failover in the events log of RDS. If you see nulls in output messages, it is possible that you are not specifying the correct VPC ID. Some additional questions to ask yourself: Why didn\u2019t the Auto Scaling Group terminate them and replace the instances? Or why did it? How could you make the application resilient to the transient failure? What if this was a single AZ RDS? How would you fail the S3 portion of the application?","title":"RDS Failover"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#az-failure","text":"The Java and C# implementations have some improved error checking over the bash implementation, but essentially perform the same logic. This is what the failure simulation does: Loop through all the Auto Scaling Groups by calling Auto Scaling\u2019s DescribeAutoScalingGroups ; for each group, look at the AZs it is configured for. If the desired AZ in in the list, we can reconfigure the group by calling UpdateAutoScalingGroup to update the AZs to the list without this AZ in it. Call EC2\u2019s DescribeSubnets to identify the subnets in the AZ desired within the VPC. It then creates a NACL, adds entries to block ingress and egress of all ports and protocols, then calls EC2\u2019s ReplaceNetworkAclAssociation to associate the subnets with the NACL. This will cause the ELB to route traffic to the other AZs since it has Cross AZ enabled. Loop through all the RDS Instances by calling RDS DescribeInstances . If this instance\u2019s AvailabilityZone is this AZ, then if it is an RDS Multi-AZ, call RDS RebootDBInstance with ForceFailover set to True . Some additional questions to ask yourself: What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes?","title":"AZ Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#region-failure","text":"Unfortunately, you need a DNS domain registered to effect a Region failover, so you won\u2019t be able to perform this failure. However, you can think about how you would simulate it. Route53 uses HealthChecks to see if the destination is available. You could use the network ACL modification above that relates to the AZ to specify that change on the health check endpoint. (A SecurityGroup modification would likely work as well). This would cause the HeathCheck to fail and the record set to use the second region. You would want to also have an alert fire on this and send a call to DMS StopReplicationTask to stop the replication if you are using DMS to replicate data to an instance in a second AWS Region. Some additional questions to ask yourself: How would you fail back? You could set up a DMS instance that is configured for the source and target going the other way, then recover by call StartReplicationTask , using a load and cdc configuration.","title":"Region Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html","text":"Setup AWS credentials and configuration You will supply configuration and credentials used by the AWS SDK to access your AWS account. You identified these credentials back in step 1 of the Lab Guide Choose an option Choose ONLY ONE option, either Option 1 , Option 2 , or Option 3 Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1 If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3 You should have already copied the credentials for your account. If not then follow the directions here The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below export AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below export AWS_DEFAULT_OUTPUT=json Note that if you end your session, or start a new one, you will need to re-execute the export statements If you completed Option 1 then STOP HERE and return to the Lab Guide Option 2 AWS CLI This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option 3 To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 Default output format [None]: json Option 3 Manually creating credential files If you already did Option 2 , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 output = json Configure a session token as part of your credentials If you used Option 2 or Option 3 , please follow these steps: Determine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \"yes\", you need to configure a session token Edit the file ~/.aws/credentials The default profile will already be present. Under it add an entry for aws_session_token [default] aws_access_key_id = aws_secret_access_key = aws_session_token = Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these systems preferentially will use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux # Use echo $varname to see if it ise set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE Click here to return to the Lab Guide","title":"Setup AWS credentials and configuration"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#setup-aws-credentials-and-configuration","text":"You will supply configuration and credentials used by the AWS SDK to access your AWS account. You identified these credentials back in step 1 of the Lab Guide","title":"Setup AWS credentials and configuration"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#choose-an-option","text":"Choose ONLY ONE option, either Option 1 , Option 2 , or Option 3","title":"Choose an option"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-1-for-instructor-supplied-aws-accounts","text":"If BOTH of the following are true then you may use Option 1 If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3 You should have already copied the credentials for your account. If not then follow the directions here The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below export AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below export AWS_DEFAULT_OUTPUT=json Note that if you end your session, or start a new one, you will need to re-execute the export statements If you completed Option 1 then STOP HERE and return to the Lab Guide","title":"Option 1 For instructor supplied AWS accounts"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-2-aws-cli","text":"This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option 3 To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: Your AWS Access Key ID AWS Secret Access Key [**************xxxx]: Your AWS Secret Access Key Default region name: [us-east-2]: us-east-2 Default output format [None]: json","title":"Option 2 AWS CLI"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-3-manually-creating-credential-files","text":"If you already did Option 2 , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = Your access key aws_secret_access_key = Your secret key Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 output = json","title":"Option 3 Manually creating credential files"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#configure-a-session-token-as-part-of-your-credentials","text":"If you used Option 2 or Option 3 , please follow these steps: Determine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \"yes\", you need to configure a session token Edit the file ~/.aws/credentials The default profile will already be present. Under it add an entry for aws_session_token [default] aws_access_key_id = aws_secret_access_key = aws_session_token =","title":"Configure a session token as part of your credentials"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#clear-environment-variables","text":"If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these systems preferentially will use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux # Use echo $varname to see if it ise set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE Click here to return to the Lab Guide","title":"Clear environment variables"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html","text":"CloudFormation Parameters All entries are Case-Sensitive single region stack Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip multi region stack Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true DMSLambdaKey Reliability/DMSLambda.zip LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip RDSRRLambdaKey Reliability/RDSReadReplicaLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip Click here to return to the Lab Guide","title":"CloudFormation Parameters"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html#cloudformation-parameters","text":"All entries are Case-Sensitive","title":"CloudFormation Parameters"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html#single-region-stack","text":"Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip","title":"single region stack"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html#multi-region-stack","text":"Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true DMSLambdaKey Reliability/DMSLambda.zip LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip RDSRRLambdaKey Reliability/RDSReadReplicaLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip Click here to return to the Lab Guide","title":"multi region stack"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Event_Data.html","text":"New Execution Input for multi region Deployment On the \"New execution\" dialog, for \"Enter an execution name\" enter BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. multi region uses the following values { \"region1\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"secondary_region_name\": \"us-west-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" }, \"region2\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-west-2\", \"secondary_region_name\": \"us-east-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } } Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Click here to return to the Lab Guide","title":"New Execution Input for **multi region** Deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Event_Data.html#new-execution-input-for-multi-region-deployment","text":"On the \"New execution\" dialog, for \"Enter an execution name\" enter BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. multi region uses the following values { \"region1\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"secondary_region_name\": \"us-west-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" }, \"region2\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-west-2\", \"secondary_region_name\": \"us-east-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } } Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Click here to return to the Lab Guide","title":"New Execution Input for multi region Deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Stack_Deletion.html","text":"Delete workshop CloudFormation stacks - Multi region deployment Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal Order CloudFormation stack Region 1 DMSforResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Oregon 1 DMSforResiliencyTesting Ohio 1 MySQLReadReplicaResiliencyTesting Ohio 2 WebServersforResiliencyTesting Ohio 2 MySQLforResiliencyTesting Ohio 3 ResiliencyVPC Ohio 3 DeployResiliencyWorkshop Ohio Click here to return to the Lab Guide","title":"Delete workshop CloudFormation stacks - Multi region deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Stack_Deletion.html#delete-workshop-cloudformation-stacks-multi-region-deployment","text":"Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal Order CloudFormation stack Region 1 DMSforResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Oregon 1 DMSforResiliencyTesting Ohio 1 MySQLReadReplicaResiliencyTesting Ohio 2 WebServersforResiliencyTesting Ohio 2 MySQLforResiliencyTesting Ohio 3 ResiliencyVPC Ohio 3 DeployResiliencyWorkshop Ohio Click here to return to the Lab Guide","title":"Delete workshop CloudFormation stacks - Multi region deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_State_Machine.html","text":"Multi Region State Machine Click here to return to the Lab Guide","title":"Multi Region State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_State_Machine.html#multi-region-state-machine","text":"Click here to return to the Lab Guide","title":"Multi Region State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html","text":"Setting up an environment to run the workshop using a programming language If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps 1. Set up AWS credentials If you have not yet setup your AWS credentials, then follow this guide 2. Language specific setup Choose the appropriate section below for your language 2.1 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the resiliency Python scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: python/fail_instance.py python/fail_rds.py python/fail_az.py 2.2 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B . Option A: If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B: Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line 2.3 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 2.4 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the resiliency PowerShell scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: powershell/fail_instance.sh powershell/failover_rds.sh powershell/fail_az.sh If your PowerShell scripts are refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#setting-up-an-environment-to-run-the-workshop-using-a-programming-language","text":"If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#1-set-up-aws-credentials","text":"If you have not yet setup your AWS credentials, then follow this guide","title":"1. Set up AWS credentials"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#2-language-specific-setup","text":"Choose the appropriate section below for your language","title":"2. Language specific setup"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#21-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the resiliency Python scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: python/fail_instance.py python/fail_rds.py python/fail_az.py","title":"2.1 Setting Up the Python Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#22-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B . Option A: If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B: Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line","title":"2.2 Setting Up the Java Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#23-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.3 Setting Up the C# Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#24-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the resiliency PowerShell scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: powershell/fail_instance.sh powershell/failover_rds.sh powershell/fail_az.sh If your PowerShell scripts are refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"2.4 Setting up the Powershell Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html","text":"Disable All Public Read Access to an S3 Bucket using AWS CLI Disable read access to S3 bucket This command will disable public read from an entire bucket. If you want to only disable public read from one object, use the AWS Console instructions If your S3 bucket is in a different aWS account, you will need to provide credentials for that account first. aws ssm start-automation-execution --document-name AWS-DisableS3BucketPublicReadWrite --parameters \"{\\\"S3BucketName\\\": [\\\" bucket-name \\\"]}\" Return to the Lab Guide , but keep this page open if you want to re-enable public read access to the bucket after testing. Re-enable access (after testing) using the S3 console This requires using the S3 console. Go to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the \"Permissions\" tab Click Edit (upper-right) Un-check all the boxes Click Save You are asked to type \"confirm\" - this is a security feature to ensure you truly intend this bucket to allow public access. Click here to return to the Lab Guide","title":"Disable All Public Read Access to an S3 Bucket using AWS CLI"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html#disable-all-public-read-access-to-an-s3-bucket-using-aws-cli","text":"","title":"Disable All Public Read Access to an S3 Bucket using AWS CLI"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html#disable-read-access-to-s3-bucket","text":"This command will disable public read from an entire bucket. If you want to only disable public read from one object, use the AWS Console instructions If your S3 bucket is in a different aWS account, you will need to provide credentials for that account first. aws ssm start-automation-execution --document-name AWS-DisableS3BucketPublicReadWrite --parameters \"{\\\"S3BucketName\\\": [\\\" bucket-name \\\"]}\" Return to the Lab Guide , but keep this page open if you want to re-enable public read access to the bucket after testing.","title":"Disable read access to S3 bucket"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html#re-enable-access-after-testing-using-the-s3-console","text":"This requires using the S3 console. Go to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the \"Permissions\" tab Click Edit (upper-right) Un-check all the boxes Click Save You are asked to type \"confirm\" - this is a security feature to ensure you truly intend this bucket to allow public access. Click here to return to the Lab Guide","title":"Re-enable access (after testing) using the S3 console"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html","text":"Creating new AWS credentials for your AWS account Use these instructions to get a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY which you will need for the workshop If you are using your own AWS account These instructions are for you. Use this guide if you are running the workshop on your own, or with at an event using your own AWS account you have brought with you If you are attending an in-person workshop and were provided with an AWS account by the instructor STOP -- Follow these instructions instead Create new AWS credentials for an IAM User you already control Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users . Choose the name of the user whose access keys you want to manage, and then choose the Security credentials tab. Choose Create access key . Then choose Download .csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close. Create a new IAM User for use in the lab Use the instructions only if you cannot Create new AWS credentials for an IAM User you already control . If you have already obtained a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY using the preceding instructions then STOP and Click here to return to the Lab Guide Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users and then choose Add user . Type the user name for the new user. if you wish you can choose rel300-workshop Select programmatic access . Including access to the AWS Management Console is optional Choose Next: Permissions select Attach existing policies to user directly In the search box type PowerUserAccess tick the check box next to PowerUserAccess Choose Next: Tags Choose Next: Review Choose Create User IMPORTANT : Choose Download.csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close. Click here to return to the Lab Guide","title":"Creating new AWS credentials for your AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html#creating-new-aws-credentials-for-your-aws-account","text":"Use these instructions to get a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY which you will need for the workshop If you are using your own AWS account These instructions are for you. Use this guide if you are running the workshop on your own, or with at an event using your own AWS account you have brought with you If you are attending an in-person workshop and were provided with an AWS account by the instructor STOP -- Follow these instructions instead","title":"Creating new AWS credentials for your AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html#create-new-aws-credentials-for-an-iam-user-you-already-control","text":"Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users . Choose the name of the user whose access keys you want to manage, and then choose the Security credentials tab. Choose Create access key . Then choose Download .csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close.","title":"Create new AWS credentials for an IAM User you already control"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html#create-a-new-iam-user-for-use-in-the-lab","text":"Use the instructions only if you cannot Create new AWS credentials for an IAM User you already control . If you have already obtained a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY using the preceding instructions then STOP and Click here to return to the Lab Guide Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users and then choose Add user . Type the user name for the new user. if you wish you can choose rel300-workshop Select programmatic access . Including access to the AWS Management Console is optional Choose Next: Permissions select Attach existing policies to user directly In the search box type PowerUserAccess tick the check box next to PowerUserAccess Choose Next: Tags Choose Next: Review Choose Create User IMPORTANT : Choose Download.csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close. Click here to return to the Lab Guide","title":"Create a new IAM User for use in the lab"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html","text":"Service-Linked Roles Does AWS account already have service-linked roles AWS requires \u201cservice-linked\u201d roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. If your AWS account has been previously been used, then these roles may already exist as they would have been automatically created for you. You will determine if any of the following three IAM service-linked roles already exists in the AWS account you are using for this workshop: AWSServiceRoleForElasticLoadBalancing AWSServiceRoleForAutoScaling AWSServiceRoleForRDS Steps to determine if service-linked roles already exist Open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for the three roles. In this screenshot, the service linked role for AutoScaling exists ( AWSServiceRoleForAutoScaling ), but the roles for Elastic Load Balancing and RDS do not. Note which roles already exist as you will use this information when performing the next step. STOP HERE and return to the Lab Guide Learn more : After the lab see the AWS documentation on Service-Linked Roles Setup CloudFormation for service-linked roles If you are using your own AWS account : Then use these instructions when entering CloudFormation parameters If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go to back to the Lab Guide If you already have this role ...then set this parameter false AWSServiceRoleForElasticLoadBalancing CreateTheELBServiceRole AWSServiceRoleForAutoScaling CreateTheAutoScalingServiceRole AWSServiceRoleForRDS CreateTheRDSServiceRole If the service-linked role does not already exist, then leave the parameter value as true Leave all the other parameter values at their default values Click here to return to the Lab Guide","title":"Service-Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#service-linked-roles","text":"","title":"Service-Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#does-aws-account-already-have-service-linked-roles","text":"AWS requires \u201cservice-linked\u201d roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. If your AWS account has been previously been used, then these roles may already exist as they would have been automatically created for you. You will determine if any of the following three IAM service-linked roles already exists in the AWS account you are using for this workshop: AWSServiceRoleForElasticLoadBalancing AWSServiceRoleForAutoScaling AWSServiceRoleForRDS","title":"Does AWS account already have service-linked roles "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#steps-to-determine-if-service-linked-roles-already-exist","text":"Open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for the three roles. In this screenshot, the service linked role for AutoScaling exists ( AWSServiceRoleForAutoScaling ), but the roles for Elastic Load Balancing and RDS do not. Note which roles already exist as you will use this information when performing the next step. STOP HERE and return to the Lab Guide Learn more : After the lab see the AWS documentation on Service-Linked Roles","title":"Steps to determine if service-linked roles already exist"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#setup-cloudformation-for-service-linked-roles","text":"If you are using your own AWS account : Then use these instructions when entering CloudFormation parameters If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go to back to the Lab Guide If you already have this role ...then set this parameter false AWSServiceRoleForElasticLoadBalancing CreateTheELBServiceRole AWSServiceRoleForAutoScaling CreateTheAutoScalingServiceRole AWSServiceRoleForRDS CreateTheRDSServiceRole If the service-linked role does not already exist, then leave the parameter value as true Leave all the other parameter values at their default values Click here to return to the Lab Guide","title":"Setup CloudFormation for service-linked roles "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html","text":"Software Install This reference will help you install software necessary to setup your workshop environment AWS CLI jq AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS. Linux This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here STOP HERE and return to the Lab Guide jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Linux Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq : $ sudo apt-get install jq ...(lots of output)... $ jq --version jq-1.5-1-a5b5cbe * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Software Install"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#software-install","text":"This reference will help you install software necessary to setup your workshop environment AWS CLI jq","title":"Software Install"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#aws-cli","text":"The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.","title":"AWS CLI "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#linux","text":"This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here STOP HERE and return to the Lab Guide","title":"Linux"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#jq","text":"jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.","title":"jq"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#linux_1","text":"Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq : $ sudo apt-get install jq ...(lots of output)... $ jq --version jq-1.5-1-a5b5cbe * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Linux"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html","text":"Accessing your instructor-provided AWS account Go to https://dashboard.eventengine.run/login Enter the 12 character hashcode you were provided and click \"Proceed\" [optional] assign a name to your account (this is referred to as \"Team name\") click \"Set Team Name\" Enter a name and click \"Set Team Name\" Click \"AWS Console\" AWS credentials IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using. Access the AWS console Click \"AWS Console\". The AWS Console will open. Click here to return to the Lab Guide","title":"Accessing your instructor-provided AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html#accessing-your-instructor-provided-aws-account","text":"Go to https://dashboard.eventengine.run/login Enter the 12 character hashcode you were provided and click \"Proceed\" [optional] assign a name to your account (this is referred to as \"Team name\") click \"Set Team Name\" Enter a name and click \"Set Team Name\" Click \"AWS Console\"","title":"Accessing your instructor-provided AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html#aws-credentials","text":"IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.","title":"AWS credentials"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html#access-the-aws-console","text":"Click \"AWS Console\". The AWS Console will open. Click here to return to the Lab Guide","title":"Access the AWS console"},{"location":"Security/README.html","text":"AWS Well-Architected Security Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper or online https://wa.aws.amazon.com/ . Also check out https://awssecworkshops.com/ for hands-on workshops, AWS Training and Certification Learning Library for official security training options. Labs Level 100: AWS Account and Root User Level 100: Basic Identity and Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 100: Enable Security Hub Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Automated Deployment of Web Application Firewall Level 200: Automated IAM User Cleanup Level 200: Basic EC2 with WAF Protection Level 200: Certificate Manager Request Public Certificate Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response with AWS Console and CLI Level 300: Lambda Cross Account IAM Role Assumption Quests Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn. Level 100: Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events. Level 100: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture. Level 200: Incident Response Day This quest is the guide for incident response workshop often ran at AWS led events. Level 300: Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits. Level 300: Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity access management, detective controls, infrastructure protection, data protection and incident response. The following quests are aligned to the security best practice questions in AWS Well-Architected. Managing Credentials Authentication Control Human Access Control Programmatic Access Detect and Investigate Events Defend Against New Threats Protect Networks Protect Compute Classify Data Protect Data at Rest Protect Data in Transit Incident Response License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Security/README.html#aws-well-architected-security-labs","text":"","title":"AWS Well-Architected Security Labs"},{"location":"Security/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper or online https://wa.aws.amazon.com/ . Also check out https://awssecworkshops.com/ for hands-on workshops, AWS Training and Certification Learning Library for official security training options.","title":"Introduction"},{"location":"Security/README.html#labs","text":"Level 100: AWS Account and Root User Level 100: Basic Identity and Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 100: Enable Security Hub Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Automated Deployment of Web Application Firewall Level 200: Automated IAM User Cleanup Level 200: Basic EC2 with WAF Protection Level 200: Certificate Manager Request Public Certificate Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response with AWS Console and CLI Level 300: Lambda Cross Account IAM Role Assumption","title":"Labs"},{"location":"Security/README.html#quests","text":"Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn. Level 100: Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events. Level 100: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture. Level 200: Incident Response Day This quest is the guide for incident response workshop often ran at AWS led events. Level 300: Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits. Level 300: Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity access management, detective controls, infrastructure protection, data protection and incident response. The following quests are aligned to the security best practice questions in AWS Well-Architected. Managing Credentials Authentication Control Human Access Control Programmatic Access Detect and Investigate Events Defend Against New Threats Protect Networks Protect Compute Classify Data Protect Data at Rest Protect Data in Transit Incident Response","title":"Quests"},{"location":"Security/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_AWS_Account_and_Root_User/README.html","text":"Level 100: AWS Account and Root User Introduction This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting AWS credentials Fine-grained authorization Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#level-100-aws-account-and-root-user","text":"","title":"Level 100: AWS Account and Root User"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html","text":"Level 100: AWS Account and Root User: Lab Guide 1. Account Settings Root User Security When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . 1.1 Generate and Review the AWS Account Credential Report Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html 1.2 Enable a Virtual MFA Device for Your AWS Account Root User You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page . 1.3 Configure Account Security Challenge Questions Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update. 1.4 Configure Account Alternate Contacts Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update. 1.5 Remove Your AWS Account Root User Access Keys You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone. 1.6 Periodically Change the AWS Account Root User Password You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ * () [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained. 1.7 Configure a Strong Password Policy for Your Users You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy. 2. Tear down this lab Please note that the changes you made to the account and root user have no charges associated with them. References useful resources AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#level-100-aws-account-and-root-user-lab-guide","text":"","title":"Level 100: AWS Account and Root User: Lab Guide"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#1-account-settings-root-user-security","text":"When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"1. Account Settings &amp; Root User Security"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#11-generate-and-review-the-aws-account-credential-report","text":"Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html","title":"1.1 Generate and Review the AWS Account Credential Report"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#12-enable-a-virtual-mfa-device-for-your-aws-account-root-user","text":"You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page .","title":"1.2 Enable a Virtual MFA Device for Your AWS Account Root User"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#13-configure-account-security-challenge-questions","text":"Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update.","title":"1.3 Configure Account Security Challenge Questions"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#14-configure-account-alternate-contacts","text":"Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update.","title":"1.4 Configure Account Alternate Contacts"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#15-remove-your-aws-account-root-user-access-keys","text":"You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone.","title":"1.5 Remove Your AWS Account Root User Access Keys"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#16-periodically-change-the-aws-account-root-user-password","text":"You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ * () [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained.","title":"1.6 Periodically Change the AWS Account Root User Password"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#17-configure-a-strong-password-policy-for-your-users","text":"You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy.","title":"1.7 Configure a Strong Password Policy for Your Users"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#2-tear-down-this-lab","text":"Please note that the changes you made to the account and root user have no charges associated with them.","title":"2. Tear down this lab"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#references-useful-resources","text":"AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working","title":"References &amp; useful resources"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html","text":"Level 100: Basic Identity and Access Management User, Group, Role Introduction This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting AWS credentials Fine-grained authorization Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#level-100-basic-identity-and-access-management-user-group-role","text":"","title":"Level 100: Basic Identity and Access Management User, Group, Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html","text":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide 1. AWS Identity Access Management As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. The following image shows what you will be doing in the next section 1.1 Create Administrator IAM User and Group. 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Users and then click Add user . For User name , type a user name for yourself, such as Bob or Alice. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. Select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. Click Next: Permissions . On the Set permissions for user page, click Add user to group . Click Create group . In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess . Then click Create group . Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. Click Next: Tags . For this lab we will not add tags to the user. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . Configure MFA on your new administrator user by choosing Users from the navigation pane. In the User Name list, click the name of the intended MFA user. Click the Security credentials tab. Next to Assigned MFA device , click the edit icon. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least privileged access approach to granting permissions, not everyone needs full administrator access! The following image shows what you will be doing in the next section 1.2 Create Administrator IAM Role. 1.2 Create Administrator IAM Role To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA , then click Next: Permissions Tick AdministratorAccess from the list, and then click Next: Tags . Click Next: Review Enter a role name, e.g. 'Administrators' then click Create role . Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. The role is now created, with full administrative access and MFA enforced. 2. Assume Administrator Role from an IAM user We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. The following image shows what you will be doing in the next section 2.1 Use Administrator Role in Web Console. 2.1 Use Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias in the Account field, and the name of the role that you created for the Administrator in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 3. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. References useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#level-100-basic-identity-and-access-management-user-group-role-lab-guide","text":"","title":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#1-aws-identity-access-management","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. The following image shows what you will be doing in the next section 1.1 Create Administrator IAM User and Group.","title":"1. AWS Identity &amp; Access Management"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Users and then click Add user . For User name , type a user name for yourself, such as Bob or Alice. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. Select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. Click Next: Permissions . On the Set permissions for user page, click Add user to group . Click Create group . In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess . Then click Create group . Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. Click Next: Tags . For this lab we will not add tags to the user. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . Configure MFA on your new administrator user by choosing Users from the navigation pane. In the User Name list, click the name of the intended MFA user. Click the Security credentials tab. Next to Assigned MFA device , click the edit icon. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least privileged access approach to granting permissions, not everyone needs full administrator access! The following image shows what you will be doing in the next section 1.2 Create Administrator IAM Role.","title":"1.1 Create Administrator IAM User and Group"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#12-create-administrator-iam-role","text":"To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA , then click Next: Permissions Tick AdministratorAccess from the list, and then click Next: Tags . Click Next: Review Enter a role name, e.g. 'Administrators' then click Create role . Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. The role is now created, with full administrative access and MFA enforced.","title":"1.2 Create Administrator IAM Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#2-assume-administrator-role-from-an-iam-user","text":"We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. The following image shows what you will be doing in the next section 2.1 Use Administrator Role in Web Console.","title":"2. Assume Administrator Role from an IAM user"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#21-use-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias in the Account field, and the name of the role that you created for the Administrator in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Administrator Role in Web Console"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#3-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them.","title":"3. Tear down this lab"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases","title":"References &amp; useful resources"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html","text":"Level 100: CloudFront with S3 Bucket Origin Introduction This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting S3 bucket from direct public access Improving access time with caching Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#level-100-cloudfront-with-s3-bucket-origin","text":"","title":"Level 100: CloudFront with S3 Bucket Origin"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#introduction","text":"This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#goals","text":"Protecting S3 bucket from direct public access Improving access time with caching","title":"Goals"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html","text":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide 1. Create S3 bucket Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . From the console dashboard, choose Create bucket . Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create . 2. Upload example index.html file Create a simple index.html file, you can create by coping the following text into your favourite text editor. !DOCTYPE html html head title Example /title /head body h1 Example Heading /h1 p Example paragraph. /p /body /html Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list. 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, click Create Distribution . Click Get Started in the Web section. Specify the following settings for the distribution: In the Origin Domain Name field Select the S3 bucket you created previously. In Restrict Bucket Access click the Yes radio then click Create a New Identity . Click the Yes, Update Bucket Policy Button . Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html Click Create Distribution. To return to the main CloudFront page click Distributions from the left navigation menu. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 4. Tear down this lab The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: Open the Amazon CloudFront console at (https://console.aws.amazon.com/cloudfront/home). From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket you created previously, then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting. References useful resources Amazon S3 Developer Guide Amazon CloudFront Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#level-100-cloudfront-with-s3-bucket-origin-lab-guide","text":"","title":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#1-create-s3-bucket","text":"Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . From the console dashboard, choose Create bucket . Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create .","title":"1. Create S3 bucket"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#2-upload-example-indexhtml-file","text":"Create a simple index.html file, you can create by coping the following text into your favourite text editor. !DOCTYPE html html head title Example /title /head body h1 Example Heading /h1 p Example paragraph. /p /body /html Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list.","title":"2. Upload example index.html file"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, click Create Distribution . Click Get Started in the Web section. Specify the following settings for the distribution: In the Origin Domain Name field Select the S3 bucket you created previously. In Restrict Bucket Access click the Yes radio then click Create a New Identity . Click the Yes, Update Bucket Policy Button . Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html Click Create Distribution. To return to the main CloudFront page click Distributions from the left navigation menu. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: Open the Amazon CloudFront console at (https://console.aws.amazon.com/cloudfront/home). From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket you created previously, then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting.","title":"4. Tear down this lab"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#references-useful-resources","text":"Amazon S3 Developer Guide Amazon CloudFront Developer Guide","title":"References &amp; useful resources"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Create_a_Data_Bunker/README.html","text":"Level 100: Create a Data Bunker Account Overview In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups. Prerequisites An multi-account structure with AWS Organizations has been setup for your organization You have access to a role with administrative access to the root account for your AWS Organization NOTE: You will be billed for the AWS CloudTrail logs and Amazon S3 storage setup as part of this lab. See AWS CloudTrail Pricing and Amazon S3 Pricing for further details. Detailed Instructions 1. (Highly reccomended) Create a Security account from the organizations master account Best practice is to have a seperate security account for your data bunker. This account should only be accessible by folks in your security group with a read only role. How you create this account will depend on your organization's policies, the instructions below are guidance on how to do this. If you do not currently have organizations setup see the quest Quick Steps to Security Success or read the multi account strategy whitepaper for a more in-depth discussion. Login to the master account of your AWS Organization If you do not have an account within your organization to store security logs. Navigate to AWS Organizations and select Create Account . Include a cross account access role and note it's name (default is OrganizationAccountAccessRole) - we will modify this later to remove unnecessary access (Optional) If your role does not have permission to assume any role you will also have to add an IAM policy. The AWS administrator policy has this by default, otherwise follow the steps in the AWS Organizations Documentation to grant permissions to access the role Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication Navigate to Settings and take a note of your Organization ID 2. Create the bucket for CloudTrail logs Swtich roles into the security account for your organization Navigate to S3 Press Create Bucket Enter a name for your bucket, make note of it and click Next Under configuration options enable versioning and enable object lock . This will prevent our logs from being deleted. Press Next Do not modify any permissions - press Next Press Create Bucket Press the bucket we just create and navigate to the Properties tab Under Object Lock , enable compliance mode and set a retention period . The length of the retention period will depend on your organisational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Pres Save { Version : 2012-10-17 , Statement : [ { Sid : AWSCloudTrailAclCheck20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:GetBucketAcl , Resource : arn:aws:s3:::[bucket] }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/[organization id]/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } } ] } (Optional) Next we will add a lifecycle policy to clean up old logs. Navigate to Management (Optional) Add a lifecycle rule named Delete old logs , press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days 3. (Highly reccomended) Ensure cross account access is read-only These instructions outline how to modify the cross account access created in step 1 is read-only. As with step 1, this will depend on how your organization's policies. The key is that our security team are not able to modify data in our data bunker. Human access should only be in a breal-glass emergency situation. Navigate to IAM and select Roles Select the organizations account access role for your orgainzation: Note: the default is OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy 4. Turn on CloudTrail from the root account Switch back to the root account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Enter a name for the trail such as OrganizationTrail Select Yes next to Apply trail to my organization Under Storage location , select No for Create new S3 bucket and enter the bucket name of the bucket created in step 2 Verification Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_Create_a_Data_Bunker/README.html#level-100-create-a-data-bunker-account","text":"","title":"Level 100: Create a Data Bunker Account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#overview","text":"In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.","title":"Overview"},{"location":"Security/100_Create_a_Data_Bunker/README.html#prerequisites","text":"An multi-account structure with AWS Organizations has been setup for your organization You have access to a role with administrative access to the root account for your AWS Organization NOTE: You will be billed for the AWS CloudTrail logs and Amazon S3 storage setup as part of this lab. See AWS CloudTrail Pricing and Amazon S3 Pricing for further details.","title":"Prerequisites"},{"location":"Security/100_Create_a_Data_Bunker/README.html#detailed-instructions","text":"","title":"Detailed Instructions"},{"location":"Security/100_Create_a_Data_Bunker/README.html#1-highly-reccomended-create-a-security-account-from-the-organizations-master-account","text":"Best practice is to have a seperate security account for your data bunker. This account should only be accessible by folks in your security group with a read only role. How you create this account will depend on your organization's policies, the instructions below are guidance on how to do this. If you do not currently have organizations setup see the quest Quick Steps to Security Success or read the multi account strategy whitepaper for a more in-depth discussion. Login to the master account of your AWS Organization If you do not have an account within your organization to store security logs. Navigate to AWS Organizations and select Create Account . Include a cross account access role and note it's name (default is OrganizationAccountAccessRole) - we will modify this later to remove unnecessary access (Optional) If your role does not have permission to assume any role you will also have to add an IAM policy. The AWS administrator policy has this by default, otherwise follow the steps in the AWS Organizations Documentation to grant permissions to access the role Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication Navigate to Settings and take a note of your Organization ID","title":"1. (Highly reccomended) Create a Security account from the organizations master account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#2-create-the-bucket-for-cloudtrail-logs","text":"Swtich roles into the security account for your organization Navigate to S3 Press Create Bucket Enter a name for your bucket, make note of it and click Next Under configuration options enable versioning and enable object lock . This will prevent our logs from being deleted. Press Next Do not modify any permissions - press Next Press Create Bucket Press the bucket we just create and navigate to the Properties tab Under Object Lock , enable compliance mode and set a retention period . The length of the retention period will depend on your organisational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Pres Save { Version : 2012-10-17 , Statement : [ { Sid : AWSCloudTrailAclCheck20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:GetBucketAcl , Resource : arn:aws:s3:::[bucket] }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } }, { Sid : AWSCloudTrailWrite20150319 , Effect : Allow , Principal : { Service : cloudtrail.amazonaws.com }, Action : s3:PutObject , Resource : arn:aws:s3:::[bucket]/AWSLogs/[organization id]/* , Condition : { StringEquals : { s3:x-amz-acl : bucket-owner-full-control } } } ] } (Optional) Next we will add a lifecycle policy to clean up old logs. Navigate to Management (Optional) Add a lifecycle rule named Delete old logs , press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days","title":"2. Create the bucket for CloudTrail logs"},{"location":"Security/100_Create_a_Data_Bunker/README.html#3-highly-reccomended-ensure-cross-account-access-is-read-only","text":"These instructions outline how to modify the cross account access created in step 1 is read-only. As with step 1, this will depend on how your organization's policies. The key is that our security team are not able to modify data in our data bunker. Human access should only be in a breal-glass emergency situation. Navigate to IAM and select Roles Select the organizations account access role for your orgainzation: Note: the default is OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy","title":"3. (Highly reccomended) Ensure cross account access is read-only"},{"location":"Security/100_Create_a_Data_Bunker/README.html#4-turn-on-cloudtrail-from-the-root-account","text":"Switch back to the root account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Enter a name for the trail such as OrganizationTrail Select Yes next to Apply trail to my organization Under Storage location , select No for Create new S3 bucket and enter the bucket name of the bucket created in step 2","title":"4. Turn on CloudTrail from the root account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#verification","text":"Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail","title":"Verification"},{"location":"Security/100_Create_a_Data_Bunker/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Enable_Security_Hub/README.html","text":"Level 100: Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Goals Enable AWS Security Hub Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_Enable_Security_Hub/README.html#level-100-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Level 100: Enable Security Hub"},{"location":"Security/100_Enable_Security_Hub/README.html#goals","text":"Enable AWS Security Hub","title":"Goals"},{"location":"Security/100_Enable_Security_Hub/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/100_Enable_Security_Hub/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_Enable_Security_Hub/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html","text":"Level 100: Enable AWS Security Hub via AWS Console Authors Pierre Liddle, Principal Security Architect Table of Contents Getting Started 1. Getting Started The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub. 1.1 AWS Security Hub Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console 1.2 Enable AWS Security Hub In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub 1.3 Explore AWS Security Hub With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers. References useful resources AWS Security Hub License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#level-100-enable-aws-security-hub-via-aws-console","text":"","title":"Level 100: Enable AWS Security Hub via AWS Console"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#authors","text":"Pierre Liddle, Principal Security Architect","title":"Authors"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#table-of-contents","text":"Getting Started","title":"Table of Contents"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#1-getting-started","text":"The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub.","title":"1. Getting Started "},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#11-aws-security-hub","text":"Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console","title":"1.1 AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#12-enable-aws-security-hub","text":"In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub","title":"1.2 Enable AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#13-explore-aws-security-hub","text":"With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers.","title":"1.3 Explore AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#references-useful-resources","text":"AWS Security Hub","title":"References &amp; useful resources"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html","text":"Level 200: Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Implement detective controls Automate security best practices Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#level-200-automated-deployment-of-detective-controls","text":"","title":"Level 200: Automated Deployment of Detective Controls"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#goals","text":"Implement detective controls Automate security best practices","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html","text":"Level 200: Automated Deployment of Detective Controls: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Deployment Knowledge Check Tear Down 1. AWS CloudFormation to configure AWS CloudTrail, AWS Config, and Amazon GuardDuty AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Download the latest version of the cloudtrail-config-guardduty.yaml CloudFormation template from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following details for each section: General Stack name: The name of this stack. For this lab, use DetectiveControls . CloudTrail: Enable CloudTrail Yes/No. If you already have CloudTrail enabled select No. Config: Enable Config Yes/No. If you already have Config enabled select No. GuardDuty: Enable GuardDuty Yes/No. If you already have GuardDuty enabled select No. Note that GuardDuty will create and leave an IAM role the first time its enabled. S3BucketPolicyExplicitDeny: (Optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. S3AccessLogsBucketName: (Optional) The name of an existing S3 bucket for storing S3 access logs. CloudTrail CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. CloudTrailCWLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. CloudTrailS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. CloudTrailEncryptS3Logs: (Optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. CloudTrailLogS3DataEvents: (Optional) These events provide insight into the resource operations performed on or within S3. Config ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. ConfigSnapshotFrequency: AWS Config configuration snapshot frequency ConfigS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. Guard Duty GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. Once you have finished entering the details for the template continue to the bottom of the page and click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! You should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format. 2. Knowledge Check The security best practices followed in this lab are: Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Implement new security services and features: New features such as Amazon GuardDuty have been adopted. Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Note: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab. Delete the stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the DetectiveControls stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets: Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. Select the CloudTrail bucket name you previously created without clicking the name. Click Empty bucket and enter the bucket name in the confirmation box. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. With the bucket now empty, click Delete bucket. Enter the bucket name in the confirmation box and click Confirm. Repeat steps 2 to 6 for the Config bucket you created. References useful resources AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#level-200-automated-deployment-of-detective-controls-lab-guide","text":"","title":"Level 200: Automated Deployment of Detective Controls: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#table-of-contents","text":"Deployment Knowledge Check Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#1-aws-cloudformation-to-configure-aws-cloudtrail-aws-config-and-amazon-guardduty","text":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Download the latest version of the cloudtrail-config-guardduty.yaml CloudFormation template from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following details for each section: General Stack name: The name of this stack. For this lab, use DetectiveControls . CloudTrail: Enable CloudTrail Yes/No. If you already have CloudTrail enabled select No. Config: Enable Config Yes/No. If you already have Config enabled select No. GuardDuty: Enable GuardDuty Yes/No. If you already have GuardDuty enabled select No. Note that GuardDuty will create and leave an IAM role the first time its enabled. S3BucketPolicyExplicitDeny: (Optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. S3AccessLogsBucketName: (Optional) The name of an existing S3 bucket for storing S3 access logs. CloudTrail CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. CloudTrailCWLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. CloudTrailS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. CloudTrailEncryptS3Logs: (Optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. CloudTrailLogS3DataEvents: (Optional) These events provide insight into the resource operations performed on or within S3. Config ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. ConfigSnapshotFrequency: AWS Config configuration snapshot frequency ConfigS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. Guard Duty GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. Once you have finished entering the details for the template continue to the bottom of the page and click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! You should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format.","title":"1. AWS CloudFormation to configure AWS CloudTrail, AWS Config, and Amazon GuardDuty "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#2-knowledge-check","text":"The security best practices followed in this lab are: Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Implement new security services and features: New features such as Amazon GuardDuty have been adopted. Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment.","title":"2. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Note: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab. Delete the stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the DetectiveControls stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets: Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. Select the CloudTrail bucket name you previously created without clicking the name. Click Empty bucket and enter the bucket name in the confirmation box. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. With the bucket now empty, click Delete bucket. Enter the bucket name in the confirmation box and click Confirm. Repeat steps 2 to 6 for the Config bucket you created.","title":"3. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#references-useful-resources","text":"AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html","text":"Level 200: Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Goals EC2 automated deployment Autoscaling and load balancing Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#goals","text":"EC2 automated deployment Autoscaling and load balancing","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC .","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html","text":"Level 200: Automated Deployment of EC2 Web Application Authors Ben Potter, Security Lead, Well-Architected Rodney Lester, Reliability Lead, Well-Architected Table of Contents Overview Create Web Stack Knowledge Check Further Considerations Tear Down 1. Overview Overview of wordpress stack architecture: 2. Create Web Stack Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method. Choose the version of the CloudFormation template and download to your computer or by cloning this repository: wordpress.yaml to create a WordPress site, including an RDS database. staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack. Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). In the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created. 3. Knowledge Check The security best practices followed in this lab are: Grant access through roles or federation: A role is attached to the auto-scaled instances. Implement dynamic authentication: The role attached to the auto-scaled instances dynamically acquires credentials. Grant least privileges: The role attached to the auto-scaled instances uses minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Automate configuration management: CloudFormation is being used to deploy the application automatically. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. Reduce attack surface: Instances do not allow for SSH, instead Systems Manager may be used for administration. Implement managed services: Managed services are utilized including Secrets Manager, Aurora serverless. Implement secure key management: AWS Key Management Service is used for key management of Aurora database. Provide mechanisms to keep people away from data: SSH to the instances is not allowed, Systems Manager may be used to control access and CloudFormation is used to deploy and update all infrastructure to reduce human error. 4. Further considerations: Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice if supported would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Encrypting the EC2 AMI for the web instances would automatically enable encrypted volumes. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront. Create an automated process for patching the AMI's and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack. 5. Tear down this lab The following instructions will remove the resources that you have created in this lab. Delete the WordPress or Static Web Application CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/ References useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected Rodney Lester, Reliability Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#table-of-contents","text":"Overview Create Web Stack Knowledge Check Further Considerations Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#1-overview","text":"Overview of wordpress stack architecture:","title":"1. Overview "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#2-create-web-stack","text":"Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method. Choose the version of the CloudFormation template and download to your computer or by cloning this repository: wordpress.yaml to create a WordPress site, including an RDS database. staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack. Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). In the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created.","title":"2. Create Web Stack "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: Grant access through roles or federation: A role is attached to the auto-scaled instances. Implement dynamic authentication: The role attached to the auto-scaled instances dynamically acquires credentials. Grant least privileges: The role attached to the auto-scaled instances uses minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Automate configuration management: CloudFormation is being used to deploy the application automatically. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. Reduce attack surface: Instances do not allow for SSH, instead Systems Manager may be used for administration. Implement managed services: Managed services are utilized including Secrets Manager, Aurora serverless. Implement secure key management: AWS Key Management Service is used for key management of Aurora database. Provide mechanisms to keep people away from data: SSH to the instances is not allowed, Systems Manager may be used to control access and CloudFormation is used to deploy and update all infrastructure to reduce human error.","title":"3. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#4-further-considerations","text":"Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice if supported would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Encrypting the EC2 AMI for the web instances would automatically enable encrypted volumes. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront. Create an automated process for patching the AMI's and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack.","title":"4. Further considerations: "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#5-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Delete the WordPress or Static Web Application CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/","title":"5. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html","text":"Level 200: Automated Deployment of IAM Groups and Roles Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Fine-grained authorization Automate security best practices Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#level-200-automated-deployment-of-iam-groups-and-roles","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#goals","text":"Fine-grained authorization Automate security best practices","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html","text":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide 1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account. 1.1 Create AWS CloudFormation Stack Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security! 2. Assume Roles from an IAM user We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user. 2.1 Use Restricted Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 2.2 Use Restricted Administrator Role in Command Line Interface (CLI) Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. References useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#level-200-automated-deployment-of-iam-groups-and-roles-lab-guide","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#1-aws-cloudformation-to-create-a-groups-policies-and-roles-with-mfa-enforced","text":"Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account.","title":"1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#11-create-aws-cloudformation-stack","text":"Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security!","title":"1.1 Create AWS CloudFormation Stack"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#2-assume-roles-from-an-iam-user","text":"We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user.","title":"2. Assume Roles from an IAM user"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#21-use-restricted-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Restricted Administrator Role in Web Console"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#22-use-restricted-administrator-role-in-command-line-interface-cli","text":"Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html","title":"2.2 Use Restricted Administrator Role in Command Line Interface (CLI)"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button.","title":"3. Tear down this lab"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html","text":"Level 200: Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Goals VPC security features VPC layered subnet architecture Automated deployments Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. We recommend you clone the Git repository for easy access to the AWS CloudFormation templates. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs.","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#goals","text":"VPC security features VPC layered subnet architecture Automated deployments","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. We recommend you clone the Git repository for easy access to the AWS CloudFormation templates.","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html","text":"Level 200: Automated Deployment of VPC Authors Ben Potter, Security Lead, Well-Architected Table of Contents Overview Create VPC Stack Knowledge Check Tear Down 1. Overview 2. Create VPC Stack This step will create the VPC and all components using the example CloudFormation template. Download the latest version of the vpc-alb-app-db.yaml CloudFormation template from file from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack. Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you). 3. Knowledge Check The security best practices followed in this lab are: Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Use of Internet Gateways and NAT Gateways in use to control traffic flows. Automate configuration management: CloudFormation is being used to deploy the networking constructs. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. 4. Tear down this lab The following instructions will remove the resources that you have created in this lab. Note: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC. Delete the VPC CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-VPC stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Delete the CloudWatch Logs: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPCFlowLog . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete . References useful resources AWS CloudFormation User Guide Amazon VPC User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#table-of-contents","text":"Overview Create VPC Stack Knowledge Check Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#1-overview","text":"","title":"1. Overview "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#2-create-vpc-stack","text":"This step will create the VPC and all components using the example CloudFormation template. Download the latest version of the vpc-alb-app-db.yaml CloudFormation template from file from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack. Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you).","title":"2. Create VPC Stack "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Use of Internet Gateways and NAT Gateways in use to control traffic flows. Automate configuration management: CloudFormation is being used to deploy the networking constructs. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables.","title":"3. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Note: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC. Delete the VPC CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-VPC stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Delete the CloudWatch Logs: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPCFlowLog . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete .","title":"4. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon VPC User Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html","text":"Level 200: Automated Deployment of Web Application Firewall Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#level-200-automated-deployment-of-web-application-firewall","text":"","title":"Level 200: Automated Deployment of Web Application Firewall"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html","text":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Configure WAF Configure CloudFront for WAF Tear Down 1. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 2. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. References useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#level-200-automated-deployment-of-web-application-firewall-lab-guide","text":"","title":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#table-of-contents","text":"Configure WAF Configure CloudFront for WAF Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#1-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"1. Configure AWS WAF "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#2-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"2. Configure Amazon CloudFront "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html","text":"Level 200: Automated IAM User Cleanup Introduction This hands-on lab will guide you through the steps to deploy a AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. You will use the AWS SAM CLI to package your deployment. Skills learned will help you secure your AWS account in alignment with the AWS Well-Architected Framework . Goals Identify orphaned IAM Users and AWS Access Keys Take action to automatically remove IAM Users and AWS Access Keys no longer needed Reduce identity sprawl Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS Lambda from the list: AWS Regions and Endpoints . AWS Serverless Application Model (SAM) installed and configured. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#level-200-automated-iam-user-cleanup","text":"","title":"Level 200: Automated IAM User Cleanup"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#introduction","text":"This hands-on lab will guide you through the steps to deploy a AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. You will use the AWS SAM CLI to package your deployment. Skills learned will help you secure your AWS account in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#goals","text":"Identify orphaned IAM Users and AWS Access Keys Take action to automatically remove IAM Users and AWS Access Keys no longer needed Reduce identity sprawl","title":"Goals"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS Lambda from the list: AWS Regions and Endpoints . AWS Serverless Application Model (SAM) installed and configured. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.","title":"Prerequisites"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html","text":"Level 200: Automated IAM User Cleanup: Lab Guide Authors Pierre Liddle, Principal Security Architect Byron Pogson, Solutions Architect Table of Contents Architecture Overview Deploying IAM Lambda Cleanup with AWS SAM 1. Architecture Overview The AWS Lambda function is triggered by a regular scheduled event in Amazon CloudWatch Events. Once the Lambda function runs to check the status of the AWS IAM Users and associated IAM Access Keys the results are sent the designated email contact via Amazon SNS. A check is also performed for unused roles. The logs from the AWS Lambda function are captured in Amazon CloudWatch Logs for review and trouble shooting purposes. 2. Deploying IAM Lambda Cleanup with AWS SAM Download the latest version of the templates from the GitHub code folder as raw objects, or by cloning this repository. Create an Amazon S3 bucket if you don't already have one, it needs to be in the same AWS region being deployed into. Now that you have the S3 bucket created and the files downloaded to your machine. You can start to create your deployment package on the command line with AWS SAM. Make sure you are working in the folder where where you have downloaded the files to. Run the following command to prepare your deployment package: `aws cloudformation package --template-file cloudformation-iam-user-cleanup.yml --output-template-file output-template.yaml --s3-bucket bucket ` Once you have finished preparing the package you can deploy the CloudFormation with AWS SAM: NOTE: The template file to use here is the output file from the previous command: aws cloudformation deploy --template-file output-template.yaml --stack-name IAM-User-Cleanup --capabilities CAPABILITY_IAM --parameter-overrides NotificationEmail= replace_with_your_email_address Once you have completed the deployment of your AWS Lambda function, test the function by going to the AWS Lambda function in your AWS account and create a dummy event by selecting test. If your test runs successfully you should receive an email from: AWS Notifications with the subject line of: IAM user cleanup from and the body of the email will have a status report from the findings. E.g. IAM Users and AWS Access Keys which require a cleanup IAM user cleanup successfully ran. User John Doe has not logged in since 2018-04-19 08:36:18+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 21:32: 00+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 20:08:00+00:00 and needs cleanup References useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS SAM CLI AWS Serverless Application Model (SAM) License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#level-200-automated-iam-user-cleanup-lab-guide","text":"","title":"Level 200: Automated IAM User Cleanup: Lab Guide"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#authors","text":"Pierre Liddle, Principal Security Architect Byron Pogson, Solutions Architect","title":"Authors"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#table-of-contents","text":"Architecture Overview Deploying IAM Lambda Cleanup with AWS SAM","title":"Table of Contents"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#1-architecture-overview","text":"The AWS Lambda function is triggered by a regular scheduled event in Amazon CloudWatch Events. Once the Lambda function runs to check the status of the AWS IAM Users and associated IAM Access Keys the results are sent the designated email contact via Amazon SNS. A check is also performed for unused roles. The logs from the AWS Lambda function are captured in Amazon CloudWatch Logs for review and trouble shooting purposes.","title":"1. Architecture Overview "},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#2-deploying-iam-lambda-cleanup-with-aws-sam","text":"Download the latest version of the templates from the GitHub code folder as raw objects, or by cloning this repository. Create an Amazon S3 bucket if you don't already have one, it needs to be in the same AWS region being deployed into. Now that you have the S3 bucket created and the files downloaded to your machine. You can start to create your deployment package on the command line with AWS SAM. Make sure you are working in the folder where where you have downloaded the files to. Run the following command to prepare your deployment package: `aws cloudformation package --template-file cloudformation-iam-user-cleanup.yml --output-template-file output-template.yaml --s3-bucket bucket ` Once you have finished preparing the package you can deploy the CloudFormation with AWS SAM: NOTE: The template file to use here is the output file from the previous command: aws cloudformation deploy --template-file output-template.yaml --stack-name IAM-User-Cleanup --capabilities CAPABILITY_IAM --parameter-overrides NotificationEmail= replace_with_your_email_address Once you have completed the deployment of your AWS Lambda function, test the function by going to the AWS Lambda function in your AWS account and create a dummy event by selecting test. If your test runs successfully you should receive an email from: AWS Notifications with the subject line of: IAM user cleanup from and the body of the email will have a status report from the findings. E.g. IAM Users and AWS Access Keys which require a cleanup IAM user cleanup successfully ran. User John Doe has not logged in since 2018-04-19 08:36:18+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 21:32: 00+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 20:08:00+00:00 and needs cleanup","title":"2. Deploying IAM Lambda Cleanup with AWS SAM "},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS SAM CLI AWS Serverless Application Model (SAM)","title":"References &amp; useful resources"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html","text":"Level 200: EC2 Web Infrastructure Protection Introduction This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#level-200-ec2-web-infrastructure-protection","text":"","title":"Level 200: EC2 Web Infrastructure Protection"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints .","title":"Prerequisites"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html","text":"Level 200: EC2 Web Infrastructure Protection: Lab Guide 1. Launch Instance For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console. 1.1 Launch Single Linux Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . ![Security Group](Images/ec2-launch-wizard-security-group.png) 7.3 Click Review and Launch. ![ec2-launch-wizard](Images/ec2-launch-wizard-launch.png) On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Create AWS WAF Rules 2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use! 3. Create Application Load Balancer with WAF integration Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test. 3.1 Create Application Load Balancer Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successful message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing. 3.2 Configure Application Load Balancer with WAF Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser. 4. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the left console instance menu, select Instances. Select the instance you created to terminate. From the Actions button (or right click) select Instance State Terminate. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. Confirm by clicking Yes, Delete. From the console dashboard, choose Target Groups from the Load Balancing section. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the lab-waf-regional stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. References useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#level-200-ec2-web-infrastructure-protection-lab-guide","text":"","title":"Level 200: EC2 Web Infrastructure Protection: Lab Guide"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#1-launch-instance","text":"For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console.","title":"1. Launch Instance"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#11-launch-single-linux-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + 6. Accept defaults and Choose Next: Add tags. 7. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.2 Select Add Rule to add both SSH and HTTP, and on source, select My IP . ![Security Group](Images/ec2-launch-wizard-security-group.png) 7.3 Click Review and Launch. ![ec2-launch-wizard](Images/ec2-launch-wizard-launch.png) On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1.1 Launch Single Linux Instance"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#2-create-aws-waf-rules","text":"","title":"2. Create AWS WAF Rules"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#21-aws-cloudformation-to-create-aws-waf-acl-for-application-load-balancer","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use!","title":"2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#3-create-application-load-balancer-with-waf-integration","text":"Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test.","title":"3. Create Application Load Balancer with WAF integration"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#31-create-application-load-balancer","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successful message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing.","title":"3.1 Create Application Load Balancer"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#32-configure-application-load-balancer-with-waf","text":"Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser.","title":"3.2 Configure Application Load Balancer with WAF"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the left console instance menu, select Instances. Select the instance you created to terminate. From the Actions button (or right click) select Instance State Terminate. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. Confirm by clicking Yes, Delete. From the console dashboard, choose Target Groups from the Load Balancing section. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the lab-waf-regional stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.","title":"4. Tear down this lab"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html","text":"Level 200: AWS Certificate Manager Request Public Certificate AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates. Goals Request AWS Certificate Manager public certificate Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . See pricing for further information on AWS Certificate Manager. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#level-200-aws-certificate-manager-request-public-certificate","text":"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#goals","text":"Request AWS Certificate Manager public certificate","title":"Goals"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . See pricing for further information on AWS Certificate Manager.","title":"Prerequisites"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html","text":"Level 200: AWS Certificate Manager Request Public Certificate Authors Ben Potter, Security Lead, Well-Architected Table of Contents Requesting a public certificate using the console Tear Down 1. Request Certificate Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select your prefferred region for regional certificates including Elastic Load Balancing, or US East (N. Virginia) for global services including Amazon CloudFront. If you see a welcome page, click Get started under provision certificates area. On the Request a certificate page, click Request a public certificate , then click Request a certificate . Type your domain name. You can use a fully qualified domain name (FQDN) such as www.example.com or a bare or apex domain name such as example.com . You can also use an asterisk * as a wildcard in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com , and images.example.com . The wildcard name will appear in the Subject field and the Subject Alternative Name extension of the ACM certificate. Note: When you request a wildcard certificate, the asterisk * must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com , and test.example.com , but it cannot protect test.login.example.com . Also note that *.example.com protects only the subdomains of example.com , it does not protect the bare or apex domain example.com . To protect both, see the next step. To add more domain names to the ACM certificate, choose Add another name to this certificate and type another domain name in the text box that opens. This is useful for protecting both a bare or apex domain (like example.com ) and its subdomains *.example.com . After you have typed valid domain names, choose Next . Before ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database and to five common system administration addresses for each domain name. You or an authorized representative must approve one of these email messages. If you use DNS validation, you simply create a CNAME record provided by ACM to your DNS configuration. Choose your option, then click Review . Note: If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See Use DNS to Validate Domain Ownership. If the review page correctly contains the information that you provided for your request, choose Confirm and request . The following page shows that your request status is pending validation. You must approve the request either through email link or DNS record. Important: Unless you choose to opt out, your certificate will be automatically recorded in at least two public certificate transparency databases. You cannot currently use the console to opt out. You must use the AWS CLI or the API. For more information, see Opting Out of Certificate Transparency Logging . For general information about transparency logs, see Certificate Transparency Logging . 8. Your certificate is now ready to associate with a supported service . 2. Tear down this lab The following instructions will remove the certificate you have created. Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select the region where you created the certificate. Click the check box for the domain name of the certificate to delete. Click Actions then Delete . Verify this is the certificate to delete and click Delete . Note: You cannot delete an ACM Certificate that is being used by another AWS service. To delete a certificate that is in use, you must first remove the certificate association. References useful resources AWS Certificate Manager Create an HTTPS Listener for Your Application Load Balancer License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#level-200-aws-certificate-manager-request-public-certificate","text":"","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#table-of-contents","text":"Requesting a public certificate using the console Tear Down","title":"Table of Contents"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#1-request-certificate","text":"Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select your prefferred region for regional certificates including Elastic Load Balancing, or US East (N. Virginia) for global services including Amazon CloudFront. If you see a welcome page, click Get started under provision certificates area. On the Request a certificate page, click Request a public certificate , then click Request a certificate . Type your domain name. You can use a fully qualified domain name (FQDN) such as www.example.com or a bare or apex domain name such as example.com . You can also use an asterisk * as a wildcard in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com , and images.example.com . The wildcard name will appear in the Subject field and the Subject Alternative Name extension of the ACM certificate. Note: When you request a wildcard certificate, the asterisk * must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com , and test.example.com , but it cannot protect test.login.example.com . Also note that *.example.com protects only the subdomains of example.com , it does not protect the bare or apex domain example.com . To protect both, see the next step. To add more domain names to the ACM certificate, choose Add another name to this certificate and type another domain name in the text box that opens. This is useful for protecting both a bare or apex domain (like example.com ) and its subdomains *.example.com . After you have typed valid domain names, choose Next . Before ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database and to five common system administration addresses for each domain name. You or an authorized representative must approve one of these email messages. If you use DNS validation, you simply create a CNAME record provided by ACM to your DNS configuration. Choose your option, then click Review . Note: If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See Use DNS to Validate Domain Ownership. If the review page correctly contains the information that you provided for your request, choose Confirm and request . The following page shows that your request status is pending validation. You must approve the request either through email link or DNS record. Important: Unless you choose to opt out, your certificate will be automatically recorded in at least two public certificate transparency databases. You cannot currently use the console to opt out. You must use the AWS CLI or the API. For more information, see Opting Out of Certificate Transparency Logging . For general information about transparency logs, see Certificate Transparency Logging . 8. Your certificate is now ready to associate with a supported service .","title":"1. Request Certificate "},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#2-tear-down-this-lab","text":"The following instructions will remove the certificate you have created. Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select the region where you created the certificate. Click the check box for the domain name of the certificate to delete. Click Actions then Delete . Verify this is the certificate to delete and click Delete . Note: You cannot delete an ACM Certificate that is being used by another AWS service. To delete a certificate that is in use, you must first remove the certificate association.","title":"2. Tear down this lab "},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#references-useful-resources","text":"AWS Certificate Manager Create an HTTPS Listener for Your Application Load Balancer","title":"References &amp; useful resources"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_for_Web_Application/README.html","text":"Level 200: CloudFront for Web Application Introduction This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront for Web Application"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#level-200-cloudfront-for-web-application","text":"","title":"Level 200: CloudFront for Web Application"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#introduction","text":"This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront.","title":"Prerequisites"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html","text":"Level 200: CloudFront for Web Application: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Configure CloudFront - EC2 or Load Balancer Tear Down 1. Configure Amazon CloudFront for EC2 or Elastic Load Balancer Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 2. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. References useful resources Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront for Web Application: Lab Guide"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#level-200-cloudfront-for-web-application-lab-guide","text":"","title":"Level 200: CloudFront for Web Application: Lab Guide"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#table-of-contents","text":"Configure CloudFront - EC2 or Load Balancer Tear Down","title":"Table of Contents"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#1-configure-amazon-cloudfront-for-ec2-or-elastic-load-balancer","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"1. Configure Amazon CloudFront for EC2 or Elastic Load Balancer"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#2-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button.","title":"2. Tear down this lab "},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#references-useful-resources","text":"Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html","text":"Level 200: CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#level-200-cloudfront-with-waf-protection","text":"","title":"Level 200: CloudFront with WAF Protection"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html","text":"Level 200: CloudFront with WAF Protection: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Launch Instance Configure WAF Configure CloudFront Tear Down 1. Launch Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + Accept defaults and click Next: Add tags . Click Next: Configure Security Group . 7.1 Accept default option Create a new security group . 7.2 On the line of the first default entry SSH , select Source as My IP . 7.3 Click Add Rule , select Type as HTTP and Source as Anywhere . Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer. 7.5 Click Review and Launch. On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New,then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#level-200-cloudfront-with-waf-protection-lab-guide","text":"","title":"Level 200: CloudFront with WAF Protection: Lab Guide"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#table-of-contents","text":"Launch Instance Configure WAF Configure CloudFront Tear Down","title":"Table of Contents"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#1-launch-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + Accept defaults and click Next: Add tags . Click Next: Configure Security Group . 7.1 Accept default option Create a new security group . 7.2 On the line of the first default entry SSH , select Source as My IP . 7.3 Click Add Rule , select Type as HTTP and Source as Anywhere . Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer. 7.5 Click Review and Launch. On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New,then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1. Launch Instance "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#2-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"2. Configure AWS WAF "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Introduction This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals IAM permission boundaries IAM policy conditions Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#goals","text":"IAM permission boundaries IAM policy conditions","title":"Goals"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Knowledge Check Tear Down The following image shows what you will be doing in this lab. 1. Create IAM policies 1.1 Create policy for permission boundary This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : EC2RestrictRegion , Effect : Allow , Action : ec2:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } }, { Sid : LambdaRestrictRegion , Effect : Allow , Action : lambda:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create developer IAM restricted policy This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { Version : 2012-10-17 , Statement : [ { Sid : CreatePolicy , Effect : Allow , Action : [ iam:CreatePolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion ], Resource : arn:aws:iam::123456789012:policy/app1* }, { Sid : CreateRole , Effect : Allow , Action : [ iam:CreateRole ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { StringEquals : { iam:PermissionsBoundary : arn:aws:iam::123456789012:policy/restrict-region-boundary } } }, { Sid : AttachDetachRolePolicy , Effect : Allow , Action : [ iam:DetachRolePolicy , iam:AttachRolePolicy ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { ArnEquals : { iam:PolicyARN : [ arn:aws:iam::123456789012:policy/* , arn:aws:iam::aws:policy/* ] } } } ] } 1.3 Create developer IAM console access policy This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { Version : 2012-10-17 , Statement : [ { Sid : Get , Effect : Allow , Action : [ iam:ListPolicies , iam:GetRole , iam:GetPolicyVersion , iam:ListRoleTags , iam:GetPolicy , iam:ListPolicyVersions , iam:ListAttachedRolePolicies , iam:ListRoles , iam:ListRolePolicies , iam:GetRolePolicy ], Resource : * } ] } 2. Create and Test Developer Role 2.1 Create Developer Role Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of developer-restricted-iam for the Role name and click Create role . Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test! 2.2. Test Developer Role Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 6. You are now using the developer role with the granted permissions, stay logged in using the role for the next section. 3. Create and Test User Role 3.1 Create User Role While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! In the navigation pane, click Roles and then click Create role . Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the Role name of app1-user-region-restricted-services for the role and click Create role . The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps. 3.2 Test User Role Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. Select a different color to before, otherwise it will overwrite that profile in your browser. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. Congratulations! You have now learnt about IAM permission boundaries and have one working! 4. Knowledge Check The security best practices followed in this lab are: Manage credentials and authentication Use of MFA for access to provide additional access control. Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. 5. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read References useful resources Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#table-of-contents","text":"Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Knowledge Check Tear Down The following image shows what you will be doing in this lab.","title":"Table of Contents"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#1-create-iam-policies","text":"","title":"1. Create IAM policies "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#11-create-policy-for-permission-boundary","text":"This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : EC2RestrictRegion , Effect : Allow , Action : ec2:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } }, { Sid : LambdaRestrictRegion , Effect : Allow , Action : lambda:* , Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy for permission boundary"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#12-create-developer-iam-restricted-policy","text":"This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { Version : 2012-10-17 , Statement : [ { Sid : CreatePolicy , Effect : Allow , Action : [ iam:CreatePolicy , iam:CreatePolicyVersion , iam:DeletePolicyVersion ], Resource : arn:aws:iam::123456789012:policy/app1* }, { Sid : CreateRole , Effect : Allow , Action : [ iam:CreateRole ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { StringEquals : { iam:PermissionsBoundary : arn:aws:iam::123456789012:policy/restrict-region-boundary } } }, { Sid : AttachDetachRolePolicy , Effect : Allow , Action : [ iam:DetachRolePolicy , iam:AttachRolePolicy ], Resource : arn:aws:iam::123456789012:role/app1* , Condition : { ArnEquals : { iam:PolicyARN : [ arn:aws:iam::123456789012:policy/* , arn:aws:iam::aws:policy/* ] } } } ] }","title":"1.2 Create developer IAM restricted policy"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#13-create-developer-iam-console-access-policy","text":"This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { Version : 2012-10-17 , Statement : [ { Sid : Get , Effect : Allow , Action : [ iam:ListPolicies , iam:GetRole , iam:GetPolicyVersion , iam:ListRoleTags , iam:GetPolicy , iam:ListPolicyVersions , iam:ListAttachedRolePolicies , iam:ListRoles , iam:ListRolePolicies , iam:GetRolePolicy ], Resource : * } ] }","title":"1.3 Create developer IAM console access policy"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#2-create-and-test-developer-role","text":"","title":"2. Create and Test Developer Role "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#21-create-developer-role","text":"Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of developer-restricted-iam for the Role name and click Create role . Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test!","title":"2.1 Create Developer Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#22-test-developer-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 6. You are now using the developer role with the granted permissions, stay logged in using the role for the next section.","title":"2.2. Test Developer Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#3-create-and-test-user-role","text":"","title":"3. Create and Test User Role "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#31-create-user-role","text":"While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! In the navigation pane, click Roles and then click Create role . Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the Role name of app1-user-region-restricted-services for the role and click Create role . The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps.","title":"3.1 Create User Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#32-test-user-role","text":"Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. Select a different color to before, otherwise it will overwrite that profile in your browser. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. Congratulations! You have now learnt about IAM permission boundaries and have one working!","title":"3.2 Test User Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: Manage credentials and authentication Use of MFA for access to provide additional access control. Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.","title":"4. Knowledge Check "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#5-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read","title":"5. Tear down this lab "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#references-useful-resources","text":"Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html","text":"Level 300: IAM Tag Based Access Control for EC2 Introduction This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals IAM least privilege IAM policy conditions Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#goals","text":"IAM least privilege IAM policy conditions","title":"Goals"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html","text":"Level 300: IAM Tag Based Access Control for EC2 Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create IAM Role Test Knowledge Check Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource. 1. Create IAM managed policies The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California). 1.1 Create policy named ec2-list-read This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : ec2listread , Effect : Allow , Action : [ ec2:Describe* , ec2:Get* ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create policy named ec2-create-tags This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. Create a managed policy using the JSON policy below and name of ec2-create-tags . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtags , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:CreateAction : RunInstances } } } ] } 1.3 Create policy named ec2-create-tags-existing This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtagsexisting , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Team , Name ] }, StringEqualsIfExists : { aws:RequestTag/Team : Alpha } } } ] } 1.4 Create policy named ec2-run-instances This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. Create a managed policy using the JSON policy below and name of ec2-run-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2runinstances , Effect : Allow , Action : ec2:RunInstances , Resource : arn:aws:ec2:*:*:instance/* , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ], aws:RequestTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Name , Team ] } } }, { Sid : ec2runinstancesother , Effect : Allow , Action : ec2:RunInstances , Resource : [ arn:aws:ec2:*:*:subnet/* , arn:aws:ec2:*:*:key-pair/* , arn:aws:ec2:*::snapshot/* , arn:aws:ec2:*:*:launch-template/* , arn:aws:ec2:*:*:volume/* , arn:aws:ec2:*:*:security-group/* , arn:aws:ec2:*:*:placement-group/* , arn:aws:ec2:*:*:network-interface/* , arn:aws:ec2:*::image/* ], Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } 1.5 Create policy named ec2-manage-instances This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2manageinstances , Effect : Allow , Action : [ ec2:RebootInstances , ec2:TerminateInstances , ec2:StartInstances , ec2:StopInstances ], Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha , aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } 2. Create Role Create a role for EC2 administrators, and attach the managed policies previously created. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of ec2-admin-team-alpha for the Role name and click Create role . Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test! 3. Test Role 3.1 Assume ec2-admin-team-alpha Role Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 3.2 Launch Instance With Without Tags Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet. 3.3 Modify Tags On Instances Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed. 3.4 Manage Instances Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2! 4. Knowledge Check The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. 5. Tear down this lab Please note that the changes you made to the policies and roles have no charges associated with them. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances References useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#table-of-contents","text":"Create IAM Policies Create IAM Role Test Knowledge Check Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource.","title":"Table of Contents"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#1-create-iam-managed-policies","text":"The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California).","title":"1. Create IAM managed policies "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#11-create-policy-named-ec2-list-read","text":"This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { Version : 2012-10-17 , Statement : [ { Sid : ec2listread , Effect : Allow , Action : [ ec2:Describe* , ec2:Get* ], Resource : * , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy named ec2-list-read"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#12-create-policy-named-ec2-create-tags","text":"This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. Create a managed policy using the JSON policy below and name of ec2-create-tags . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtags , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:CreateAction : RunInstances } } } ] }","title":"1.2 Create policy named ec2-create-tags"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#13-create-policy-named-ec2-create-tags-existing","text":"This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { Version : 2012-10-17 , Statement : [ { Sid : ec2createtagsexisting , Effect : Allow , Action : ec2:CreateTags , Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Team , Name ] }, StringEqualsIfExists : { aws:RequestTag/Team : Alpha } } } ] }","title":"1.3 Create policy named ec2-create-tags-existing"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#14-create-policy-named-ec2-run-instances","text":"This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. Create a managed policy using the JSON policy below and name of ec2-run-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2runinstances , Effect : Allow , Action : ec2:RunInstances , Resource : arn:aws:ec2:*:*:instance/* , Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ], aws:RequestTag/Team : Alpha }, ForAllValues:StringEquals : { aws:TagKeys : [ Name , Team ] } } }, { Sid : ec2runinstancesother , Effect : Allow , Action : ec2:RunInstances , Resource : [ arn:aws:ec2:*:*:subnet/* , arn:aws:ec2:*:*:key-pair/* , arn:aws:ec2:*::snapshot/* , arn:aws:ec2:*:*:launch-template/* , arn:aws:ec2:*:*:volume/* , arn:aws:ec2:*:*:security-group/* , arn:aws:ec2:*:*:placement-group/* , arn:aws:ec2:*:*:network-interface/* , arn:aws:ec2:*::image/* ], Condition : { StringEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"1.4 Create policy named ec2-run-instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#15-create-policy-named-ec2-manage-instances","text":"This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { Version : 2012-10-17 , Statement : [ { Sid : ec2manageinstances , Effect : Allow , Action : [ ec2:RebootInstances , ec2:TerminateInstances , ec2:StartInstances , ec2:StopInstances ], Resource : * , Condition : { StringEquals : { ec2:ResourceTag/Team : Alpha , aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"1.5 Create policy named ec2-manage-instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#2-create-role","text":"Create a role for EC2 administrators, and attach the managed policies previously created. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of ec2-admin-team-alpha for the Role name and click Create role . Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test!","title":"2. Create Role "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#3-test-role","text":"","title":"3. Test Role"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#31-assume-ec2-admin-team-alpha-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.","title":"3.1 Assume ec2-admin-team-alpha Role"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#32-launch-instance-with-without-tags","text":"Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet.","title":"3.2 Launch Instance With &amp; Without Tags"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#33-modify-tags-on-instances","text":"Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed.","title":"3.3 Modify Tags On Instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#34-manage-instances","text":"Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2!","title":"3.4 Manage Instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.","title":"4. Knowledge Check "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#5-tear-down-this-lab","text":"Please note that the changes you made to the policies and roles have no charges associated with them. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances","title":"5. Tear down this lab "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html","text":"Level 300: Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#level-300-incident-response-with-aws-console-and-cli","text":"","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#introduction","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#goals","text":"Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response","title":"Goals"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable.","title":"Prerequisites"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html","text":"Level 300: Incident Response with AWS Console and CLI Authors Ben Potter, Security Lead, Well-Architected Table of Contents Getting Started Identity Access Management Amazon VPC Knowledge Check 1. Getting Started 1.1 Install the AWS CLI Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts. Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI: Install jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced. 1.2 Amazon CloudWatch Logs Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data. To list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions . To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1 2. Identity Access Management 2.1 Investigate AWS CloudTrail As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab. 2.1.1 AWS Console The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . IAM access denied attempts: To list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details: filter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key: If you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE : filter userIdentity.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM source ip address: If you suspect a particular IP address as an adversary you can search such as 192.0.2.1 : filter sourceIPAddress = \"192.0.2.1\" | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key created An access key id will be part of the responseElements when its created so you can query that: filter responseElements.credentials.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM users and roles created Listing users and roles created can help identify unauthorized activity: filter eventName=\"CreateUser\" or eventName = \"CreateRole\" | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: filter eventName =\"ListBuckets\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent 2.1.2 AWS CLI Remember you might need to update the --log-group-name , --region and/or --start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com . IAM access denied attempts: To list all IAM access denied attempts you can use CloudWatch Logs with --filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM access key: If you need to search for what actions an access key has performed you can modify the --filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM source ip address: If you suspect a particular IP address as an adversary you can modify the --filter-pattern parameter to be the IP address to search such as 192.0.2.1 : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' 2.2 Block access in AWS IAM Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions. 2.3 List AWS IAM roles/users/groups If you need to confirm the name of a role, user or group you can list: 2.3.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field. 2.3.2 AWS CLI aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query: aws iam list-roles --output table --query 'Roles[*].RoleName' List all users: aws iam list-users --output table --query 'Users[*].UserName' List all groups: aws iam list-groups --output table --query 'Groups[*].GroupName' 2.4 Attach inline deny policy Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations. 2.4.1 AWS Console Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups , Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy . Click the JSON tab then replace the example with the following: { \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] } Click Review policy . Enter Name of DenyAll then click Create policy . 2.4.2 AWS CLI Block a role, modify ROLENAME to match your role name: aws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a user, modify USERNAME to match your user name: aws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a group, modify GROUPNAME to match your user name: aws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' 2.5 Delete inline deny policy To delete the policy you just attached and restore the original permissions the entity had: 2.5.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role . Confirm the role to delete then click Yes, delete 2.5.2 AWS CLI Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll 3. Amazon VPC A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled. 3.1 Investigate Amazon VPC Flow Logs 3.1.1 AWS Management Console The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . Rejected requests by IP address: Rejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\"REJECT\" | stats count(*) as numRejections by srcAddr | sort numRejections desc Reject requests originating from inside your VPC Rejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10. : filter action=\"REJECT\" and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc Requests from an IP address If you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \"192.0.2.1\" | fields @timestamp, interfaceId, dstAddr, dstPort, action Request count from a private IP address by destination address If you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \"10.1.1.1\" | stats count(*) as numConnections by dstAddr | sort numConnections desc 4. Knowledge Check The security best practices followed in this lab are: Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations. References useful resources AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#level-300-incident-response-with-aws-console-and-cli","text":"","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#table-of-contents","text":"Getting Started Identity Access Management Amazon VPC Knowledge Check","title":"Table of Contents"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#1-getting-started","text":"","title":"1. Getting Started "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#11-install-the-aws-cli","text":"Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts. Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI: Install jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.","title":"1.1 Install the AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#12-amazon-cloudwatch-logs","text":"Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data. To list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions . To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1","title":"1.2 Amazon CloudWatch Logs"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#2-identity-access-management","text":"","title":"2. Identity &amp; Access Management "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#21-investigate-aws-cloudtrail","text":"As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab.","title":"2.1 Investigate AWS CloudTrail"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#211-aws-console","text":"The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . IAM access denied attempts: To list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details: filter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key: If you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE : filter userIdentity.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM source ip address: If you suspect a particular IP address as an adversary you can search such as 192.0.2.1 : filter sourceIPAddress = \"192.0.2.1\" | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key created An access key id will be part of the responseElements when its created so you can query that: filter responseElements.credentials.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM users and roles created Listing users and roles created can help identify unauthorized activity: filter eventName=\"CreateUser\" or eventName = \"CreateRole\" | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: filter eventName =\"ListBuckets\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent","title":"2.1.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#212-aws-cli","text":"Remember you might need to update the --log-group-name , --region and/or --start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com . IAM access denied attempts: To list all IAM access denied attempts you can use CloudWatch Logs with --filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM access key: If you need to search for what actions an access key has performed you can modify the --filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM source ip address: If you suspect a particular IP address as an adversary you can modify the --filter-pattern parameter to be the IP address to search such as 192.0.2.1 : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'","title":"2.1.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#22-block-access-in-aws-iam","text":"Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions.","title":"2.2 Block access in AWS IAM"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#23-list-aws-iam-rolesusersgroups","text":"If you need to confirm the name of a role, user or group you can list:","title":"2.3 List AWS IAM roles/users/groups"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#231-aws-console","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field.","title":"2.3.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#232-aws-cli","text":"aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query: aws iam list-roles --output table --query 'Roles[*].RoleName' List all users: aws iam list-users --output table --query 'Users[*].UserName' List all groups: aws iam list-groups --output table --query 'Groups[*].GroupName'","title":"2.3.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#24-attach-inline-deny-policy","text":"Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations.","title":"2.4 Attach inline deny policy"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#241-aws-console","text":"Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups , Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy . Click the JSON tab then replace the example with the following: { \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] } Click Review policy . Enter Name of DenyAll then click Create policy .","title":"2.4.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#242-aws-cli","text":"Block a role, modify ROLENAME to match your role name: aws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a user, modify USERNAME to match your user name: aws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a group, modify GROUPNAME to match your user name: aws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }'","title":"2.4.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#25-delete-inline-deny-policy","text":"To delete the policy you just attached and restore the original permissions the entity had:","title":"2.5 Delete inline deny policy"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#251-aws-console","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role . Confirm the role to delete then click Yes, delete","title":"2.5.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#252-aws-cli","text":"Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll","title":"2.5.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#3-amazon-vpc","text":"A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled.","title":"3. Amazon VPC "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#31-investigate-amazon-vpc-flow-logs","text":"","title":"3.1 Investigate Amazon VPC Flow Logs"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#311-aws-management-console","text":"The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . Rejected requests by IP address: Rejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\"REJECT\" | stats count(*) as numRejections by srcAddr | sort numRejections desc Reject requests originating from inside your VPC Rejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10. : filter action=\"REJECT\" and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc Requests from an IP address If you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \"192.0.2.1\" | fields @timestamp, interfaceId, dstAddr, dstPort, action Request count from a private IP address by destination address If you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \"10.1.1.1\" | stats count(*) as numConnections by dstAddr | sort numConnections desc","title":"3.1.1 AWS Management Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations.","title":"4. Knowledge Check "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#references-useful-resources","text":"AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax","title":"References &amp; useful resources"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html","text":"Level 300: Lambda Cross Account Using Bucket Policy Introduction This lab demonstrates configuration of an S3 bucket policy (which is a type of resource based policy) in AWS account 2 (the destination) that enables a Lambda function in AWS account 1 (the origin) to list the objects in that bucket using Python boto SDK. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals S3 bucket policies Resource based policies versus identity based policies Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#level-300-lambda-cross-account-using-bucket-policy","text":"","title":"Level 300: Lambda Cross Account Using Bucket Policy"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#introduction","text":"This lab demonstrates configuration of an S3 bucket policy (which is a type of resource based policy) in AWS account 2 (the destination) that enables a Lambda function in AWS account 1 (the origin) to list the objects in that bucket using Python boto SDK. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#goals","text":"S3 bucket policies Resource based policies versus identity based policies","title":"Goals"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html","text":"Level 300: Lambda Cross Account Using Bucket Policy Authors Seth Eliot, Resiliency Lead, Well-Architected, AWS Table of Contents Identify (or create) S3 bucket in account 2 Create role for Lambda in account 1 Create bucket policy for the S3 bucket in account 2 Create Lambda in account 1 Tear Down This lab is best run using two AWS accounts Identify the AWS account number for account 1 (no dashes) Identify the AWS account number for account 2 (no dashes) If you only have one AWS account, then use the same AWS account number for both account1 and account2 1. Identify (or create) S3 bucket in account 2 In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Choose an S3 bucket that contains some objects. You will enable the ability to list the objects in this bucket from the other account. If you would rather create a new bucket to use, follow these directions Record the bucketname 2. Create role for Lambda in account 1 In account 1 sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ Click Roles on the left, then create role AWS service will be pre-selected, select Lambda , then click Next: Permissions Do not select any managed policies, click Next: Tags Click Next: Review Enter Lambda-List-S3-Role for the Role name then click Create role From the list of roles click the name of Lambda-List-S3-Role Click Add inline policy , then click JSON tab Replace the sample json with the following Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Then click Review Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListBucket\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::bucketname\" }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-List-S3*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-List-S3-Policy , then click Create policy 3. Create bucket policy for the S3 bucket in account 2 In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Click on the name of the bucket you will use for this workshop Go to the Permissions tab Click Bucket Policy Enter the following JSON policy Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Note: This policy uses least privilege. Only resources using the IAM role from account 1 will have access { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1565731301209\", \"Action\": [ \"s3:ListBucket\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::bucketname\", \"Principal\": { \"AWS\":\"arn:aws:iam::account1:role/Lambda-List-S3-Role\" }, \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } } ] } Click Save 4. Create Lambda in account 1 Open the Lambda console https://console.aws.amazon.com/lambda Click Create a function Accept the default Author from scratch Enter function name as Lambda-List-S3 Select Python 3.7 runtime Expand Permissions, click Use an existing role , then select the Lambda-List-S3-Role Click Create function Replace the example function code with the following Replace bucketname with the S3 bucket name from account 2 import json import boto3 import os import uuid def lambda_handler(event, context): try: # Create an S3 client s3 = boto3.client('s3') # Call S3 to list current buckets objlist = s3.list_objects( Bucket='bucketname', MaxKeys = 10) print (objlist['Contents']) return str(objlist['Contents']) except Exception as e: print(e) raise e Click Save . Click Test , accept the default event template, enter an event name for the test, then click Create Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API 5. Tear down this lab Remove the lambda function, then roles If you created a new S3 bucket, then you may remove it References useful resources https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#level-300-lambda-cross-account-using-bucket-policy","text":"","title":"Level 300: Lambda Cross Account Using Bucket Policy"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#authors","text":"Seth Eliot, Resiliency Lead, Well-Architected, AWS","title":"Authors"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#table-of-contents","text":"Identify (or create) S3 bucket in account 2 Create role for Lambda in account 1 Create bucket policy for the S3 bucket in account 2 Create Lambda in account 1 Tear Down This lab is best run using two AWS accounts Identify the AWS account number for account 1 (no dashes) Identify the AWS account number for account 2 (no dashes) If you only have one AWS account, then use the same AWS account number for both account1 and account2","title":"Table of Contents"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#1-identify-or-create-s3-bucket-in-account-2","text":"In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Choose an S3 bucket that contains some objects. You will enable the ability to list the objects in this bucket from the other account. If you would rather create a new bucket to use, follow these directions Record the bucketname","title":"1. Identify (or create) S3 bucket in account 2 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#2-create-role-for-lambda-in-account-1","text":"In account 1 sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ Click Roles on the left, then create role AWS service will be pre-selected, select Lambda , then click Next: Permissions Do not select any managed policies, click Next: Tags Click Next: Review Enter Lambda-List-S3-Role for the Role name then click Create role From the list of roles click the name of Lambda-List-S3-Role Click Add inline policy , then click JSON tab Replace the sample json with the following Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Then click Review Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListBucket\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::bucketname\" }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-List-S3*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-List-S3-Policy , then click Create policy","title":"2. Create role for Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#3-create-bucket-policy-for-the-s3-bucket-in-account-2","text":"In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Click on the name of the bucket you will use for this workshop Go to the Permissions tab Click Bucket Policy Enter the following JSON policy Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Note: This policy uses least privilege. Only resources using the IAM role from account 1 will have access { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1565731301209\", \"Action\": [ \"s3:ListBucket\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::bucketname\", \"Principal\": { \"AWS\":\"arn:aws:iam::account1:role/Lambda-List-S3-Role\" }, \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } } ] } Click Save","title":"3. Create bucket policy for the S3 bucket in account 2 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#4-create-lambda-in-account-1","text":"Open the Lambda console https://console.aws.amazon.com/lambda Click Create a function Accept the default Author from scratch Enter function name as Lambda-List-S3 Select Python 3.7 runtime Expand Permissions, click Use an existing role , then select the Lambda-List-S3-Role Click Create function Replace the example function code with the following Replace bucketname with the S3 bucket name from account 2 import json import boto3 import os import uuid def lambda_handler(event, context): try: # Create an S3 client s3 = boto3.client('s3') # Call S3 to list current buckets objlist = s3.list_objects( Bucket='bucketname', MaxKeys = 10) print (objlist['Contents']) return str(objlist['Contents']) except Exception as e: print(e) raise e Click Save . Click Test , accept the default event template, enter an event name for the test, then click Create Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API","title":"4. Create Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#5-tear-down-this-lab","text":"Remove the lambda function, then roles If you created a new S3 bucket, then you may remove it","title":"5. Tear down this lab "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#references-useful-resources","text":"https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html","title":"References &amp; useful resources"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Documentation/CreateNewS3Bucket.html","text":"Create New S3 Bucket These steps will guide you to create a bucket containing some objects. Go to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket For Bucket name supply a name. This must be unique across all buckets in AWS Tip : Name the bucket your_first_name _ date in yyyymmm10 format Click Next three times Review screen: click Create bucket Click on the name of the bucket you created Drag some files into the object upload area Click Next three times Click Upload Click here to return to the Lab Guide","title":"Create New S3 Bucket"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Documentation/CreateNewS3Bucket.html#create-new-s3-bucket","text":"These steps will guide you to create a bucket containing some objects. Go to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket For Bucket name supply a name. This must be unique across all buckets in AWS Tip : Name the bucket your_first_name _ date in yyyymmm10 format Click Next three times Review screen: click Create bucket Click on the name of the bucket you created Drag some files into the object upload area Click Next three times Click Upload Click here to return to the Lab Guide","title":"Create New S3 Bucket"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html","text":"Level 300: Lambda Cross Account IAM Role Assumption Introduction This lab demonstrates a Lambda function in AWS account 1 (the origin) using Python boto SDK to assume an IAM role in account 2 (the destination), then list the buckets. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Cross account role assumption Lambda assuming another role Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#level-300-lambda-cross-account-iam-role-assumption","text":"","title":"Level 300: Lambda Cross Account IAM Role Assumption"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#introduction","text":"This lab demonstrates a Lambda function in AWS account 1 (the origin) using Python boto SDK to assume an IAM role in account 2 (the destination), then list the buckets. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#goals","text":"Cross account role assumption Lambda assuming another role","title":"Goals"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html","text":"Level 300: Lambda Cross Account IAM Role Assumption Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create role for Lambda in account 2 Create role for Lambda in account 1 Create Lambda in account 1 Tear Down 1. Create role for Lambda in account 2 Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. Click Another AWS account, enter the account id for account 1 (the origin), then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter LambdaS3ListBuckets for the Role name then click Create role. From the list of roles click the name of LambdaS3ListBuckets. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListAllMyBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\" ], \"Resource\": \"*\" } ] } Name this policy LambdaS3ListBucketsPolicy, then click Create policy. 2. Create role for Lambda in account 1 Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. AWS service will be pre-selected, select Lambda, then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter Lambda-Assume-Roles for the Role name then click Create role. From the list of roles click the name of Lambda-Assume-Roles. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, replacing account1 and account2 with your respective account id's, us-east-1 region with the region you are using, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"stsassumerole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::account2:role/LambdaS3ListBuckets\", \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-Assume-Roles*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-Assume-Roles-Policy, then click Create policy. 3. Create Lambda in account 1 Open the Lambda console. Click Create a function. Accept the default Author from scratch. Enter function name as Lambda-Assume-Roles. Select Python 3.6 runtime. Expand Permissions, click Use an existing role, then select the Lambda-Assume-Roles role. Click Create function. Replace the example function code with the following, replacing the RoleArn with the one from account 2 you created previously. import json import boto3 import os import uuid def lambda_handler(event, context): try: client = boto3.client('sts') response = client.assume_role(RoleArn='arn:aws:iam::account2:role/LambdaS3ListBuckets',RoleSessionName=\"{}-s3\".format(str(uuid.uuid4())[:5])) session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken']) s3 = session.client('s3') s3list = s3.list_buckets() print (s3list) return str(s3list['Buckets']) except Exception as e: print(e) raise e Click Save. Click Test, accept the default event template, enter event name of test, then click Create. Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API. How could the example policies be improved? 4. Tear down this lab Remove the lambda function, then roles. References useful resources https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#level-300-lambda-cross-account-iam-role-assumption","text":"","title":"Level 300: Lambda Cross Account IAM Role Assumption"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#table-of-contents","text":"Create role for Lambda in account 2 Create role for Lambda in account 1 Create Lambda in account 1 Tear Down","title":"Table of Contents"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#1-create-role-for-lambda-in-account-2","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. Click Another AWS account, enter the account id for account 1 (the origin), then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter LambdaS3ListBuckets for the Role name then click Create role. From the list of roles click the name of LambdaS3ListBuckets. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListAllMyBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\" ], \"Resource\": \"*\" } ] } Name this policy LambdaS3ListBucketsPolicy, then click Create policy.","title":"1. Create role for Lambda in account 2 "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#2-create-role-for-lambda-in-account-1","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. AWS service will be pre-selected, select Lambda, then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter Lambda-Assume-Roles for the Role name then click Create role. From the list of roles click the name of Lambda-Assume-Roles. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, replacing account1 and account2 with your respective account id's, us-east-1 region with the region you are using, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"stsassumerole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::account2:role/LambdaS3ListBuckets\", \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-Assume-Roles*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-Assume-Roles-Policy, then click Create policy.","title":"2. Create role for Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#3-create-lambda-in-account-1","text":"Open the Lambda console. Click Create a function. Accept the default Author from scratch. Enter function name as Lambda-Assume-Roles. Select Python 3.6 runtime. Expand Permissions, click Use an existing role, then select the Lambda-Assume-Roles role. Click Create function. Replace the example function code with the following, replacing the RoleArn with the one from account 2 you created previously. import json import boto3 import os import uuid def lambda_handler(event, context): try: client = boto3.client('sts') response = client.assume_role(RoleArn='arn:aws:iam::account2:role/LambdaS3ListBuckets',RoleSessionName=\"{}-s3\".format(str(uuid.uuid4())[:5])) session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken']) s3 = session.client('s3') s3list = s3.list_buckets() print (s3list) return str(s3list['Buckets']) except Exception as e: print(e) raise e Click Save. Click Test, accept the default event template, enter event name of test, then click Create. Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API. How could the example policies be improved?","title":"3. Create Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#4-tear-down-this-lab","text":"Remove the lambda function, then roles.","title":"4. Tear down this lab "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#references-useful-resources","text":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html","title":"References &amp; useful resources"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html","text":"Quest: Loft - Introduction to Security About this Guide This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Step 1 - New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Step 2 - Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Walkthrough Basic Identity and Access Management User, Group, Role Step 3 - CloudFront with WAF Protection This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying. Walkthrough CloudFront with WAF Protection Step 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail. Walkthrough Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Introduction to Security"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#quest-loft-introduction-to-security","text":"","title":"Quest: Loft - Introduction to Security"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#about-this-guide","text":"This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-1-new-aws-account-setup-and-securing-root-user","text":"","title":"Step 1 - New AWS Account Setup and Securing Root User"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#further-considerations","text":"Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy","title":"Further Considerations"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-2-basic-identity-and-access-management-user-group-role","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Step 2 - Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_1","text":"Basic Identity and Access Management User, Group, Role","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-3-cloudfront-with-waf-protection","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying.","title":"Step 3 -  CloudFront with WAF Protection"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_2","text":"CloudFront with WAF Protection","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-4-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail.","title":"Step 4 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_3","text":"Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html","text":"Quest: Quick Steps to Security Success About this Guide This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. By the end of the quest we will have a set of accounts with security best practices applied, ready to develop your product in knowing that when you launch your workload you have secure foundations in place. Step 1 - Multi-Account Strategy Implementing multiple accounts for our workload improves our security by limiting the blast radius of any potential breaches and separating our workload into discrete accounts. Leverage AWS Organizations to create separate AWS accounts for a sandbox, your workload, a secure \u201cdata bunker\u201d for audit logs and backups, and a shared services account for common tools. If you currently only have one account, create a new AWS account for your organizations master AWS account and invite your existing account to join as your sandbox AWS account. By the end of this step you will have a separate AWS account for: Organizations root account \u2013 used only for identity and billing Shared services \u2013 for common tools such as deployment tooling Workload accounts \u2013 customers who have a single product will have a separate AWS account for each environment. If you have multiple workloads, or you rely on account separation for separation of customer data you will want to set up further accounts to reflect your structure Walkthrough Define your multi-account strategy . A suggested structure is shown below. If you do not current have AWS Organizations setup it is recommended that you use your existing account as a sandbox (If required) Sign up a new root account Create an AWS Organization in the root account Invite any existing accounts For each AWS account required Create a new account in organizations . Make note of the organizations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account Step 2 - Identity Every user must leverage unique credentials so we can trace actions within our accounts. Setup your identity structure in the master account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give IAM permissions to as they can create their own permissions. Walkthrough Perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Federate Identity leveraging a SAML provider Use cross account access roles to access the accounts that we setup in part 1. Step 3 - Data Bunker Now we will create a data bunker account to store secure read only security logs and backups. In this step we will send our logs from CloudTrail to that account. The role for accessing this account will have read only access. Only ensure that this role can be accessed by those with a security role in your organization. Walkthrough Setup a security account, secure Amazon S3 bucket and turn on our AWS Organization CloudTrail Step 4 - Enable organizations policies AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization. Walkthrough (repeat for each policy below) Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select * roots Press Attach to attach the policy to your organizations root Policy to prevent users disabling CloudTrail { Version : 2012-10-17 , Statement : [ { Effect : Deny , Action : cloudtrail:StopLogging , Resource : * } ] } (Optional) Disable unused regions This policy specifically enables only us-east-1 and us-west-1 . Replace with the list of region codes you wish to allow access to. { Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ * ], Resource : [ * ], Condition : { ForAnyValue:StringNotEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] } Step 5 - Disable public access to data in S3 Amazon Simple Storage Service (S3) allows you to upload objects to a \"bucket\" which are then accessible depending on the access control list implemented. It is important to consider how you make data public. By blocking public access your team will have to be deliberate when it exposes data. Note: The steps below apply at an individual account level. It is important to consider turning this on as you create additional accounts for your organization. At a minimum you should apply this to any account which hosts production data. Walkthrough For each account that you want to block public access to data stored in S3 - use a cross-account access role to block S3 public access Repeat the walkthrough in part 4 to apply the policy below. Instead of applying this policy to the root, apply this specifically to the accounts where you have blocked public access. Policy to block public { Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ s3:PutBucketPublicAccessBlock , s3:PutAccountPublicAccessBlock ], Resource : * } ] } Step 6 - Monitoring and Alerting Lastly, we will setup our foundations for monitoring the security status of our AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across our AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. Walkthrough Enable AWS Security Hub . Leverage the AWS Security Hub Multiaccount Scripts to enable it across accounts. Enable Amazon GuardDuty . Leverage Amazon to enable it across accounts. Additional Resources and next steps Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well Architected Framework and how applying it can improve your security posture License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Quick Steps to Security Success"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#quest-quick-steps-to-security-success","text":"","title":"Quest: Quick Steps to Security Success"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#about-this-guide","text":"This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. By the end of the quest we will have a set of accounts with security best practices applied, ready to develop your product in knowing that when you launch your workload you have secure foundations in place.","title":"About this Guide"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-1-multi-account-strategy","text":"Implementing multiple accounts for our workload improves our security by limiting the blast radius of any potential breaches and separating our workload into discrete accounts. Leverage AWS Organizations to create separate AWS accounts for a sandbox, your workload, a secure \u201cdata bunker\u201d for audit logs and backups, and a shared services account for common tools. If you currently only have one account, create a new AWS account for your organizations master AWS account and invite your existing account to join as your sandbox AWS account. By the end of this step you will have a separate AWS account for: Organizations root account \u2013 used only for identity and billing Shared services \u2013 for common tools such as deployment tooling Workload accounts \u2013 customers who have a single product will have a separate AWS account for each environment. If you have multiple workloads, or you rely on account separation for separation of customer data you will want to set up further accounts to reflect your structure","title":"Step 1 - Multi-Account Strategy"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough","text":"Define your multi-account strategy . A suggested structure is shown below. If you do not current have AWS Organizations setup it is recommended that you use your existing account as a sandbox (If required) Sign up a new root account Create an AWS Organization in the root account Invite any existing accounts For each AWS account required Create a new account in organizations . Make note of the organizations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-2-identity","text":"Every user must leverage unique credentials so we can trace actions within our accounts. Setup your identity structure in the master account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give IAM permissions to as they can create their own permissions.","title":"Step 2 - Identity"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_1","text":"Perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Federate Identity leveraging a SAML provider Use cross account access roles to access the accounts that we setup in part 1.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-3-data-bunker","text":"Now we will create a data bunker account to store secure read only security logs and backups. In this step we will send our logs from CloudTrail to that account. The role for accessing this account will have read only access. Only ensure that this role can be accessed by those with a security role in your organization.","title":"Step 3 - Data Bunker"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_2","text":"Setup a security account, secure Amazon S3 bucket and turn on our AWS Organization CloudTrail","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-4-enable-organizations-policies","text":"AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization.","title":"Step 4 - Enable organizations policies"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough-repeat-for-each-policy-below","text":"Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select * roots Press Attach to attach the policy to your organizations root","title":"Walkthrough (repeat for each policy below)"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#policy-to-prevent-users-disabling-cloudtrail","text":"{ Version : 2012-10-17 , Statement : [ { Effect : Deny , Action : cloudtrail:StopLogging , Resource : * } ] }","title":"Policy to prevent users disabling CloudTrail"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#optional-disable-unused-regions","text":"This policy specifically enables only us-east-1 and us-west-1 . Replace with the list of region codes you wish to allow access to. { Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ * ], Resource : [ * ], Condition : { ForAnyValue:StringNotEquals : { aws:RequestedRegion : [ us-east-1 , us-west-1 ] } } } ] }","title":"(Optional) Disable unused regions"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-5-disable-public-access-to-data-in-s3","text":"Amazon Simple Storage Service (S3) allows you to upload objects to a \"bucket\" which are then accessible depending on the access control list implemented. It is important to consider how you make data public. By blocking public access your team will have to be deliberate when it exposes data. Note: The steps below apply at an individual account level. It is important to consider turning this on as you create additional accounts for your organization. At a minimum you should apply this to any account which hosts production data.","title":"Step 5 - Disable public access to data in S3"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_3","text":"For each account that you want to block public access to data stored in S3 - use a cross-account access role to block S3 public access Repeat the walkthrough in part 4 to apply the policy below. Instead of applying this policy to the root, apply this specifically to the accounts where you have blocked public access.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#policy-to-block-public","text":"{ Version : 2012-10-17 , Statement : [ { Sid : Statement1 , Effect : Deny , Action : [ s3:PutBucketPublicAccessBlock , s3:PutAccountPublicAccessBlock ], Resource : * } ] }","title":"Policy to block public"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-6-monitoring-and-alerting","text":"Lastly, we will setup our foundations for monitoring the security status of our AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across our AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs.","title":"Step 6 - Monitoring and Alerting"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_4","text":"Enable AWS Security Hub . Leverage the AWS Security Hub Multiaccount Scripts to enable it across accounts. Enable Amazon GuardDuty . Leverage Amazon to enable it across accounts.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#additional-resources-and-next-steps","text":"Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well Architected Framework and how applying it can improve your security posture","title":"Additional Resources and next steps"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_200_Incident_Response_Day/README.html","text":"Quest: AWS Incident Response Day About this Guide This quest is the guide for an AWS led event including incident response day. Using an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Start now! Lab 2 - Protecting workloads on AWS from the instance to the edge In this workshop, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them. Start now! Lab 3 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. Start now! Lab 4 - Getting Hands on with Amazon GuardDuty Walks you through a scenario covering threat detection and automated remediation using Amazon GuardDuty; a managed threat detection service. The scenario simulates an attack that spans a few threat vectors, representing just a small sample of the threats that GuardDuty is able to detect. Start now! Lab 5 - Open Source AWS Memory Forensics This lab consists of using an open source python module for orchestrating memory acquisitions and analysis using AWS Systems Manager . It analyzes the memory dump using Rekall with the most common plugins: psaux, pstree, netstat, ifconfig, pidhashtable. Start now! Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Authors Ben Potter, Security Lead, Well-Architected License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Incident Response Day"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#quest-aws-incident-response-day","text":"","title":"Quest: AWS Incident Response Day"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including incident response day. Using an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-1-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 1 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-2-protecting-workloads-on-aws-from-the-instance-to-the-edge","text":"In this workshop, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them.","title":"Lab 2 - Protecting workloads on AWS from the instance to the edge"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_1","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-3-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.","title":"Lab 3 - Incident Response with AWS Console and CLI"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_2","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-4-getting-hands-on-with-amazon-guardduty","text":"Walks you through a scenario covering threat detection and automated remediation using Amazon GuardDuty; a managed threat detection service. The scenario simulates an attack that spans a few threat vectors, representing just a small sample of the threats that GuardDuty is able to detect.","title":"Lab 4 - Getting Hands on with Amazon GuardDuty"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_3","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-5-open-source-aws-memory-forensics","text":"This lab consists of using an open source python module for orchestrating memory acquisitions and analysis using AWS Systems Manager . It analyzes the memory dump using Rekall with the most common plugins: psaux, pstree, netstat, ifconfig, pidhashtable.","title":"Lab 5 - Open Source AWS Memory Forensics"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_4","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#further-learning","text":"AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model","title":"Further Learning"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html","text":"Quest: AWS Security Best Practices Day Authors Ben Potter, Security Lead, Well-Architected About this Guide This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1: Identity Access Management For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed: Introductory Lab 1.1: AWS Account and Root User This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Lab 1.2 Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role Advanced Lab 1.3 - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation Lab 1.4 - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Automated Deployment of Detective Controls Lab 3 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Enable Security Hub Lab 4 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 5 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Automated Deployment of EC2 Web Application Lab 6 - Automated Deployment of Web Application Firewall This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Automated Deployment of Web Application Firewall Lab 7 - CloudFront for Web Application This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. CloudFront for Web Application Lab 8 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . Incident Response with AWS Console and CLI License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Security Best Practices Day"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#quest-aws-security-best-practices-day","text":"","title":"Quest: AWS Security Best Practices Day"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-1-identity-access-management","text":"For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed:","title":"Lab 1: Identity &amp; Access Management"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#introductory","text":"","title":"Introductory"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-11-aws-account-and-root-user","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.","title":"Lab 1.1: AWS Account and Root User"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#aws-account-and-root-user","text":"","title":"AWS Account and Root User"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-12-basic-identity-and-access-management-user-group-role","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Lab 1.2 Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#advanced","text":"","title":"Advanced"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-13-iam-permission-boundaries-delegating-role-creation","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Lab 1.3 - IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-14-iam-tag-based-access-control-for-ec2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Lab 1.4 - IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-2-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 2 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-3-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Lab 3 - Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-4-automated-deployment-of-vpc","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.","title":"Lab 4 - Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-5-automated-deployment-of-ec2-web-application","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Lab 5 - Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-6-automated-deployment-of-web-application-firewall","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods.","title":"Lab 6 - Automated Deployment of Web Application Firewall"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-web-application-firewall","text":"","title":"Automated Deployment of Web Application Firewall"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-7-cloudfront-for-web-application","text":"This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront.","title":"Lab 7 - CloudFront for Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#cloudfront-for-web-application","text":"","title":"CloudFront for Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-8-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .","title":"Lab 8 - Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html","text":"Quest: AWS Security Best Practices Workshop Authors Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect About this Guide This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Identity Access Management For Lab 1 choose one of labs to run based on your interest or experience: Lab 1a - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation Lab 1b - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 3 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Automated Deployment of EC2 Web Application Lab 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Automated Deployment of Detective Controls Lab 5 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Enable Security Hub Lab 6 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . Incident Response with AWS Console and CLI License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Security Best Practices Workshop"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#quest-aws-security-best-practices-workshop","text":"","title":"Quest: AWS Security Best Practices Workshop"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect","title":"Authors"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1-identity-access-management","text":"For Lab 1 choose one of labs to run based on your interest or experience:","title":"Lab 1 -  Identity &amp; Access Management"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1a-iam-permission-boundaries-delegating-role-creation","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Lab 1a - IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1b-iam-tag-based-access-control-for-ec2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Lab 1b - IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-2-automated-deployment-of-vpc","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.","title":"Lab 2 - Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-3-automated-deployment-of-ec2-web-application","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Lab 3 - Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-4-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 4 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-5-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Lab 5 - Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-6-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .","title":"Lab 6 - Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Classify_Data/README.html","text":"Quest: Classify Data Labs coming soon Check out: Amazon Macie User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Classify Data"},{"location":"Security/Quest_Classify_Data/README.html#quest-classify-data","text":"","title":"Quest: Classify Data"},{"location":"Security/Quest_Classify_Data/README.html#labs-coming-soon","text":"Check out: Amazon Macie User Guide","title":"Labs coming soon"},{"location":"Security/Quest_Classify_Data/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Control_Human_Access/README.html","text":"Quest: Control Human Access Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Control Human Access"},{"location":"Security/Quest_Control_Human_Access/README.html#quest-control-human-access","text":"","title":"Quest: Control Human Access"},{"location":"Security/Quest_Control_Human_Access/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Control_Human_Access/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Identity Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Control_Human_Access/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Control_Human_Access/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Control_Human_Access/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Walkthrough"},{"location":"Security/Quest_Control_Human_Access/README.html#basic-identity-and-access-management-user-group-role_1","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Control_Human_Access/README.html#walkthrough_1","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Walkthrough"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-permission-boundaries-delegating-role-creation_1","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Control_Human_Access/README.html#walkthrough_2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Walkthrough"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-tag-based-access-control-for-ec2_1","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Control_Human_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Control_Programmatic_Access/README.html","text":"Quest: Control Programmatic Access Labs coming soon License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Control Programmatic Access"},{"location":"Security/Quest_Control_Programmatic_Access/README.html#quest-control-programmatic-access","text":"","title":"Quest: Control Programmatic Access"},{"location":"Security/Quest_Control_Programmatic_Access/README.html#labs-coming-soon","text":"","title":"Labs coming soon"},{"location":"Security/Quest_Control_Programmatic_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html","text":"Quest: Defend Against New Threats Labs coming soon Check out: AWS Security Blog AWS Security Bulletins License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Defend Against New Threats"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html#quest-defend-against-new-threats","text":"","title":"Quest: Defend Against New Threats"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html#labs-coming-soon","text":"Check out: AWS Security Blog AWS Security Bulletins","title":"Labs coming soon"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html","text":"Quest: Detect Investigate Events Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Detective Controls . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Start the Lab! Enable Security Hub Introduction AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Start the Lab! Further Learning: AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Detect and Investigate Events"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#quest-detect-investigate-events","text":"","title":"Quest: Detect &amp; Investigate Events"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Detective Controls . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Introduction"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#introduction_1","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Introduction"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#start-the-lab_1","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#further-learning","text":"AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide","title":"Further Learning:"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Incident_Response/README.html","text":"Quest: Incident Response Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. Start the Lab! Further Learning AWS Security Incident Response Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Incident Response"},{"location":"Security/Quest_Incident_Response/README.html#quest-incident-response","text":"","title":"Quest: Incident Response"},{"location":"Security/Quest_Incident_Response/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Incident_Response/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Incident_Response/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Incident_Response/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_Incident_Response/README.html#introduction","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.","title":"Introduction"},{"location":"Security/Quest_Incident_Response/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Incident_Response/README.html#further-learning","text":"AWS Security Incident Response Guide","title":"Further Learning"},{"location":"Security/Quest_Incident_Response/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html","text":"Quest: Managing Credentials Authentication Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role Automated IAM User Cleanup Walkthrough This hands-on lab will guide you through the steps to deploy an AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. IAM Tag Based Access Control for EC2 IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Managing Credentials and Authentication"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#quest-managing-credentials-authentication","text":"","title":"Quest: Managing Credentials &amp; Authentication"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Identity Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#new-aws-account-setup-and-securing-root-user","text":"","title":"New AWS Account Setup and Securing Root User"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#aws-account-and-root-user","text":"","title":"AWS Account and Root User"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#further-considerations","text":"Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy","title":"Further Considerations"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_1","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#basic-identity-and-access-management-user-group-role_1","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#automated-iam-user-cleanup","text":"","title":"Automated IAM User Cleanup"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_2","text":"This hands-on lab will guide you through the steps to deploy an AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_3","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-permission-boundaries-delegating-role-creation_1","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-tag-based-access-control-for-ec2_1","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_4","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-tag-based-access-control-for-ec2_2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Compute/README.html","text":"Quest: Protect Compute Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. Start the Lab! Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. You must have first deployed the Automated Deployment of VPC lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Compute"},{"location":"Security/Quest_Protect_Compute/README.html#quest-protect-compute","text":"","title":"Quest: Protect Compute"},{"location":"Security/Quest_Protect_Compute/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Protect_Compute/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Protect_Compute/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Protect_Compute/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_Protect_Compute/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab.","title":"Introduction"},{"location":"Security/Quest_Protect_Compute/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Compute/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_Protect_Compute/README.html#introduction_1","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. You must have first deployed the Automated Deployment of VPC lab.","title":"Introduction"},{"location":"Security/Quest_Protect_Compute/README.html#start-the-lab_1","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Compute/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html","text":"Quest: Protect Data at Rest Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Data Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Create a Data Bunker Account Introduction In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation tp send these logs to th bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups. Start the Lab! Further Learning S3: Protecting Data Using Server-Side Encryption with AWS KMS\u2013Managed Keys Opt-in to Default Encryption for New EBS Volumes License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Data at Rest"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#quest-protect-data-at-rest","text":"","title":"Quest: Protect Data at Rest"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Data Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#create-a-data-bunker-account","text":"","title":"Create a Data Bunker Account"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#introduction","text":"In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation tp send these logs to th bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.","title":"Introduction"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#further-learning","text":"S3: Protecting Data Using Server-Side Encryption with AWS KMS\u2013Managed Keys Opt-in to Default Encryption for New EBS Volumes","title":"Further Learning"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html","text":"Quest: Protect Data in Transit Labs coming soon Check out: AWS Certificate Manager - Getting Started AWS Encryption SDK AWS Site-to-Site VPN User Guide AWS Client VPN User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Data in Transit"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html#quest-protect-data-in-transit","text":"","title":"Quest: Protect Data in Transit"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html#labs-coming-soon","text":"Check out: AWS Certificate Manager - Getting Started AWS Encryption SDK AWS Site-to-Site VPN User Guide AWS Client VPN User Guide","title":"Labs coming soon"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Networks/README.html","text":"Quest: Protect Networks Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Networks"},{"location":"Security/Quest_Protect_Networks/README.html#quest-protect-networks","text":"","title":"Quest: Protect Networks"},{"location":"Security/Quest_Protect_Networks/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Protect_Networks/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Protect_Networks/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Protect_Networks/README.html#cloudfront-with-waf-protection","text":"","title":"CloudFront with WAF Protection"},{"location":"Security/Quest_Protect_Networks/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration.","title":"Introduction"},{"location":"Security/Quest_Protect_Networks/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Networks/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Well-ArchitectedTool/README.html","text":"AWS Well-Architected Tool Labs https://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation . Labs Level 100: Walkthrough of the AWS Well-Architected Tool License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Well-ArchitectedTool/README.html#aws-well-architected-tool-labs","text":"https://wellarchitectedlabs.com","title":"AWS Well-Architected Tool Labs"},{"location":"Well-ArchitectedTool/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation .","title":"Introduction"},{"location":"Well-ArchitectedTool/README.html#labs","text":"Level 100: Walkthrough of the AWS Well-Architected Tool","title":"Labs"},{"location":"Well-ArchitectedTool/README.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html","text":"Level 100: Walkthrough of the Well-Architected Tool https://wellarchitectedlabs.com Introduction The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report. The knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework Goals: Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool. Prequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to usee Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#level-100-walkthrough-of-the-well-architected-tool","text":"https://wellarchitectedlabs.com","title":"Level 100: Walkthrough of the Well-Architected Tool"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#introduction","text":"The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report. The knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#goals","text":"Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool.","title":"Goals:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#prequisites","text":"An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to usee Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy).","title":"Prequisites:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html","text":"Level 100: Walkthrough of the Well-Architected Tool Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Table of Contents Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear Down 1. Navigating to the console The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ . If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool : 2. Creating a workload Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Click the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Industry Type: InfoTech Industry: Internet Environment: Select \"Pre-production\" Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Define workload button: 3. Performing a review From the detail page for the workload, click the Start review button: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability : Select the first question: REL 1. How do you manage service limits? Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit: 4. Saving a milestone From the detail page for the workload, click the Save milestone button : Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it: 5. Viewing and downloading the report From the detail page for the workload, click the Improvement Plan tab: This will display the number of high and medium risk items and allow you to update the Improvement status: You can also edit the improvement plan configuration. Click on the Edit button next to the words Improvement plan configuration : Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Review tab to get the option to download the improvement plan: Click the Generate report button to generate and download the report: You can either open the file or save it to view it. 6. Tear down this lab In order to take down the lab environment, you simply delete the workload you created. 1. Select Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog: References useful resources: License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#level-100-walkthrough-of-the-well-architected-tool","text":"","title":"Level 100: Walkthrough of the Well-Architected Tool"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS","title":"Authors"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#table-of-contents","text":"Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear Down","title":"Table of Contents"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#1-navigating-to-the-console","text":"The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ . If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool :","title":"1. Navigating to the console "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#2-creating-a-workload","text":"Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Click the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Industry Type: InfoTech Industry: Internet Environment: Select \"Pre-production\" Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Define workload button:","title":"2. Creating a workload "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#3-performing-a-review","text":"From the detail page for the workload, click the Start review button: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability : Select the first question: REL 1. How do you manage service limits? Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit:","title":"3. Performing a review "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#4-saving-a-milestone","text":"From the detail page for the workload, click the Save milestone button : Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it:","title":"4. Saving a milestone "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#5-viewing-and-downloading-the-report","text":"From the detail page for the workload, click the Improvement Plan tab: This will display the number of high and medium risk items and allow you to update the Improvement status: You can also edit the improvement plan configuration. Click on the Edit button next to the words Improvement plan configuration : Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Review tab to get the option to download the improvement plan: Click the Generate report button to generate and download the report: You can either open the file or save it to view it.","title":"5. Viewing and downloading the report "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#6-tear-down-this-lab","text":"In order to take down the lab environment, you simply delete the workload you created. 1. Select Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog:","title":"6. Tear down this lab "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#references-useful-resources","text":"","title":"References &amp; useful resources:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"}]}