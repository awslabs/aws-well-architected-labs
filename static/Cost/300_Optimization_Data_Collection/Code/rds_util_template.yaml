AWSTemplateFormatVersion: '2010-09-09'
Description: Retrieves RDS Metric data
Parameters:
  DatabaseName:
    Type: String
    Description: Name of the Athena database to be created to hold lambda information
    Default: optimization_data
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket to be created to hold data information
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  DestinationBucketARN:
    Type: String
    Description: ARN of the S3 Bucket that exists or needs to be created to hold rightsizing information
  MultiAccountRoleName:
    Type: String
    Description: Name of the IAM role deployed in all accounts which can retrieve AWS Data.
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: rds_metrics
  DAYS:
    Type: Number
    Description: Number of days going back that you want to get data for
    Default: 1
  GlueRoleArn:
    Type: String
    Description: ARN of the IAM role deployed to use for glue.
  RolePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
Outputs:
  AthenaSavedQuery:
    Description: This saved query will provide you a summary of your lambda data
    Value:
      Ref: AthenaQuery
  LambdaRoleARN:
    Description: Role for Lambda execution of lambda data.
    Value:
      Fn::GetAtt:
        - LambdaRole
        - Arn
  SQSUrl:
    Description: TaskQueue URL the account collector lambda
    Value: !Ref TaskQueue
Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${RolePrefix}Lambda-Role-${CFDataName}"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
      Path: /
      Policies:
        - PolicyName: !Sub "Assume-Management-${CFDataName}-Account-Role"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: "*"
        - PolicyName: !Sub "${CFDataName}-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "s3:PutObject"
                  - "s3:ListBucket"
                  - "s3:GetBucketLocation"
                Resource: !Ref  DestinationBucketARN
              - Effect: "Allow"
                Action:
                  - "glue:StartCrawler"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "cloudwatch:GetMetricStatistics"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "sqs:ReceiveMessage"
                  - "sqs:DeleteMessage"
                  - "sqs:GetQueueAttributes"
                Resource: "*"
              - Effect: "Allow"
                Action:
                  - "rds:DescribeDBProxyTargetGroups"
                  - "rds:DescribeDBInstanceAutomatedBackups"
                  - "rds:DescribeDBEngineVersions"
                  - "rds:DescribeDBSubnetGroups"
                  - "rds:DescribeGlobalClusters"
                  - "rds:DescribeExportTasks"
                  - "rds:DescribePendingMaintenanceActions"
                  - "rds:DescribeEngineDefaultParameters"
                  - "rds:DescribeDBParameterGroups"
                  - "rds:DescribeDBClusterBacktracks"
                  - "rds:DescribeCustomAvailabilityZones"
                  - "rds:DescribeReservedDBInstancesOfferings"
                  - "rds:DescribeDBProxyTargets"
                  - "rds:DownloadDBLogFilePortion"
                  - "rds:DescribeDBInstances"
                  - "rds:DescribeSourceRegions"
                  - "rds:DescribeEngineDefaultClusterParameters"
                  - "rds:DescribeInstallationMedia"
                  - "rds:DescribeDBProxies"
                  - "rds:DescribeDBParameters"
                  - "rds:DescribeEventCategories"
                  - "rds:DescribeDBProxyEndpoints"
                  - "rds:DescribeEvents"
                  - "rds:DescribeDBClusterSnapshotAttributes"
                  - "rds:DescribeDBClusterParameters"
                  - "rds:DescribeEventSubscriptions"
                  - "rds:DescribeDBSnapshots"
                  - "rds:DescribeDBLogFiles"
                  - "rds:DescribeDBSecurityGroups"
                  - "rds:DescribeDBSnapshotAttributes"
                  - "rds:DescribeReservedDBInstances"
                  - "rds:ListTagsForResource"
                  - "rds:DescribeValidDBInstanceModifications"
                  - "rds:DescribeDBClusterSnapshots"
                  - "rds:DescribeOrderableDBInstanceOptions"
                  - "rds:DescribeOptionGroupOptions"
                  - "rds:DescribeDBClusterEndpoints"
                  - "rds:DescribeCertificates"
                  - "rds:DescribeDBClusters"
                  - "rds:DescribeAccountAttributes"
                  - "rds:DescribeOptionGroups"
                  - "rds:DescribeDBClusterParameterGroups"
                  - "ec2:DescribeRegions"
                Resource: "*"
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub "${CFDataName}-Lambda-Function"
      Description: !Sub "LambdaFunction to retrieve ${CFDataName}"
      Runtime: python3.8
      Code:
        ZipFile: |
          import boto3
          from botocore.exceptions import ClientError
          import os
          from boto3.s3.transfer import S3Transfer
          from datetime import datetime, timedelta, date
          import json
          from re import sub
          import logging

          #Environment Variables
          DEST_BUCKET = os.environ['S3_BUCKET']
          DEST_PREFIX = os.environ['DEST_PREFIX']

          functions = {'rds':
                          {'regional' : 1,
                          'api' : 'rds',
                          'functions' : [
                              {'name' : 'get_rds_stats',
                              'output_path' : 'rds_stats',
                              'output_file_name' : 'rds_stats.json' }
                          ]
                          }
                      }

          METRICS_FOR_VOLUMES = [
              'FreeableMemory',
              'CPUUtilization',
              'NetworkReceiveThroughput',
              'NetworkTransmitThroughput',
              'ReadIOPS',
              'WriteIOPS',
              'FreeStorageSpace'
              ]

          TAGS_TO_RETRIEVE = [
              'Environment',
              'Schedule',
              'Application',
              'Project',
          ]


          now = datetime.utcnow()
          DAYS = int(os.environ['DAYS'])
          past = now - timedelta(days=DAYS)
          today = datetime.today().strftime('%Y-%m-%d')
          tod = date.today()
          year = tod.year
          month = tod.strftime('%m')
          future = now# + timedelta(minutes=10)
          period = 3600
          s3client = boto3.client('s3')

          def format_rds(rds):

              if len(rds["Attachments"]) == 0:
                  instance_id = "unattached"
                  device = ""
              else:
                  instance_id = rds["Attachments"][0]["InstanceId"] if rds["Attachments"][0]["InstanceId"] else ""
                  device = rds["Attachments"][0]["Device"] if rds["Attachments"][0]["Device"] else ""

              output = {
                  "az": rds["AvailabilityZone"],
                  "instanceId": instance_id,
                  "rdsId": rds["VolumeId"],
                  "device": device,
                  "rdsType": rds["VolumeType"],
                  "sizeGB": rds["Size"],
                  "IOPS": rds["Iops"],
                  "rdsCreateTime": rds["CreateTime"]
              }

              tags = {}
              filtered_tags = {}
              if "Tags" in rds:
                  for tag in rds["Tags"]:
                      tags[sub('[^a-zA-Z0-9-_ *.]', '', tag["Key"].replace(",", " "))] = sub('[^a-zA-Z0-9-_ *.]', '', tag["Value"].replace(",", " "))
              for tags_required in TAGS_TO_RETRIEVE:
                  filtered_tags[tags_required] = tags[tags_required] if tags_required in tags else ''

              output["tags"] = filtered_tags

              return output


          def get_rds_ids(rds_client):
              rds_inventory = []
              rds_information = rds_client.describe_rds(Filters=[{'Name': 'status','Values':['in-use','available']}])
              if not rds_information or not rds_information['Volumes']:
                  rds_information = []
              else:
                  rds_information = rds_information['Volumes']

              for rds in rds_information:
                  rds_inventory.append(format_rds(rds))

              return rds_inventory

          def upload_file_to_s3(s3client, bucket_name, src, dest):
              transfer = S3Transfer(s3client)
              print("Uploading file %s to %s/%s" %(src, bucket_name, dest))
              transfer.upload_file(src, bucket_name, dest, extra_args={'ACL': 'bucket-owner-full-control'})
              print('file upload successful')

          def store_data_to_s3(data, s3client, region, service, path, resource_type, resource_value, filename, accountID):
              
                filepath=f"/tmp/{region}-{filename}"
                with open(filepath, 'w') as f:
                    json.dump(data, f, default=str)
                    f.write('\n')
                #Check file size
                fileSize = os.path.getsize(filepath)
                if fileSize == 0:  
                    print(f"No data in file for {DestinationPrefix}")
                else:
                    upload_file_to_s3(s3client, DEST_BUCKET, filepath,
                                    f"{DEST_PREFIX}/{path}/accountid={accountID}/region={region}/{resource_type}={resource_value}/year={year}/month={month}/{today}-{filename}")
          def start_crawler():
              glue_client = boto3.client('glue')
              try:
                  glue_client.start_crawler(Name= os.environ["CRAWLER_NAME"])
                  print("crawler started")
              except Exception as e:
                  # Send some context about this error to Lambda Logs
                  print("%s" % e)

          def get_rds_stats(cwclient, client, s3client, region, service, path, filename, accountID):
              datapoints = {}
              data = {}
              rds_inventory = client.describe_db_instances()

              for rds in rds_inventory['DBInstances']:
                  
                  for metric in METRICS_FOR_VOLUMES:
                      results = cwclient.get_metric_statistics(StartTime=past, EndTime=future, MetricName=metric,
                      Namespace='AWS/RDS',Statistics=['Average','Maximum','Minimum'],Period=period,
                      Dimensions=[{"Name": "DBInstanceIdentifier", "Value": rds["DBInstanceIdentifier"]}])
                      datapoints.update({metric:results['Datapoints']})
                  rds["Datapoints"] = datapoints
                  store_data_to_s3(rds, s3client, region, service, path, 'rds_id', rds['DBInstanceIdentifier'], filename, accountID)
          
          def assume_role(account_id, service, region):
            role_name = os.environ['ROLENAME']
            role_arn = f"arn:aws:iam::{account_id}:role/{role_name}" 
            sts_client = boto3.client('sts')
            
            try:
                assumedRoleObject = sts_client.assume_role(
                    RoleArn=role_arn,
                    RoleSessionName="AssumeRoleRoot"
                    )
                
                credentials = assumedRoleObject['Credentials']
                client = boto3.client(
                    service,
                    aws_access_key_id=credentials['AccessKeyId'],
                    aws_secret_access_key=credentials['SecretAccessKey'],
                    aws_session_token=credentials['SessionToken'],
                    region_name = region
                )
                return client

            except ClientError as e:
                logging.warning(f"Unexpected error Account {account_id}: {e}")
                return None


          def lambda_handler(event, context):
              try:
                  for record in event['Records']:
                      body = json.loads(record["body"])
                      account_id = body["account_id"]
                      print(account_id)
                      s3client = boto3.client('s3')
                      regions = get_supported_regions('ec2')
                      
                      
                      for service in functions.keys():
                          if functions[service]['regional']:
                              for region in regions:
                                  client = assume_role(account_id, functions[service]['api'], region['RegionName'])
                                  for f in functions[service]['functions']:
                                      cw_client = boto3.client('cloudwatch', region_name = region['RegionName'])
                                      data = globals()[f['name']](cw_client, client, s3client, region['RegionName'], service, f['output_path'], f['output_file_name'], account_id)
                          else:
                              client=boto3.client(service)
                              for f in functions[service]['functions']:
                                  cw_client = boto3.client('cloudwatch', region_name = 'us-east-1')
                  
                                  data = globals()[f['name']](cw_client, client, s3client, 'us-east-1', service, f['output_path'], f['output_file_name'], account_id)
                      start_crawler()
                      return "Successful"
              except Exception as e:
                  # Send some context about this error to Lambda Logs
                  logging.warning("%s" % e)



          def get_supported_regions(service):
              response = []
              if service == 'ec2':
                  ec2_c = boto3.client('ec2')
                  response = ec2_c.describe_regions()
              return response['Regions'] if response['Regions'] else []


      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 300
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Environment:
        Variables:
          S3_BUCKET:
            !Ref DestinationBucket
          ACCOUNT_ID: AWS::AccountId
          DAYS:
            !Ref DAYS
          CRAWLER_NAME: !Ref Crawler
          DEST_PREFIX: !Ref CFDataName
          ROLENAME: !Ref MultiAccountRoleName
  Crawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name:
        !Sub "${CFDataName}Crawler"
      Role: !Ref GlueRoleArn
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/"
  AthenaQuery:
    Type: AWS::Athena::NamedQuery
    Properties:
      Database: !Ref DatabaseName
      Description: Provides a summary view of the lambda data
      Name: !Sub ${CFDataName}-summary-view
      QueryString: !Sub
        "SELECT dbinstanceidentifier, 'memory' as Metric, memory.timestamp, memory.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.FreeableMemory) as t(memory)
        union
        SELECT dbinstanceidentifier, 'cpu' as Metric, cpu.timestamp, cpu.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.CPUUtilization) as t(cpu)
        union
        SELECT dbinstanceidentifier, 'NetworkReceiveThroughput' as Metric, NetworkReceiveThroughput.timestamp, NetworkReceiveThroughput.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.NetworkReceiveThroughput) as t(NetworkReceiveThroughput)
        union
        SELECT dbinstanceidentifier, 'NetworkTransmitThroughput' as Metric, NetworkTransmitThroughput.timestamp, NetworkTransmitThroughput.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.CPUUtilization) as t(NetworkTransmitThroughput)
        union
        SELECT dbinstanceidentifier, 'ReadIOPS' as Metric, ReadIOPS.timestamp, ReadIOPS.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.ReadIOPS) as t(ReadIOPS)
        union
        SELECT dbinstanceidentifier, 'WriteIOPS' as Metric, WriteIOPS.timestamp, WriteIOPS.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.WriteIOPS) as t(WriteIOPS)
        union
        SELECT dbinstanceidentifier, 'FreeStorageSpace' as Metric, FreeStorageSpace.timestamp, FreeStorageSpace.maximum
        FROM ${DatabaseName}.${CFDataName}
        cross join unnest(datapoints.FreeStorageSpace) as t(FreeStorageSpace)"
  EventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaFunction.Arn
      Action: lambda:InvokeFunction
      Principal: sqs.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !GetAtt TaskQueue.Arn
  TaskQueue: 
    Type: AWS::SQS::Queue
    Properties: 
      VisibilityTimeout: 300
      ReceiveMessageWaitTimeSeconds: 20
      DelaySeconds: 2
      KmsMasterKeyId: "alias/aws/sqs"
  EventSourceMapping:
    DependsOn:
      - EventPermission
    Type: AWS::Lambda::EventSourceMapping
    Properties:
      EventSourceArn: !GetAtt TaskQueue.Arn
      FunctionName: !GetAtt LambdaFunction.Arn