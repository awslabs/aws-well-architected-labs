---
AWSTemplateFormatVersion: '2010-09-09'
Description: Lambda to collect Org data and store in S3 
Parameters:
  DestinationBucket:
    Type: String
    Description: Name of the S3 Bucket that is created to hold org data
    AllowedPattern: (?=^.{3,63}$)(?!^(\d+\.)+\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])\.)*([a-z0-9]|[a-z0-9][a-z0-9\-]*[a-z0-9])$)
  ManagementRoleName:
    Type: String
    Description: The name of the IAM role that will be deployed in the management account which can retrieve AWS Organization data. KEEP THE SAME AS WHAT IS DEPLOYED INTO MANAGEMENT ACCOUNT
  ManagementAccountID:
    Type: String
    AllowedPattern: ([a-z0-9\-, ]*?$)
    Description: "(Ex: 123456789,098654321,789054312) List of Payer IDs you wish to collect data for. Can just be one Accounts"
  CFDataName:
    Type: String
    Description: The name of what this cf is doing.
    Default: organization
  DatabaseName:
    Type: String
    Description: Athena Database name where you table will be created
    Default: optimization_data
  CURTable:
    Type: String
    Description: Athena Table with CUR
    Default: cid_cur.cur
  GlueRoleARN:
    Type: String
  RolePrefix:
    Type: String
    Description: This prefix will be placed in front of all roles created. Note you may wish to add a dash at the end to make more readable
  Schedule:
    Type: String
    Description: Cron job to trigger the lambda using cloudwatch event
    Default: "rate(14 days)"
  LambdaAnalyticsARN:
    Type: String
    Description: Arn of lambda for Analytics
Outputs:
  LambdaFunctionName:
    Value:
      Ref: LambdaOrgData
  LambdaFunctionARN:
    Description: Lambda function ARN.
    Value:
      Fn::GetAtt:
        - LambdaOrgData
        - Arn
Resources:
  LambdaOrgData:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub
        - 'Organization-Data-${Id}'
        - Id: !Select [0, !Split ['-', !Ref AWS::StackName]]
      Description: Well Architected Lab - Collecting Accounts
      Runtime: python3.8
      Code:
        ZipFile: |
          #!/usr/bin/env python3
          #Gets org data, grouped by ous and tags from management accounts in json
          #Author Stephanie Gooch 2020
          import boto3
          import logging
          from botocore.exceptions import ClientError
          from botocore.client import Config
          import os
          import datetime
          import json

          bucket = os.environ["BUCKET_NAME"]
          prefix = os.environ["PREFIX"]
          crawler = os.environ["CRAWLER"]
          MANAGEMENT_ACCOUNT_IDS = os.environ['MANAGEMENT_ACCOUNT_IDS']
          role_name = os.environ['ROLE']

          def timeconverter(o):
              if isinstance(o, datetime.datetime):
                  return o.__str__()
              
          def lambda_handler(event, context):
              for payer_id in [r.strip() for r in MANAGEMENT_ACCOUNT_IDS.split(',')]:
                  try:
                      management_role_arn = f"arn:aws:iam::{payer_id}:role/{role_name}"
                      sts_connection = boto3.client('sts')
                      acct_b = sts_connection.assume_role(
                          RoleArn=management_role_arn,
                          RoleSessionName="cross_acct_lambda"
                      )
                      ACCESS_KEY = acct_b['Credentials']['AccessKeyId']
                      SECRET_KEY = acct_b['Credentials']['SecretAccessKey']
                      SESSION_TOKEN = acct_b['Credentials']['SessionToken']
                      client = boto3.client(
                          "organizations", region_name="us-east-1", #Using the Organizations client to get the data. This MUST be us-east-1 regardless of region you have the Lamda in
                          aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY, aws_session_token=SESSION_TOKEN, )

                      root_id    = client.list_roots()['Roots'][0]['Id']
                      ou_id_list = get_ou_ids(root_id, client)
                      with open('/tmp/ou-org.json', 'w') as f:
                          for ou in ou_id_list.keys():
                              account_data(f, ou, ou_id_list[ou][0], client, ou_id_list[ou][1])
                      s3_upload('ou-org', payer_id)

                      with open('/tmp/acc-org.json', 'w') as f:
                          account_data(f, root_id, root_id, client,'')
                      s3_upload('acc-org', payer_id)
                      start_crawler(crawler)
                  except Exception as e:
                      # Send some context about this error to Lambda Logs
                      logging.warning("%s" % e)
                      continue 

          def get_ou_ids(parent_id, client):
              full_result = {}
              test = []
              ous = ou_loop(parent_id, test, client)
              print(ous)
              
              for ou in ous:
                  ou_info = client.describe_organizational_unit(OrganizationalUnitId=ou)
                  full_result[ou]=[]
                  full_result[ou].append(ou_info['OrganizationalUnit']['Name'])
                  tags =list_tags(client, ou)
                  full_result[ou].append(tags)
              return full_result

          def ou_loop(parent_id, test, client):
              print(parent_id)
              paginator = client.get_paginator('list_children')
              iterator = paginator.paginate( ParentId=parent_id, ChildType='ORGANIZATIONAL_UNIT')
              for page in iterator:
                  for ou in page['Children']:
                      test.append(ou['Id'])
                      ou_loop(ou['Id'], test, client)
              return test

          def account_data(f, parent, parent_name, client, parent_tags):
              account_id_list = get_acc_ids(parent, client)
              for account_id in account_id_list:
                  response = client.describe_account(AccountId=account_id)
                  account  = response["Account"]          
                  tags_list = list_tags(client, account["Id"])
                  for org_tag in tags_list:
                      tag = org_tag['Key']
                      value = org_tag['Value']
                      kv = {tag : value}
                      account.update(kv)
                  account.update({'Parent' : parent_name, 'Parent_Tags':parent_tags})        
                  data = json.dumps(account, default = timeconverter) 
                  f.write(data)
                  f.write('\n')
                  

          def get_acc_ids(parent_id,  client):
              full_result = []
              paginator = client.get_paginator('list_accounts_for_parent')
              iterator  = paginator.paginate(ParentId=parent_id)
              for page in iterator:
                  for acc in page['Accounts']:
                      print(acc['Id'])
                      full_result.append(acc['Id'])
              return full_result

          def list_tags(client, resource_id): 
              tags = []
              paginator = client.get_paginator("list_tags_for_resource")
              response_iterator = paginator.paginate(ResourceId=resource_id)
              for response in response_iterator:
                  tags.extend(response['Tags'])
              return tags

          def s3_upload(file_name, payer_id):
              try:
                  s3 = boto3.client('s3', os.environ["REGION"],config=Config(s3={'addressing_style': 'path'}))
                  s3.upload_file(f'/tmp/{file_name}.json', bucket, f"{prefix}/{prefix}-data/payer_id={payer_id}/{file_name}.json") 
                  print(f"{file_name}org data in s3")
              except Exception as e:
                  print(e)

          def start_crawler(crawler):
              glue_client = boto3.client("glue")
              try:
                  glue_client.start_crawler(Name=crawler)
                  print(f"{crawler} has been started")
              except Exception as e:
                  # Send some context about this error to Lambda Logs
                  logging.warning("%s" % e)

      Handler: 'index.lambda_handler'
      MemorySize: 2688
      Timeout: 600
      Role:
        Fn::GetAtt:
          - LambdaRole
          - Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref DestinationBucket
          REGION: !Ref "AWS::Region"
          CRAWLER: !Ref OrgCrawler
          ROLE: !Ref ManagementRoleName
          MANAGEMENT_ACCOUNT_IDS: !Ref ManagementAccountID
          PREFIX: !Ref CFDataName
  OrgCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name:  !Join
          - '-'
          - - !Ref CFDataName
            - !Select [0, !Split ['-', !Ref AWS::StackName]]
      Role: !Ref GlueRoleARN
      DatabaseName: !Ref DatabaseName
      Targets:
        S3Targets:
          - Path: !Sub "s3://${DestinationBucket}/${CFDataName}/${CFDataName}-data/"
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub "${RolePrefix}AWS-Organization-Data-Execute-Lambda"
      AssumeRolePolicyDocument:
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
      Path: /
      Policies:
        - PolicyName: "Assume-Management-Organization-Data-Role"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "sts:AssumeRole"
                Resource: !Sub "arn:aws:iam::*:role/${ManagementRoleName}" # Need to assume a Read role in all Management Accounts
        - PolicyName: "S3-Access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "logs:CreateLogGroup"
                  - "logs:CreateLogStream"
                  - "logs:PutLogEvents"
                  - "logs:DescribeLogStreams"
                Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/Lambda_Organization_Data_Collector*"
              - Effect: "Allow"
                Action:
                  - "glue:StartCrawler"
                Resource: !Sub "arn:aws:glue:${AWS::Region}:${AWS::AccountId}:crawler/${OrgCrawler}"
  CloudWatchTrigger:
    Type: AWS::Events::Rule
    Properties:
      Description: Monthly
      Name: Monthly-Scheduler-For-Accounts
      ScheduleExpression: !Ref Schedule
      State: ENABLED
      Targets:
        - Arn:
            Fn::GetAtt:
              - LambdaOrgData
              - Arn
          Id: WeeklyTriggerForGetAccounts
  EventPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !GetAtt LambdaOrgData.Arn
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
      SourceArn: !GetAtt CloudWatchTrigger.Arn
  AthenaQuery:
      Type: AWS::Athena::NamedQuery
      Properties:
        Database: !Ref DatabaseName
        Description: Provides a summary view of the budgets
        Name: budgets_cur
        QueryString: !Sub
          CREATE OR REPLACE VIEW budgets_cur AS
          SELECT *
          FROM ("${CURTable}" cur
          INNER JOIN "${DatabaseName}"."organisation_data"
            ON ("cur"."line_item_usage_account_id" = "${DatabaseName}"."organisation_data"."id")) 
  LambdaAnalyticsExecutor:
    Type: Custom::LambdaAnalyticsExecutor
    Properties:
      ServiceToken: !Ref LambdaAnalyticsARN
      Name: !Ref CFDataName
